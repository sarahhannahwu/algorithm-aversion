ID,entry_type,title,abstract,url...5,"Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons","Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews","Statistics: (1) yes, (2) no",domain,agent characteristics,task features,notes,author,doi,issn,issue,journal,keywords,month,note,pages,pmid,publisher,volume,url...25,year,city,editor
ID-001,article,Humans as creativity gatekeepers: Are we biased against AI creativity?,"With artificial intelligence (AI) increasingly involved in the creation of organizational and commercial artifacts, human evaluators’ role as creativity gatekeepers of AI-produced artifacts will become critical for innovation processes. However, when humans evaluate creativity, their judgment is clouded by biases triggered by the characteristics of the creator. Drawing from folk psychology and algorithm aversion research, we examine whether the identity of the producer of a given artifact as artificial intelligence (AI) or human is a source of bias affecting people’s creativity evaluation of such artifact and what drives this effect. With four experimental studies (n = 2039), of which two were pre-registered, using different experimental designs and evaluation targets, we found that people sometimes—but not always—ascribe lower creativity to a product when they are told that the producer is an AI rather than a human. In addition, we found that people consistently perceive generative AI to exert less effort than humans in the creation of a given artifact, which drives the lower creativity ratings ascribed to generative AI producers. We discuss the implication of these findings for organizational creativity and innovation in the context of human-AI interaction. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/humans-as-creativity-gatekeepers-are-we-biased/docview/2866833583/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Humans+as+creativity+gatekeepers%3A+Are+we+biased+against+AI+creativity%3F&title=Journal+of+Business+and+Psychology&issn=08893268&date=2024-06-01&volume=39&issue=3&spage=643&au=Magni%2C+Federico%3BPark%2C+Jiyoung%3BChao%2C+Melody+Manchi&isbn=&jtitle=Journal+of+Business+and+Psychology&btitle=&rft_id=info:eric/2024-10238-001&rft_id=info:doi/10.1007%2Fs10869-023-09910-x,1,1,1,visual art,NA,NA,NA,Federico Magni and Jiyoung Park and Melody Manchi Chao,https://doi.org/10.1007/s10869-023-09910-x,"0889-3268, 0889-3268",3,Journal of Business and Psychology,"4120:Artificial Intelligence & Expert Systems,Adulthood (18 yrs & older),Artificial Intelligence,Artificial intelligence,Creativity,Empirical Study,Female,Hong Kong,Human,Human Computer Interaction,Male,Quantitative Study,US,article,cognitive biases,commercial artifacts,creativity evaluation,creativity gatekeepers,effort,folk psychology,human-AI interaction,innovation processes",6,"Copyright - © 2023, The Author(s). This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.",643-656,2866833583; 2024-10238-001,Springer,39,https://www.proquest.com/scholarly-journals/humans-as-creativity-gatekeepers-are-we-biased/docview/2866833583/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Humans+as+creativity+gatekeepers%3A+Are+we+biased+against+AI+creativity%3F&title=Journal+of+Business+and+Psychology&issn=08893268&date=2024-06-01&volume=39&issue=3&spage=643&au=Magni%2C+Federico%3BPark%2C+Jiyoung%3BChao%2C+Melody+Manchi&isbn=&jtitle=Journal+of+Business+and+Psychology&btitle=&rft_id=info:eric/2024-10238-001&rft_id=info:doi/10.1007%2Fs10869-023-09910-x,2024,NA,NA
ID-002,article,Overcome medical algorithm aversion: Conditional joint effect of direct and indirect information,"The just-ended COVID-19 pandemic and the looming global aging remind us that we need to be prepared for the shortage of doctors, which might become an urgent medical crisis in the future. Medical AI could relieve this urgency and sometimes perform better than human doctors; however, people are reluctant to trust medical AI because of algorithm aversion. Although several factors that can minimize algorithm aversion have been identified, they are not effective enough to promote medical AI as people’s first choice. Therefore, inspired by the direct and indirect information model of trust and media equation hypothesis, this research explored a new method to minimize aversion to medical AI by highlighting its social attributes. In 3 between-subject studies, a medical AI system’s direct information (i.e., transparency and quantitation of the decision-making process (DMP)) and indirect information (i.e., social proof) were manipulated. Study 1 (N = 193) and 2 (N = 429) showed that transparency of DMP and social proof increased trust in AI, but did not affect trust in human doctors. Social proof jointly affected trust in AI with non-quantitative DMP but not quantitative DMP. Study 3 (N = 184) further revealed the joint effect of the transparent non-quantitative DMP and near-perfect social proof, which could minimize algorithm aversion. These results extended the direct-indirect information model in interpersonal trust, revealed conditional media equation in human-AI trust, and offered practical implications for medical AI interface design. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/overcome-medical-algorithm-aversion-conditional/docview/3052746178/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Overcome+medical+algorithm+aversion%3A+Conditional+joint+effect+of+direct+and+indirect+information&title=International+Journal+of+Human-Computer+Interaction&issn=10447318&date=2024-04-26&volume=&issue=&spage=&au=Zhao%2C+Yansong%3BXiao%2C+Chengli&isbn=&jtitle=International+Journal+of+Human-Computer+Interaction&btitle=&rft_id=info:eric/2024-81330-001&rft_id=info:doi/10.1080%2F10447318.2024.2344146,1,1,1,medicine,NA,NA,NA,Yansong Zhao and Chengli Xiao,https://doi.org/10.1080/10447318.2024.2344146,"1044-7318, 1044-7318",NA,International Journal of Human-Computer Interaction,"3370:Health & Mental Health Services,Algorithms,Artificial Intelligence,COVID-19,Decision Making,Direct and indirect information,Empirical Study,Information Systems,Pandemics,Quantitative Study,algorithm aversion,algorithmic transparency,article,quantitation,social proof,trust",4,"Copyright - © 2024, Taylor & Francis",NA,3052746178; 2024-81330-001,Taylor & Francis Lawrence Erlbaum,NA,https://www.proquest.com/scholarly-journals/overcome-medical-algorithm-aversion-conditional/docview/3052746178/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Overcome+medical+algorithm+aversion%3A+Conditional+joint+effect+of+direct+and+indirect+information&title=International+Journal+of+Human-Computer+Interaction&issn=10447318&date=2024-04-26&volume=&issue=&spage=&au=Zhao%2C+Yansong%3BXiao%2C+Chengli&isbn=&jtitle=International+Journal+of+Human-Computer+Interaction&btitle=&rft_id=info:eric/2024-81330-001&rft_id=info:doi/10.1080%2F10447318.2024.2344146,2024,NA,NA
ID-003,article,Perceived corruption reduces algorithm aversion,"Scholarship on when and why humans are willing to rely on algorithms rather than other humans has made substantial progress in recent years, although virtually all such research is based on Western, educated, industrialized, rich, and democratic (WEIRD) research participants. This limits efforts to understand the cultural generalizability of attitudes toward algorithms. In this paper, I study algorithm aversion among participants from over 30 countries on all inhabited continents, thereby significantly increasing the diversity of this field's knowledge base. Furthermore, I leverage this diversity to test a theoretically derived prediction: that perceived corruption makes algorithmic decision‐making more appealing. I find that participants who are born or raised in countries with high levels of perceived corruption are much less averse to algorithmic decision‐making (or, in some studies, are not at all algorithm averse), relative to those from countries with low perceived corruption. Furthermore, experimentally varying corruption salience causes a decrease in algorithm aversion. I explore mechanisms and boundary conditions of these effects and discuss the implications in the context of algorithms that can both increase and decrease injustice. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/perceived-corruption-reduces-algorithm-aversion/docview/2839835009/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Perceived+corruption+reduces+algorithm+aversion&title=Journal+of+Consumer+Psychology&issn=10577408&date=2024-04-01&volume=34&issue=2&spage=326&au=Castelo%2C+Noah&isbn=&jtitle=Journal+of+Consumer+Psychology&btitle=&rft_id=info:eric/2023-90653-001&rft_id=info:doi/10.1002%2Fjcpy.1373,1,1,1,NA,NA,NA,NA,Noah Castelo,https://doi.org/10.1002/jcpy.1373,"1057-7408, 1057-7408",2,Journal of Consumer Psychology,"2300:Human Experimental Psychology,Adulthood (18 yrs & older),Afghanistan,Algorithms,Angola,Australia,Aversion,Belgium,Canada,Corruption,Decision Making,Denmark,Empirical Study,Estonia,Female,Finland,Germany,Hong Kong,Human,Iraq,Ireland,Kyrgyzstan,Male,Netherlands,Quantitative Study,Republic of Congo,Sociocultural Factors,Sweden,Syria,United Kingdom,Venezuela,Western, educated, industrialized, rich & democratic,Yemen,Zimbabwe,algorithm aversion,article,corruption,culture",4,"Copyright - © 2023, The Author. Journal of Consumer Psychology published by Wiley Periodicals LLC on behalf of Society for Consumer Psychology. This is an open access article under the terms of the Creative Commons Attribution‐NonCommercial‐NoDerivs License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non‐commercial and no modifications or adaptations are made.",326-333,2839835009; 2023-90653-001,Wiley-Blackwell Publishing Ltd. Elsevier Science Lawrence Erlbaum,34,https://www.proquest.com/scholarly-journals/perceived-corruption-reduces-algorithm-aversion/docview/2839835009/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Perceived+corruption+reduces+algorithm+aversion&title=Journal+of+Consumer+Psychology&issn=10577408&date=2024-04-01&volume=34&issue=2&spage=326&au=Castelo%2C+Noah&isbn=&jtitle=Journal+of+Consumer+Psychology&btitle=&rft_id=info:eric/2023-90653-001&rft_id=info:doi/10.1002%2Fjcpy.1373,2024,NA,NA
ID-004,article,On the (non-) reliance on algorithms—A decision-theoretic account,"A wealth of empirical evidence shows that people display opposite behaviors when deciding whether to rely on an algorithm, even if it is inexpensive to do so and using the algorithm should enhance their own performance. This paper develops a formal theory to explain some of these conflicting facts and submit new testable predictions. Drawing from decision analysis, I invoke two key notions: the ‘value of information’ and the ‘value of control’. The value of information matters to users of algorithms like recommender systems and prediction machines, which essentially provide information. I find that ambiguity aversion or a subjective cost of employing an algorithm will tend to decrease the value of algorithmic information, while repeated exposure to an algorithm might not always increase this value. The value of control matters to users who may delegate decision making to an algorithm. I model how, under partial delegation, imperfect understanding of what the algorithm actually does (so the algorithm is in fact a black box) can cause algorithm aversion. Some possible remedies are formulated and discussed. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/on-non-reliance-algorithms-decision-theoretic/docview/3076769143/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=On+the+%28non-%29+reliance+on+algorithms%26mdash%3BA+decision-theoretic+account&title=Journal+of+Mathematical+Psychology&issn=00222496&date=2024-04-01&volume=119&issue=&spage=1&au=Sinclair-Desgagn%C3%A9%2C+Bernard&isbn=&jtitle=Journal+of+Mathematical+Psychology&btitle=&rft_id=info:eric/2024-73172-001&rft_id=info:doi/10.1016%2Fj.jmp.2024.102844,1,1,1,NA,NA,NA,"makes a model of information weighting, discusses the role of transparency in algorithm aversion",Bernard Sinclair-Desgagné,https://doi.org/10.1016/j.jmp.2024.102844,"0022-2496, 0022-2496",NA,Journal of Mathematical Psychology,"4100:Cognitive Psychology & Intelligent Systems,AI interaction,Algorithm aversion/appreciation,Algorithms,Artificial Intelligence,Aversion,Black-box AI,Decision Tree Algorithms,Human,Human Computer Interaction,Prediction,Value of control,Value of information,article",4,"Copyright - © 2024, Elsevier Inc. All rights reserved.",1-13,3076769143; 2024-73172-001,Elsevier Science,119,https://www.proquest.com/scholarly-journals/on-non-reliance-algorithms-decision-theoretic/docview/3076769143/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=On+the+%28non-%29+reliance+on+algorithms%26mdash%3BA+decision-theoretic+account&title=Journal+of+Mathematical+Psychology&issn=00222496&date=2024-04-01&volume=119&issue=&spage=1&au=Sinclair-Desgagn%C3%A9%2C+Bernard&isbn=&jtitle=Journal+of+Mathematical+Psychology&btitle=&rft_id=info:eric/2024-73172-001&rft_id=info:doi/10.1016%2Fj.jmp.2024.102844,2024,NA,NA
ID-006,article,Conservatives endorse Fintech? Individual regulatory focus attenuates the algorithm aversion effects in automated wealth management,"Algorithm aversion, which is a widely observed phenomenon, suggests that customers prefer human advisors over robo-advisors in automated wealth management. Studies have shown that prevention-focused (vs. promotion-focused) customers refuse high-tech products; therefore, algorithm aversion should be more pronounced among them. However, our findings challenge this notion. Drawing on the regulatory focus theory, through four studies, we discovered that (1) customers' mindset was relatively more prevention-focused when using robo-advisors and relatively more promotion-focused when using human advisors; (2) perceived flexibility mediated this effect; and (3) the algorithm aversion effects existed in automated wealth management, but vanished among prevention-focused customers because of the fit effects. Our findings identified a psychological boundary condition of algorithm aversion, provided a novel antecedent variable of regulatory focus, and provided managerial implications for robo-advisory firms’ market segmentation strategies. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/conservatives-endorse-fintech-individual/docview/2881835019/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Conservatives+endorse+Fintech%3F+Individual+regulatory+focus+attenuates+the+algorithm+aversion+effects+in+automated+wealth+management&title=Computers+in+Human+Behavior&issn=07475632&date=2023-11-01&volume=148&issue=&spage=1&au=Chang%2C+Yaping%3BWang%2C+Ran&isbn=&jtitle=Computers+in+Human+Behavior&btitle=&rft_id=info:eric/2024-08164-001&rft_id=info:doi/10.1016%2Fj.chb.2023.107872,1,1,1,"wealth management, finance",NA,NA,NA,Yaping Chang and Ran Wang,https://doi.org/10.1016/j.chb.2023.107872,"0747-5632, 0747-5632",NA,Computers in Human Behavior,"4100:Cognitive Psychology & Intelligent Systems,Adulthood (18 yrs & older),Algorithm aversion,Algorithms,Artificial Intelligence,Artificial intelligence,Automated Diagnosis,Automated wealth management,Aversion,China,Consumer Psychology,Empirical Study,Female,Financial Services,Human,Human Robot Interaction,Male,Quantitative Study,Regulatory focus,Robo-advisor,article",11,"Copyright - © 2023, Elsevier Ltd. All rights reserved.",1-12,2881835019; 2024-08164-001,Elsevier Science,148,https://www.proquest.com/scholarly-journals/conservatives-endorse-fintech-individual/docview/2881835019/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Conservatives+endorse+Fintech%3F+Individual+regulatory+focus+attenuates+the+algorithm+aversion+effects+in+automated+wealth+management&title=Computers+in+Human+Behavior&issn=07475632&date=2023-11-01&volume=148&issue=&spage=1&au=Chang%2C+Yaping%3BWang%2C+Ran&isbn=&jtitle=Computers+in+Human+Behavior&btitle=&rft_id=info:eric/2024-08164-001&rft_id=info:doi/10.1016%2Fj.chb.2023.107872,2023,NA,NA
ID-007,article,How to overcome algorithm aversion: Learning from mistakes,"When consumers avoid taking algorithmic advice, it can prove costly to both marketers (whose algorithmic product offerings go unused) and to themselves (who fail to reap the benefits that algorithmic predictions often provide). In a departure from previous research focusing on when algorithm aversion proves more or less likely, we sought to identify and remedy one reason why it occurs in the first place. In seven pre-registered studies, we find that consumers tend to avoid algorithmic advice on the often faulty assumption that those algorithms, unlike their human counterparts, cannot learn from mistakes, in turn offering an inroad by which to reduce algorithm aversion: highlighting their ability to learn. Process evidence, through both mediation and moderation, examines why consumers fail to trust algorithms that err across a variety of prediction domains and how different theory-driven interventions can solve the practical problem of enhancing trust and consequential choice in algorithms.",https://www.proquest.com/scholarly-journals/how-overcome-algorithm-aversion-learning-mistakes/docview/2685599736/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=How+to+overcome+algorithm+aversion%3A+Learning+from+mistakes&title=Journal+of+Consumer+Psychology&issn=10577408&date=2023-04-01&volume=33&issue=2&spage=285&au=Reich%2C+Taly%3BKaju%2C+Alex%3BMaglio%2C+Sam+J.&isbn=&jtitle=Journal+of+Consumer+Psychology&btitle=&rft_id=info:eric/2022-79864-001&rft_id=info:doi/10.1002%2Fjcpy.1313,1,1,1,marketing,ability to learn,NA,NA,Taly Reich and Alex Kaju and Sam J Maglio,https://doi.org/10.1002/jcpy.1313,"1057-7408, 1057-7408",2,Journal of Consumer Psychology,"3900:Consumer Psychology,Adulthood (18 yrs & older),Algorithms,Aversion,Consumer Behavior,Consumer Psychology,Empirical Study,Errors,Female,Human,Learning,Male,Marketing,Prediction,Quantitative Study,algorithm appreciation,algorithm aversion,article,intervention,learning from mistakes,mistakes",4,"Copyright - © 2022, Society for Consumer Psychology",285-302,2685599736; 2022-79864-001,Wiley-Blackwell Publishing Ltd. Elsevier Science Lawrence Erlbaum,33,https://www.proquest.com/scholarly-journals/how-overcome-algorithm-aversion-learning-mistakes/docview/2685599736/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=How+to+overcome+algorithm+aversion%3A+Learning+from+mistakes&title=Journal+of+Consumer+Psychology&issn=10577408&date=2023-04-01&volume=33&issue=2&spage=285&au=Reich%2C+Taly%3BKaju%2C+Alex%3BMaglio%2C+Sam+J.&isbn=&jtitle=Journal+of+Consumer+Psychology&btitle=&rft_id=info:eric/2022-79864-001&rft_id=info:doi/10.1002%2Fjcpy.1313,2023,NA,NA
ID-008,article,The extent of algorithm aversion in decision-making situations with varying gravity,"Algorithms already carry out many tasks more reliably than human experts. Nevertheless, some subjects have an aversion towards algorithms. In some decision-making situations an error can have serious consequences, in others not. In the context of a framing experiment, we examine the connection between the consequences of a decision-making situation and the frequency of algorithm aversion. This shows that the more serious the consequences of a decision are, the more frequently algorithm aversion occurs. Particularly in the case of very important decisions, algorithm aversion thus leads to a reduction of the probability of success. This can be described as the tragedy of algorithm aversion. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/extent-algorithm-aversion-decision-making/docview/2891813982/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=The+extent+of+algorithm+aversion+in+decision-making+situations+with+varying+gravity&title=PLoS+ONE&issn=1932-6203&date=2023-02-21&volume=18&issue=2&spage=&au=Filiz%2C+Ibrahim%3BJudek%2C+Jan+Ren%C3%A9%3BLorenz%2C+Marco%3BSpiwoks%2C+Markus&isbn=&jtitle=PLoS+ONE&btitle=&rft_id=info:eric/2023-49932-001&rft_id=info:doi/10.1371%2Fjournal.pone.0278751,1,1,1,multiple,NA,"gravity, how high-stakes the task is. includes driving, evaluation of MRI scans,dating recommendations, selection of recipes",NA,Ibrahim Filiz and Jan René Judek and Marco Lorenz and Markus Spiwoks,https://doi.org/10.1371/journal.pone.0278751,NA,2,PLoS ONE,"4120:Artificial Intelligence & Expert Systems,Adulthood (18 yrs & older),Algorithms,Aversion,Computer Anxiety,Consequence,Decision Making,Decision Support Systems,Empirical Study,Female,Germany,Human,Male,Quantitative Study,algorithms,article,artificial intelligence,automated decision-making,aversion",2,"Copyright - © 2023, Filiz et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",21,2891813982; 2023-49932-001,Public Library of Science,18,https://www.proquest.com/scholarly-journals/extent-algorithm-aversion-decision-making/docview/2891813982/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=The+extent+of+algorithm+aversion+in+decision-making+situations+with+varying+gravity&title=PLoS+ONE&issn=1932-6203&date=2023-02-21&volume=18&issue=2&spage=&au=Filiz%2C+Ibrahim%3BJudek%2C+Jan+Ren%C3%A9%3BLorenz%2C+Marco%3BSpiwoks%2C+Markus&isbn=&jtitle=PLoS+ONE&btitle=&rft_id=info:eric/2023-49932-001&rft_id=info:doi/10.1371%2Fjournal.pone.0278751,2023,NA,NA
ID-009,article,When algorithms err: Differential impact of early vs. late errors on users’ reliance on algorithms,"Errors are a natural part of predictive algorithms, but may discourage users from relying on algorithms. We conduct two experiments to demonstrate that reliance on a predictive algorithm following a substantial error is affected by (i) when the error occurs and (ii) how the algorithm is used in the decision-making process. We find that the impact of an error on reliance depends on whether the error occurs early (i.e., when users first start using the algorithm) or late (i.e., after users have used the algorithm for an extended period). While an early error results in substantial and persistent reliance reduction, a late error affects reliance only temporarily and to a lesser extent. However, when users have more control over how to use the algorithm’s predictions, error timing ceases to have a significant impact. Our work advances the understanding of algorithm aversion and informs the practical design of algorithmic decision-making systems.",https://www.proquest.com/scholarly-journals/i-when-algorithms-err-differential-impact-early/docview/2886383689/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=When+algorithms+err%3A+Differential+impact+of+early+vs.+late+errors+on+users%26rsquo%3B+reliance+on+algorithms&title=ACM+Transactions+on+Computer-Human+Interaction&issn=10730516&date=2023-02-01&volume=30&issue=1&spage=1&au=Kim%2C+Antino%3BYang%2C+Mochen%3BZhang%2C+Jingjing&isbn=&jtitle=ACM+Transactions+on+Computer-Human+Interaction&btitle=&rft_id=info:eric/2023-93508-014&rft_id=info:doi/10.1145%2F3557889,1,1,1,"prediction (using fictitious example of predicting crop yield based on rainfall, fertilizer)",NA,NA,Ps choose between their own prediction vs. the algorithm's prediction. Experimenters varied the timing of the algorithm's error and user control over how to use the algorithm's predictions.,Antino Kim and Mochen Yang and Jingjing Zhang,https://doi.org/10.1145/3557889,"1073-0516, 1073-0516",1,ACM Transactions on Computer-Human Interaction,"4010:Human Factors Engineering,Algorithmic reliance,Algorithms,Decision Making,Empirical Study,Human,Prediction Errors,Quantitative Study,article,decision support,laboratory experiment,prediction error,timing of error",2,"Copyright - © 2023, Association for Computing Machinery",1-36,2886383689; 2023-93508-014,ACM - Association for Computing Machinery,30,https://www.proquest.com/scholarly-journals/i-when-algorithms-err-differential-impact-early/docview/2886383689/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=When+algorithms+err%3A+Differential+impact+of+early+vs.+late+errors+on+users%26rsquo%3B+reliance+on+algorithms&title=ACM+Transactions+on+Computer-Human+Interaction&issn=10730516&date=2023-02-01&volume=30&issue=1&spage=1&au=Kim%2C+Antino%3BYang%2C+Mochen%3BZhang%2C+Jingjing&isbn=&jtitle=ACM+Transactions+on+Computer-Human+Interaction&btitle=&rft_id=info:eric/2023-93508-014&rft_id=info:doi/10.1145%2F3557889,2023,NA,NA
ID-010,article,Algorithmic transference: People overgeneralize failures of AI in the government,"Artificial intelligence (AI) is pervading the government and transforming how public services are provided to consumers across policy areas spanning allocation of government benefits, law enforcement, risk monitoring, and the provision of services. Despite technological improvements, AI systems are fallible and may err. How do consumers respond when learning of AI failures? In 13 preregistered studies (N = 3,724) across a range of policy areas, the authors show that algorithmic failures are generalized more broadly than human failures. This effect is termed “algorithmic transference” as it is an inferential process that generalizes (i.e., transfers) information about one member of a group to another member of that same group. Rather than reflecting generalized algorithm aversion, algorithmic transference is rooted in social categorization: it stems from how people perceive a group of AI systems versus a group of humans. Because AI systems are perceived as more homogeneous than people, failure information about one AI algorithm is transferred to another algorithm to a greater extent than failure information about a person is transferred to another person. Capturing AI's impact on consumers and societies, these results show how the premature or mismanaged deployment of faulty AI technologies may undermine the very institutions that AI systems are meant to modernize. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/algorithmic-transference-people-overgeneralize/docview/2783241233/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Algorithmic+transference%3A+People+overgeneralize+failures+of+AI+in+the+government&title=Journal+of+Marketing+Research&issn=00222437&date=2023-02-01&volume=60&issue=1&spage=170&au=Longoni%2C+Chiara%3BCian%2C+Luca%3BKyung%2C+Ellie+J.&isbn=&jtitle=Journal+of+Marketing+Research&btitle=&rft_id=info:eric/2023-37176-009&rft_id=info:doi/10.1177%2F00222437221110139,1,1,1,"government (e.g., healthcare allocation, fraud detection)","failures, mistakes",NA,NA,Chiara Longoni and Luca Cian and Ellie J Kyung,https://doi.org/10.1177/00222437221110139,"0022-2437, 0022-2437",1,Journal of Marketing Research,"4120:Artificial Intelligence & Expert Systems,Adulthood (18 yrs & older),Algorithms,Artificial Intelligence,Empirical Study,Female,Government,Government Policy Making,Human,Learning,Male,Modernization,Quantitative Study,Social Categorization,Transfer (Learning),US,algorithms,article,artificial intelligence,government,public policy,social categorization,social impact",2,"Copyright - © 2022, American Marketing Association",170-188,2783241233; 2023-37176-009,Sage Publications American Marketing Association,60,https://www.proquest.com/scholarly-journals/algorithmic-transference-people-overgeneralize/docview/2783241233/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Algorithmic+transference%3A+People+overgeneralize+failures+of+AI+in+the+government&title=Journal+of+Marketing+Research&issn=00222437&date=2023-02-01&volume=60&issue=1&spage=170&au=Longoni%2C+Chiara%3BCian%2C+Luca%3BKyung%2C+Ellie+J.&isbn=&jtitle=Journal+of+Marketing+Research&btitle=&rft_id=info:eric/2023-37176-009&rft_id=info:doi/10.1177%2F00222437221110139,2023,NA,NA
ID-011,article,"Preference for human, not algorithm aversion","People sometimes exhibit a costly preference for humans relative to algorithms, which is often defined as a domain-general algorithm aversion. I propose it is instead driven by biased evaluations of the self and other humans, which occurs more narrowly in domains where identity is threatened and when evaluative criteria are ambiguous. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/preference-human-not-algorithm-aversion/docview/2700773690/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Preference+for+human%2C+not+algorithm+aversion&title=Trends+in+Cognitive+Sciences&issn=13646613&date=2022-10-01&volume=26&issue=10&spage=824&au=Morewedge%2C+Carey+K.&isbn=&jtitle=Trends+in+Cognitive+Sciences&btitle=&rft_id=info:eric/2022-90094-001&rft_id=info:doi/10.1016%2Fj.tics.2022.07.007,1,3,2,multiple,NA,"identity-relevance, subjectivity of the task",NA,Carey K Morewedge,https://doi.org/10.1016/j.tics.2022.07.007,"1364-6613, 1364-6613",10,Trends in Cognitive Sciences,"4010:Human Factors Engineering,Algorithmic Bias,Algorithms,Aversion,Consumer Behavior,Decision Making,Evaluation Criteria,Human,Humans,Preferences,algorithm aversion,algorithmic bias,algorithms,article,self-enhancement,self-protection",10,"Copyright - © 2022, Elsevier Ltd. All rights reserved.",824-826,2700773690; 2022-90094-001,Elsevier Science,26,https://www.proquest.com/scholarly-journals/preference-human-not-algorithm-aversion/docview/2700773690/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Preference+for+human%2C+not+algorithm+aversion&title=Trends+in+Cognitive+Sciences&issn=13646613&date=2022-10-01&volume=26&issue=10&spage=824&au=Morewedge%2C+Carey+K.&isbn=&jtitle=Trends+in+Cognitive+Sciences&btitle=&rft_id=info:eric/2022-90094-001&rft_id=info:doi/10.1016%2Fj.tics.2022.07.007,2022,NA,NA
ID-012,article,The effect of past algorithmic performance and decision significance on algorithmic advice acceptance,"This study aimed to investigate people’s willingness to accept algorithmic over human advice, under varying conditions of previous algorithmic performance and decision significance. We randomly presented hypothetical scenarios to 218 participants. Scenarios differed in relation to decision context (i.e., choices relating to taxi-routes, movies, restaurants, medical interventions, savings strategies, and bush fire evacuation), and within each scenario past algorithmic performance was also varied (equal, above average, or far greater than the human expert). Participants were asked to rate decision significance, and their likelihood of choosing the algorithmic advice over the human expert. Based on participants’ perceived decision significance, scenarios were classified as either low- or high-stakes. We tested for differences in participants’ ratings of algorithmic acceptance across levels of past performance and decision significance. Results revealed that as past accuracy and decision significance increased, the likelihood of algorithmic advice adoption also increased. An interaction between past accuracy and decision significance indicated increased algorithmic advice acceptance under conditions of far greater previous performance, in high-, compared to low-stakes scenarios. These findings are contrary to a large body of past research wherein people’s algorithm aversion persisted despite superior algorithmic performance and have implications to human-algorithm interaction and system design. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/effect-past-algorithmic-performance-decision/docview/2590220142/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=The+effect+of+past+algorithmic+performance+and+decision+significance+on+algorithmic+advice+acceptance&title=International+Journal+of+Human-Computer+Interaction&issn=10447318&date=2022-08-01&volume=38&issue=13&spage=1228&au=Saragih%2C+Melissa%3BMorrison%2C+Ben+W.&isbn=&jtitle=International+Journal+of+Human-Computer+Interaction&btitle=&rft_id=info:eric/2022-01353-001&rft_id=info:doi/10.1080%2F10447318.2021.1990518,1,1,1,multiple,accuracy,decision significance (from low-stakes decisions like taxi routes to high-stakes decisions like bush fire evacuation),NA,Melissa Saragih and Ben W Morrison,https://doi.org/10.1080/10447318.2021.1990518,"1044-7318, 1044-7318",13,International Journal of Human-Computer Interaction,"4010:Human Factors Engineering,Adulthood (18 yrs & older),Aged (65 yrs & older),Algorithms,Australia,Aversion,Decision Making,Empirical Study,Experience Level,Female,Films,Hospitality Industry,Human,Human Computer Interaction,Intervention,Male,Middle Age (40-64 yrs),Quantitative Study,Systems Design,Thirties (30-39 yrs),Young Adulthood (18-29 yrs),algorithmic advice acceptance,algorithmic performance,article,decision significance,technology",8,"Copyright - © 2021, Taylor & Francis Group, LLC",1228-1237,2590220142; 2022-01353-001,Taylor & Francis Lawrence Erlbaum,38,https://www.proquest.com/scholarly-journals/effect-past-algorithmic-performance-decision/docview/2590220142/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=The+effect+of+past+algorithmic+performance+and+decision+significance+on+algorithmic+advice+acceptance&title=International+Journal+of+Human-Computer+Interaction&issn=10447318&date=2022-08-01&volume=38&issue=13&spage=1228&au=Saragih%2C+Melissa%3BMorrison%2C+Ben+W.&isbn=&jtitle=International+Journal+of+Human-Computer+Interaction&btitle=&rft_id=info:eric/2022-01353-001&rft_id=info:doi/10.1080%2F10447318.2021.1990518,2022,NA,NA
ID-013,article,Trust in public policy algorithms,"Algorithms are playing an increasingly important role in many areas of public policy. Yet, we know surprisingly little about the degree of trust individuals are willing to place in these algorithms. This article reports on a series of experiments on trust in algorithms for forecasting political events and criminal recidivism. Contrary to previous literature on algorithm aversion, we find that people show high levels of trust in algorithms relative to other sources of advice, even with minimal information about the algorithm. We also explore evaluation of combined human and algorithm advice. We find that, when experts make decisions in light of algorithm advice, the relative weight given to their judgments increases, but the algorithm’s guidance is not disregarded. Finally, using a conjoint experiment, we evaluate the factors that influence people’s preferences for algorithms, finding that risk aversion, data size, human in the loop, and developer reputation play an important role in engendering trust. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/trust-public-policy-algorithms/docview/2727591258/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Trust+in+public+policy+algorithms&title=The+Journal+of+Politics&issn=00223816&date=2022-04-01&volume=84&issue=2&spage=1132&au=Kennedy%2C+Ryan+P.%3BWaggoner%2C+Philip+D.%3BWard%2C+Matthew+M.&isbn=&jtitle=The+Journal+of+Politics&btitle=&rft_id=info:eric/2022-58681-034&rft_id=info:doi/10.1086%2F716283,1,1,1,public policy,NA,NA,"includes evaluations of combined human + algorithm advice. Study 4 probes the effects of various factors on algorithmic reliance. These factors include who developed the algorithm, algorithm accuracy, human involvement in final estimate.",Ryan P Kennedy and Philip D Waggoner and Matthew M Ward,https://doi.org/10.1086/716283,"0022-3816, 0022-3816",2,The Journal of Politics,"2900:Social Processes & Social Issues,Adulthood (18 yrs & older),Algorithms,Criminal Justice,Decision Making,Empirical Study,Geography,Government Policy Making,Human,Politics,Preferences,Quantitative Study,Recidivism,Trust (Social Behavior),algorithm designs,algorithms,article,criminal justice,geopolitical forecasting,hybrid decision-making,political events,preferences,public policy,recidivism,trust",4,"Copyright - © 2022, Southern Political Science Association. All rights reserved. Published by The University of Chicago Press for the Southern Political Science Association.",1132-1148,2727591258; 2022-58681-034,Univ of Chicago Press Blackwell Publishing Cambridge University Press Wiley-Blackwell Publishing Ltd.,84,https://www.proquest.com/scholarly-journals/trust-public-policy-algorithms/docview/2727591258/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Trust+in+public+policy+algorithms&title=The+Journal+of+Politics&issn=00223816&date=2022-04-01&volume=84&issue=2&spage=1132&au=Kennedy%2C+Ryan+P.%3BWaggoner%2C+Philip+D.%3BWard%2C+Matthew+M.&isbn=&jtitle=The+Journal+of+Politics&btitle=&rft_id=info:eric/2022-58681-034&rft_id=info:doi/10.1086%2F716283,2022,NA,NA
ID-014,article,"Effect of having, but not consulting, a computerized diagnostic aid","Previous research has described physicians’ reluctance to use computerized diagnostic aids (CDAs) but has never experimentally examined the effects of not consulting an aid that was readily available. Experiment 1. Participants read about a diagnosis made either by a physician or an auto mechanic (to control for perceived expertise). Half read that a CDA was available but never actually consulted; no mention of a CDA was made for the remaining half. For the physician, failure to consult the CDA had no significant effect on competence ratings for either the positive or negative outcome. For the auto mechanic, failure to consult the CDA actually increased competence ratings following a negative but not a positive outcome. Negligence judgments were greater for the mechanic than for the physician overall. Experiment 2. Using only a negative outcome, we included 2 different reasons for not consulting the aid and provided accuracy information highlighting the superiority of the CDA over the physician. In neither condition was the physician rated lower than when no aid was mentioned. Ratings were lower when the physician did not trust the CDA and, surprisingly, higher when the physician believed he or she already knew what the CDA would say. Finally, consistent with our previous research, ratings were also high when the physician consulted and then followed the advice of a CDA and low when the CDA was consulted but ignored. Individual differences in numeracy did not qualify these results. Implications for the literature on algorithm aversion and clinical practice are discussed.",https://www.proquest.com/scholarly-journals/effect-having-not-consulting-computerized/docview/2625005932/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Effect+of+having%2C+but+not+consulting%2C+a+computerized+diagnostic+aid&title=Medical+Decision+Making&issn=0272989X&date=2022-01-01&volume=42&issue=1&spage=94&au=Pezzo%2C+Mark+V.%3BNash%2C+Brenton+E.+D.%3BVieux%2C+Pierre%3BFoster-Grammer%2C+Hannah+W.&isbn=&jtitle=Medical+Decision+Making&btitle=&rft_id=info:eric/2022-12940-010&rft_id=info:doi/10.1177%2F0272989X211011160,1,1,1,healthcare,NA,NA,NA,Mark V Pezzo and Brenton E D Nash and Pierre Vieux and Hannah W Foster-Grammer,https://doi.org/10.1177/0272989X211011160,"0272-989X, 0272-989X",1,Medical Decision Making,"3430:Professional Personnel Attitudes & Characteristics,Adulthood (18 yrs & older),Artificial Intelligence,Computer Assisted Diagnosis,Decision Support Systems,Empirical Study,Female,Human,Male,Mathematical Model,Physicians,Quantitative Study,US,article,artificial intelligence,computerized decision support systems,decision aids,diagnostic aids,physician attitudes",1,"Copyright - © 2021, The Author(s)",94-104,2625005932; 2022-12940-010,Sage Publications,42,https://www.proquest.com/scholarly-journals/effect-having-not-consulting-computerized/docview/2625005932/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Effect+of+having%2C+but+not+consulting%2C+a+computerized+diagnostic+aid&title=Medical+Decision+Making&issn=0272989X&date=2022-01-01&volume=42&issue=1&spage=94&au=Pezzo%2C+Mark+V.%3BNash%2C+Brenton+E.+D.%3BVieux%2C+Pierre%3BFoster-Grammer%2C+Hannah+W.&isbn=&jtitle=Medical+Decision+Making&btitle=&rft_id=info:eric/2022-12940-010&rft_id=info:doi/10.1177%2F0272989X211011160,2022,NA,NA
ID-015,article,"To err is human, not algorithmic—Robust reactions to erring algorithms","When seeing algorithms err, we trust them less and decrease using them compared to after seeing humans err; this is called algorithm aversion. This paper builds on the algorithm aversion literature and the third-party reactions to mistreatment model to investigate a wider array of reactions to erring algorithms. Using an experimental design deployed with a vignette-based online study, we investigate gut reactions, justice cognitions, and behavioral intentions toward erring algorithms (compared to erring humans). Our results show that when the error was committed by an algorithm (vs. a human), gut reactions were harsher (i.e., less acceptance and more negative feelings), justice cognitions weaker (i.e., less blame, less forgiveness, and less accountability), and behavioral intentions stronger. These results remain independent of factors such as the maturity of the algorithms (better than or same as human performance), the severity of the error (high or low), and the domain of use (recruitment or finance). We discuss how these results complement the current literature thanks to a robust and more nuanced pattern of reactions to erring algorithms. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/err-is-human-not-algorithmic-robust-reactions/docview/2541904477/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=To+err+is+human%2C+not+algorithmic%26mdash%3BRobust+reactions+to+erring+algorithms&title=Computers+in+Human+Behavior&issn=07475632&date=2021-11-01&volume=124&issue=&spage=&au=Renier%2C+Laetitia+A.%3BSchmid+Mast%2C+Marianne%3BBekbergenova%2C+Anely&isbn=&jtitle=Computers+in+Human+Behavior&btitle=&rft_id=info:eric/2021-57705-001&rft_id=info:doi/10.1016%2Fj.chb.2021.106879,1,1,1,"recruitment, finance","maturity, severity of errors",NA,NA,Laetitia A Renier and Marianne Schmid Mast and Anely Bekbergenova,https://doi.org/10.1016/j.chb.2021.106879,"0747-5632, 0747-5632",NA,Computers in Human Behavior,"4120:Artificial Intelligence & Expert Systems,Adulthood (18 yrs & older),Algorithm aversion,Algorithms,Artificial Intelligence,Artificial intelligence,Cognitive Computing,Emotional Responses,Empirical Study,Error,Errors,Female,Human,Human Machine Systems,Ireland,Male,Negative Emotions,Perception,Quantitative Study,Third-party,US,article,erring algorithms versus erring humans,reactions",11,"Copyright - © 2021, The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",12,2541904477; 2021-57705-001,Elsevier Science,124,https://www.proquest.com/scholarly-journals/err-is-human-not-algorithmic-robust-reactions/docview/2541904477/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=To+err+is+human%2C+not+algorithmic%26mdash%3BRobust+reactions+to+erring+algorithms&title=Computers+in+Human+Behavior&issn=07475632&date=2021-11-01&volume=124&issue=&spage=&au=Renier%2C+Laetitia+A.%3BSchmid+Mast%2C+Marianne%3BBekbergenova%2C+Anely&isbn=&jtitle=Computers+in+Human+Behavior&btitle=&rft_id=info:eric/2021-57705-001&rft_id=info:doi/10.1016%2Fj.chb.2021.106879,2021,NA,NA
ID-016,article,How information processing style shapes people’s algorithm adoption,"Across four studies I tested why people are averse to relying on algorithmic judgments in person judgment tasks (e.g., student admissions), and examined how such aversions can be attenuated. I proposed that people tend to focus more on case-specific information (vs. general propositions) in person-judgment tasks, and that algorithms (vs. human experts) are believed to be skilled at addressing general propositions (vs. case-specific information). Thus, I posited that in person-judgment tasks, people would be less averse to relying on algorithmic judgments when they focus more on general propositions (vs. case-specific information). By varying the perceived importance of case-specific information and general propositions, the research provides support for these hypotheses. In addition, the results reveal the mechanism underlying algorithm aversion in person judgments and provide a cost-effective way to increase consumers’ algorithm adoption. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/how-information-processing-style-shapes-people-s/docview/2623028481/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=How+information+processing+style+shapes+people%26rsquo%3Bs+algorithm+adoption&title=Social+Behavior+and+Personality%3A+An+International+Journal&issn=03012212&date=2021-08-01&volume=49&issue=8&spage=1&au=Zhang%2C+Ke&isbn=&jtitle=Social+Behavior+and+Personality%3A+An+International+Journal&btitle=&rft_id=info:eric/2021-85083-005&rft_id=info:doi/,1,1,1,"multiple ""person judgments""",NA,case-specific vs. general,NA,Ke Zhang,NA,"0301-2212, 0301-2212",8,Social Behavior and Personality: An International Journal,"4120:Artificial Intelligence & Expert Systems,Adulthood (18 yrs & older),Algorithms,Automated Information Processing,Aversion,Empirical Study,Experience Level,Female,Human,Judgment,Male,Quantitative Study,US,algorithm aversion,article,case-specific information,general propositions,information processing style,person-judgment tasks",8,"Copyright - © 2021, Scientific Journal Publishers Limited. All Rights Reserved.",1-13,2623028481; 2021-85083-005,Society for Personality Research,49,https://www.proquest.com/scholarly-journals/how-information-processing-style-shapes-people-s/docview/2623028481/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=How+information+processing+style+shapes+people%26rsquo%3Bs+algorithm+adoption&title=Social+Behavior+and+Personality%3A+An+International+Journal&issn=03012212&date=2021-08-01&volume=49&issue=8&spage=1&au=Zhang%2C+Ke&isbn=&jtitle=Social+Behavior+and+Personality%3A+An+International+Journal&btitle=&rft_id=info:eric/2021-85083-005&rft_id=info:doi/,2021,NA,NA
ID-017,article,Robo-investment aversion,"In five experiments (n = 3,828), we investigate whether people prefer investment decisions to be made by human investment managers rather than by algorithms (“robos”). In all of the studies we investigate morally controversial companies, as it is plausible that a preference for humans as investment managers becomes exacerbated in areas where machines are less competent, such as morality. In Study 1, participants rated the permissibility of an algorithm to autonomously exclude morally controversial stocks from investment portfolios as lower than if a human fund manager did the same; this finding was not different if participants were informed that such exclusions might be financially disadvantageous for them. In Study 2, we show that this robo-investment aversion manifests itself both when considering investment in controversial and non-controversial industries. In Study 3, our findings show that robo-investment aversion is also present when algorithms are given the autonomy to increase investment in controversial stocks. In Studies 4 and 5, we investigate choices between actual humans and an algorithm. In Study 4 –which was incentivized–participants show no robo-investment aversion, but are significantly less likely to choose machines as investment managers for controversial stocks. In contrast, in Study 5 robo-investment aversion is present, but it is not different across controversial and non-controversial stocks. Overall, our findings show a considerable mean effect size for robo-investment aversion.",https://www.proquest.com/scholarly-journals/robo-investment-aversion/docview/2493142783/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Robo-investment+aversion&title=PLoS+ONE&issn=1932-6203&date=2020-09-17&volume=15&issue=9&spage=&au=Niszczota%2C+Pawe%C5%82%3BKasz%C3%A1s%2C+D%C3%A1niel&isbn=&jtitle=PLoS+ONE&btitle=&rft_id=info:eric/2021-10482-001&rft_id=info:doi/10.1371%2Fjournal.pone.0239277,1,1,1,finance,NA,NA,NA,Paweł Niszczota and Dániel Kaszás,https://doi.org/10.1371/journal.pone.0239277,NA,9,PLoS ONE,"3600:Organizational Psychology & Human Resources,Adulthood (18 yrs & older),Algorithms,Attitude to Computers,Autonomy,Aversion,Business Investments,Business Organizations,Economics, Behavioral,Empirical Study,Female,Human,Humans,Investments,Male,Management Personnel,Morality,Preferences,Quantitative Study,Robo-investment,Robotics,Switzerland,article,aversion,non-controversial stocks",9,"Copyright - © 2020, Niszczota, Kaszás. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",19,2493142783; 2021-10482-001,Public Library of Science,15,https://www.proquest.com/scholarly-journals/robo-investment-aversion/docview/2493142783/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Robo-investment+aversion&title=PLoS+ONE&issn=1932-6203&date=2020-09-17&volume=15&issue=9&spage=&au=Niszczota%2C+Pawe%C5%82%3BKasz%C3%A1s%2C+D%C3%A1niel&isbn=&jtitle=PLoS+ONE&btitle=&rft_id=info:eric/2021-10482-001&rft_id=info:doi/10.1371%2Fjournal.pone.0239277,2020,NA,NA
ID-019,article,Resistance to medical artificial intelligence is an attribute in a compensatory decision process: Response to Pezzo and Becksted (2020),"In Longoni et al. (2019), we examine how algorithm aversion influences utilization of healthcare delivered by human and artificial intelligence providers. Pezzo and Beckstead’s commentary (see record 2020-40390-011) asks whether resistance to medical AI takes the form of a noncompensatory decision strategy, in which a single attribute determines provider choice, or whether resistance to medical AI is one of several attributes considered in a compensatory decision strategy. We clarify that our paper both claims and finds that, all else equal, resistance to medical AI is one of several attributes (e.g., cost and performance) influencing healthcare utilization decisions. In other words, resistance to medical AI is a consequential input to compensatory decisions regarding healthcare utilization and provider choice decisions, not a noncompensatory decision strategy. People do not always reject healthcare provided by AI, and our article makes no claim that they do. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/resistance-medical-artificial-intelligence-is/docview/2547069039/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Resistance+to+medical+artificial+intelligence+is+an+attribute+in+a+compensatory+decision+process%3A+Response+to+Pezzo+and+Becksted+%282020%29&title=Judgment+and+Decision+Making&issn=1930-2975&date=2020-05-01&volume=15&issue=3&spage=446&au=Longoni%2C+Chiara%3BBonezzi%2C+Andrea%3BMorewedge%2C+Carey+K.&isbn=&jtitle=Judgment+and+Decision+Making&btitle=&rft_id=info:eric/2020-40390-012&rft_id=info:doi/10.1017%2FS1930297500007233,1,1,1,healthcare,NA,NA,NA,Chiara Longoni and Andrea Bonezzi and Carey K Morewedge,https://doi.org/10.1017/S1930297500007233,NA,3,Judgment and Decision Making,"3300:Health & Mental Health Treatment & Prevention,4120:Artificial Intelligence & Expert Systems,Algorithms,Artificial Intelligence,Automation,Caregivers,Client Attitudes,Decision Making,Health Care Utilization,Human Computer Interaction,Medical Treatment (General),Preferences,Strategies,Trust (Social Behavior),article,artificial intelligence,decision strategies,medicine",5,"Copyright - © 2020, The authors license this article under the terms of the Creative Commons Attribution 3.0 License",446-448,2547069039; 2020-40390-012,Society for Judgment and Decision Making Cambridge University Press,15,https://www.proquest.com/scholarly-journals/resistance-medical-artificial-intelligence-is/docview/2547069039/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Resistance+to+medical+artificial+intelligence+is+an+attribute+in+a+compensatory+decision+process%3A+Response+to+Pezzo+and+Becksted+%282020%29&title=Judgment+and+Decision+Making&issn=1930-2975&date=2020-05-01&volume=15&issue=3&spage=446&au=Longoni%2C+Chiara%3BBonezzi%2C+Andrea%3BMorewedge%2C+Carey+K.&isbn=&jtitle=Judgment+and+Decision+Making&btitle=&rft_id=info:eric/2020-40390-012&rft_id=info:doi/10.1017%2FS1930297500007233,2020,NA,NA
ID-020,article,Is optimal recommendation the best? A laboratory investigation under the newsvendor problem,"We investigate the impacts of the decision support system's recommendations on decision makers' psychology and decision behaviors under uncertain contexts where optimal solutions exist. As a representative of such contexts, the newsvendor problem is studied by using the method of laboratory experiments. Through providing an elaborately designed decision support system in Experiment I, we validate that the optimal recommendations help to alleviate human newsvendors' Pull-to-Center bias, i.e., the actual orders fall in the range between mean demand and optimal order that maximizes the expected profit theoretically, and decrease the bias asymmetry under two profit conditions (high or low). We also reveal that optimal recommendations can't eliminate the bias, as decision makers exhibit two competing psychological factors simultaneously when using the decision support system: algorithm aversion and regret aversion. Algorithm aversion persistently impedes them from following the superior recommendations, while regret aversion sometimes pulls them to approach to the recommendations driven by the feeling of experienced regret. Further, we redesign the decision support system in Experiment II and find that, although the conservative system recommendations are valueless compared with the optimal one, the well-designed radical system recommendations may eliminate the Pull-to-Center bias under the high-profit condition, through the interaction of the dominant regret aversion, dominated algorithm aversion, and the anchoring effect. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/is-optimal-recommendation-best-laboratory/docview/2535406793/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Is+optimal+recommendation+the+best%3F+A+laboratory+investigation+under+the+newsvendor+problem&title=Decision+Support+Systems&issn=01679236&date=2020-04-01&volume=131&issue=&spage=&au=Feng%2C+Xiaojing%3BGao%2C+Jia&isbn=&jtitle=Decision+Support+Systems&btitle=&rft_id=info:eric/2020-16408-001&rft_id=info:doi/10.1016%2Fj.dss.2020.113251,1,1,1,"decision-making, using the newsvendor problem. Ps have to figure out how to maximize profits given demand and per-unit cost.",NA,NA,NA,Xiaojing Feng and Jia Gao,Is optimal recommendation the best? A laboratory investigation under the newsvendor problem - ScienceDirect,"0167-9236, 0167-9236",NA,Decision Support Systems,"4120:Artificial Intelligence & Expert Systems,Adulthood (18 yrs & older),Algorithm aversion,Algorithms,Aversion,Behavioral operations management,Decision Support Systems,Decision Theory,Decision support system,Empirical Study,Human,Mathematical Model,Newsvendor,Quantitative Study,Regret,Regret aversion,article",4,"Copyright - © 2020, Elsevier B.V. All rights reserved.",13,2535406793; 2020-16408-001,Elsevier Science,131,https://www.proquest.com/scholarly-journals/is-optimal-recommendation-best-laboratory/docview/2535406793/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Is+optimal+recommendation+the+best%3F+A+laboratory+investigation+under+the+newsvendor+problem&title=Decision+Support+Systems&issn=01679236&date=2020-04-01&volume=131&issue=&spage=&au=Feng%2C+Xiaojing%3BGao%2C+Jia&isbn=&jtitle=Decision+Support+Systems&btitle=&rft_id=info:eric/2020-16408-001&rft_id=info:doi/10.1016%2Fj.dss.2020.113251,2020,NA,NA
ID-021,article,A systematic review of algorithm aversion in augmented decision making,"Despite abundant literature theorizing societal implications of algorithmic decision making, relatively little is known about the conditions that lead to the acceptance or rejection of algorithmically generated insights by individual users of decision aids. More specifically, recent findings of algorithm aversion—the reluctance of human forecasters to use superior but imperfect algorithms—raise questions about whether joint human‐algorithm decision making is feasible in practice. In this paper, we systematically review the topic of algorithm aversion as it appears in 61 peer‐reviewed articles between 1950 and 2018 and follow its conceptual trail across disciplines. We categorize and report on the proposed causes and solutions of algorithm aversion in five themes: expectations and expertise, decision autonomy, incentivization, cognitive compatibility, and divergent rationalities. Although each of the presented themes addresses distinct features of an algorithmic decision aid, human users of the decision aid, and/or the decision making environment, apparent interdependencies are highlighted. We conclude that resolving algorithm aversion requires an updated research program with an emphasis on theory integration. We provide a number of empirical questions that can be immediately carried forth by the behavioral decision making community. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/systematic-review-algorithm-aversion-augmented/docview/2309524973/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=A+systematic+review+of+algorithm+aversion+in+augmented+decision+making&title=Journal+of+Behavioral+Decision+Making&issn=08943257&date=2020-04-01&volume=33&issue=2&spage=220&au=Burton%2C+Jason+W.%3BStein%2C+Mari%E2%80%90Klara%3BJensen%2C+Tina+Blegind&isbn=&jtitle=Journal+of+Behavioral+Decision+Making&btitle=&rft_id=info:eric/2019-64518-001&rft_id=info:doi/10.1002%2Fbdm.2155,1,3,2,NA,NA,NA,"clearly defines the scope and what counts as algorithm: ""Hence, the notion of algorithmic decision making is considered an umbrella term for related paradigms like augmented decision making, decision aids, decision support systems, expert systems, decision formulas, computerized aids, and diagnostic aids. Likewise, variations of decision making, judgement, forecasting, and prediction are considered equivalent for the purpose of this review.""",Jason W Burton and Mari‐Klara Stein and Tina Blegind Jensen,https://doi.org/10.1002/bdm.2155,"0894-3257, 0894-3257",2,Journal of Behavioral Decision Making,"4120:Artificial Intelligence & Expert Systems,Algorithms,Decision Making,Decision Support Systems,Decision Theory,Human,Human Computer Interaction,Intelligent Agents,Literature Review,Systematic Review,algorithm aversion,article,augmented decision making,human‐algorithm interaction,systematic review",4,"Copyright - © 2019, John Wiley & Sons, Ltd.",220-239,2309524973; 2019-64518-001,John Wiley & Sons,33,https://www.proquest.com/scholarly-journals/systematic-review-algorithm-aversion-augmented/docview/2309524973/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=A+systematic+review+of+algorithm+aversion+in+augmented+decision+making&title=Journal+of+Behavioral+Decision+Making&issn=08943257&date=2020-04-01&volume=33&issue=2&spage=220&au=Burton%2C+Jason+W.%3BStein%2C+Mari%E2%80%90Klara%3BJensen%2C+Tina+Blegind&isbn=&jtitle=Journal+of+Behavioral+Decision+Making&btitle=&rft_id=info:eric/2019-64518-001&rft_id=info:doi/10.1002%2Fbdm.2155,2020,NA,NA
ID-022,article,Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them,"Although evidence-based algorithms consistently outperform human forecasters, people often fail to use them after learning that they are imperfect, a phenomenon known as algorithm aversion. In this paper, we present three studies investigating how to reduce algorithm aversion. In incentivized forecasting tasks, participants chose between using their own forecasts or those of an algorithm that was built by experts. Participants were considerably more likely to choose to use an imperfect algorithm when they could modify its forecasts, and they performed better as a result. Notably, the preference for modifiable algorithms held even when participants were severely restricted in the modifications they could make (Studies 1–3). In fact, our results suggest that participants’ preference for modifiable algorithms was indicative of a desire for some control over the forecasting outcome, and not for a desire for greater control over the forecasting outcome, as participants’ preference for modifiable algorithms was relatively insensitive to the magnitude of the modifications they were able to make (Study 2). Additionally, we found that giving participants the freedom to modify an imperfect algorithm made them feel more satisfied with the forecasting process, more likely to believe that the algorithm was superior, and more likely to choose to use an algorithm to make subsequent forecasts (Study 3). This research suggests that one can reduce algorithm aversion by giving people some control—even a slight amount—over an imperfect algorithm’s forecast.",https://www.proquest.com/scholarly-journals/overcoming-algorithm-aversion-people-will-use/docview/2167953950/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Overcoming+algorithm+aversion%3A+People+will+use+imperfect+algorithms+if+they+can+%28even+slightly%29+modify+them&title=Management+Science&issn=00251909&date=2018-03-01&volume=64&issue=3&spage=1155&au=Dietvorst%2C+Berkeley+J.%3BSimmons%2C+Joseph+P.%3BMassey%2C+Cade&isbn=&jtitle=Management+Science&btitle=&rft_id=info:eric/2018-16717-004&rft_id=info:doi/10.1287%2Fmnsc.2016.2643,1,1,1,NA,NA,NA,NA,Berkeley J Dietvorst and Joseph P Simmons and Cade Massey,https://doi.org/10.1287/mnsc.2016.2643,"0025-1909, 0025-1909",3,Management Science,"2340:Cognitive Processes,Adulthood (18 yrs & older),Algorithms,Aversion,Decision Making,Empirical Study,Female,Human,Male,Quantitative Study,US,algorithm aversion,article,decision making,forecasting",3,"Copyright - © 2016, INFORMS",1155-1170,2167953950; 2018-16717-004,Institute for Operations Research & the Management Sciences (INFORMS),64,https://www.proquest.com/scholarly-journals/overcoming-algorithm-aversion-people-will-use/docview/2167953950/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Overcoming+algorithm+aversion%3A+People+will+use+imperfect+algorithms+if+they+can+%28even+slightly%29+modify+them&title=Management+Science&issn=00251909&date=2018-03-01&volume=64&issue=3&spage=1155&au=Dietvorst%2C+Berkeley+J.%3BSimmons%2C+Joseph+P.%3BMassey%2C+Cade&isbn=&jtitle=Management+Science&btitle=&rft_id=info:eric/2018-16717-004&rft_id=info:doi/10.1287%2Fmnsc.2016.2643,2018,NA,NA
ID-023,article,Algorithm aversion: People erroneously avoid algorithms after seeing them err,"Research shows that evidence-based algorithms more accurately predict the future than do human forecasters. Yet when forecasters are deciding whether to use a human forecaster or a statistical algorithm, they often choose the human forecaster. This phenomenon, which we call algorithm aversion, is costly, and it is important to understand its causes. We show that people are especially averse to algorithmic forecasters after seeing them perform, even when they see them outperform a human forecaster. This is because people more quickly lose confidence in algorithmic than human forecasters after seeing them make the same mistake. In 5 studies, participants either saw an algorithm make forecasts, a human make forecasts, both, or neither. They then decided whether to tie their incentives to the future predictions of the algorithm or the human. Participants who saw the algorithm perform were less confident in it, and less likely to choose it over an inferior human forecaster. This was true even among those who saw the algorithm outperform the human. (PsycINFO Database Record (c) 2016 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/algorithm-aversion-people-erroneously-avoid/docview/1627949818/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Algorithm+aversion%3A+People+erroneously+avoid+algorithms+after+seeing+them+err&title=Journal+of+Experimental+Psychology%3A+General&issn=00963445&date=2015-02-01&volume=144&issue=1&spage=114&au=Dietvorst%2C+Berkeley+J.%3BSimmons%2C+Joseph+P.%3BMassey%2C+Cade&isbn=&jtitle=Journal+of+Experimental+Psychology%3A+General&btitle=&rft_id=info:eric/2014-48748-001&rft_id=info:doi/10.1037%2Fxge0000033,1,1,1,predictions,failure,NA,NA,Berkeley J Dietvorst and Joseph P Simmons and Cade Massey,https://doi.org/10.1037/xge0000033,"0096-3445, 0096-3445",1,Journal of Experimental Psychology: General,"2300:Human Experimental Psychology,Adulthood (18 yrs & older),Algorithms,Aversion,Avoidance Learning,Comprehension,Culture,Decision Making,Empirical Study,Female,Forecasting,Heuristics,Human,Humans,Male,Motivation,Quantitative Study,Self-Confidence,US,Young Adult,Young Adulthood (18-29 yrs),article,confidence,decision aids,decision making,forecasting,heuristics and biases",2,"Copyright - © 2014, American Psychological Association",114-126,1627949818; 2014-48748-001,American Psychological Association Psychological Review Company,144,https://www.proquest.com/scholarly-journals/algorithm-aversion-people-erroneously-avoid/docview/1627949818/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Algorithm+aversion%3A+People+erroneously+avoid+algorithms+after+seeing+them+err&title=Journal+of+Experimental+Psychology%3A+General&issn=00963445&date=2015-02-01&volume=144&issue=1&spage=114&au=Dietvorst%2C+Berkeley+J.%3BSimmons%2C+Joseph+P.%3BMassey%2C+Cade&isbn=&jtitle=Journal+of+Experimental+Psychology%3A+General&btitle=&rft_id=info:eric/2014-48748-001&rft_id=info:doi/10.1037%2Fxge0000033,2015,NA,NA
ID-025,article,Algorithm appreciation: People prefer algorithmic to human judgment,"Even though computational algorithms often outperform human judgment, received wisdom suggests that people may be skeptical of relying on them (Dawes, 1979). Counter to this notion, results from six experiments show that lay people adhere more to advice when they think it comes from an algorithm than from a person. People showed this effect, what we call algorithm appreciation, when making numeric estimates about a visual stimulus (Experiment 1A) and forecasts about the popularity of songs and romantic attraction (Experiments 1B and 1C). Yet, researchers predicted the opposite result (Experiment 1D). Algorithm appreciation persisted when advice appeared jointly or separately (Experiment 2). However, algorithm appreciation waned when: people chose between an algorithm’s estimate and their own (versus an external advisor’s; Experiment 3) and they had expertise in forecasting (Experiment 4). Paradoxically, experienced professionals, who make forecasts on a regular basis, relied less on algorithmic advice than lay people did, which hurt their accuracy. These results shed light on the important question of when people rely on algorithmic advice over advice from people and have implications for the use of “big data” and algorithmic advice it generates.",https://www.proquest.com/scholarly-journals/algorithm-appreciation-people-prefer-algorithmic/docview/2202920929/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Algorithm+appreciation%3A+People+prefer+algorithmic+to+human+judgment&title=Organizational+Behavior+and+Human+Decision+Processes&issn=07495978&date=2019-03-01&volume=151&issue=&spage=90&au=Logg%2C+Jennifer+M.%3BMinson%2C+Julia+A.%3BMoore%2C+Don+A.&isbn=&jtitle=Organizational+Behavior+and+Human+Decision+Processes&btitle=&rft_id=info:eric/2019-16202-008&rft_id=info:doi/10.1016%2Fj.obhdp.2018.12.005,1,1,1,"visual estimation, predictions about song popularity and romantic attraction",NA,NA,NA,Jennifer M Logg and Julia A Minson and Don A Moore,https://doi.org/10.1016/j.obhdp.2018.12.005,"0749-5978, 0749-5978",NA,Organizational Behavior and Human Decision Processes,"2340:Cognitive Processes,Adulthood (18 yrs & older),Algorithms,Empirical Study,Female,Human,Judgment,Male,Preferences,Quantitative Study,US,advice taking,algorithm appreciation,algorithms,article,human judgment",3,"Copyright - © 2018, Elsevier Inc. All rights reserved.",90-103,2202920929; 2019-16202-008,Elsevier Science,151,https://www.proquest.com/scholarly-journals/algorithm-appreciation-people-prefer-algorithmic/docview/2202920929/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Algorithm+appreciation%3A+People+prefer+algorithmic+to+human+judgment&title=Organizational+Behavior+and+Human+Decision+Processes&issn=07495978&date=2019-03-01&volume=151&issue=&spage=90&au=Logg%2C+Jennifer+M.%3BMinson%2C+Julia+A.%3BMoore%2C+Don+A.&isbn=&jtitle=Organizational+Behavior+and+Human+Decision+Processes&btitle=&rft_id=info:eric/2019-16202-008&rft_id=info:doi/10.1016%2Fj.obhdp.2018.12.005,2019,NA,NA
ID-031,article,Applicants’ fairness perceptions of algorithm-driven hiring procedures,"Despite the rapid adoption of technology in human resource departments, there is little empirical work that examines the potential challenges of algorithmic decision-making in the recruitment process. In this paper, we take the perspective of job applicants and examine how they perceive the use of algorithms in selection and recruitment. Across four studies on Amazon Mechanical Turk, we show that people in the role of a job applicant perceive algorithm-driven recruitment processes as less fair compared to human only or algorithm-assisted human processes. This effect persists regardless of whether the outcome is favorable to the applicant or not. A potential mechanism underlying algorithm resistance is the belief that algorithms will not be able to recognize their uniqueness as a candidate. Although the use of algorithms has several benefits for organizations such as improved efficiency and bias reduction, our results highlight a potential cost of using them to screen potential employees during recruitment. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/applicants-fairness-perceptions-algorithm-driven/docview/2766114452/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Applicants%26rsquo%3B+fairness+perceptions+of+algorithm-driven+hiring+procedures&title=Journal+of+Business+Ethics&issn=01674544&date=2023-01-12&volume=&issue=&spage=&au=Lavanchy%2C+Maude%3BReichert%2C+Patrick%3BNarayanan%2C+Jayanth%3BSavani%2C+Krishna&isbn=&jtitle=Journal+of+Business+Ethics&btitle=&rft_id=info:eric/2023-38457-001&rft_id=info:doi/10.1007%2Fs10551-022-05320-w,1,1,1,"recruiting, employee screening",ability to recognize candidate uniqueness,NA,NA,Maude Lavanchy and Patrick Reichert and Jayanth Narayanan and Krishna Savani,https://doi.org/10.1007/s10551-022-05320-w,"0167-4544, 0167-4544",NA,Journal of Business Ethics,"3600:Organizational Psychology & Human Resources,Algorithms,Applicant reactions to selection,Fairness,J20,L20,M12,M51,No terms assigned,O15,Organizational justice,Recruitment,Selection,article",1,"Copyright - © 2023, The Author(s), under exclusive licence to Springer Nature B.V. . Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.",NA,2766114452; 2023-38457-001,Springer,NA,https://www.proquest.com/scholarly-journals/applicants-fairness-perceptions-algorithm-driven/docview/2766114452/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Applicants%26rsquo%3B+fairness+perceptions+of+algorithm-driven+hiring+procedures&title=Journal+of+Business+Ethics&issn=01674544&date=2023-01-12&volume=&issue=&spage=&au=Lavanchy%2C+Maude%3BReichert%2C+Patrick%3BNarayanan%2C+Jayanth%3BSavani%2C+Krishna&isbn=&jtitle=Journal+of+Business+Ethics&btitle=&rft_id=info:eric/2023-38457-001&rft_id=info:doi/10.1007%2Fs10551-022-05320-w,2023,NA,NA
ID-040,article,Who made that decision and why? Users’ perceptions of human versus ai decision-making and the power of explainable-ai,"With the advent of artificial intelligence (AI) based systems, a new era has begun. Decisions that were once made by humans are now increasingly being made by these advanced systems, with the inevitable consequence of our growing reliance on AI in many aspects of our lives. At the same time, the opaque nature of AI-based systems and the possibility of unintentional or hidden discriminatory practices and biases raises profound questions not only about the mechanics of AI, but also about how users perceive the fairness of these systems. We hypothesize that providing various explanations for AI decision-making processes and output may enhance users’ fairness perceptions and make them trust the system and adopt its decisions. Hence, we devised an online between-subject experiment that explores users’ fairness and comprehension perceptions of AI systems with respect to the explanations provided by the system, employing a case study of a managerial decision in the human resources (HR) domain. We manipulated (i) the decision-maker (AI or human); (ii) the input (candidate characteristics); (iii) the output (recommendation valence), and (iv) the explanation style. We examined the effect of the various manipulations (and individuals’ demographic and personality characteristics) using multivariate ordinal regression. We also performed a multi-level analysis of experiment components to examine the effects of the decision-maker type, explanation style, and their combination. The results suggest three main conclusions. The first conclusion is that there is a gap in users’ fairness and comprehension perception of AI-based decision making systems compared to human decision making. The second conclusion is that knowing that an AI-based system provided the decisions negatively affects users’ fairness and comprehension perceptions, compared to knowing that humans made the decision. Finally, the third conclusion is that providing case-based, certification-based, or sensitivity-based explanations can narrow this gap and may even eliminate it. Additionally, we found that users’ fairness and comprehension perceptions are influenced by a variety of factors such as the input, output, and explanation provided by the system, as well as by individuals’ age, education, computer skills, and personality. Our findings may help to understand when and how to use explanations to improve users’ perceptions regarding AI-based decision-making. CCS CONCEPTS • Human computer interaction (HCI) → HCI design and evaluation methods → User studies • Human-centered computing → Human computer interaction (HCI) → Empirical studies in HCI • Applied computing → Law, social and behavioral sciences → Sociology (PsycInfo Database Record (c) 2024 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/who-made-that-decision-why-users-perceptions/docview/3062248219/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Who+made+that+decision+and+why%3F+Users%26rsquo%3B+perceptions+of+human+versus+ai+decision-making+and+the+power+of+explainable-ai&title=International+Journal+of+Human-Computer+Interaction&issn=10447318&date=2024-05-18&volume=&issue=&spage=&au=Shulner-Tal%2C+Avital%3BKuflik%2C+Tsvi%3BKliger%2C+Doron%3BMancini%2C+Azzurra&isbn=&jtitle=International+Journal+of+Human-Computer+Interaction&btitle=&rft_id=info:eric/2024-87273-001&rft_id=info:doi/10.1080%2F10447318.2024.2348843,1,1,1,NA,NA,NA,NA,Avital Shulner-Tal and Tsvi Kuflik and Doron Kliger and Azzurra Mancini,https://doi.org/10.1080/10447318.2024.2348843,"1044-7318, 1044-7318",NA,International Journal of Human-Computer Interaction,"4120:Artificial Intelligence & Expert Systems,Artificial Intelligence,Decision Making,Empirical Study,Fairness,Human Computer Interaction,Quantitative Study,XAI,article,behavioral economics,decision making processes,decision making systems,explainability,intelligent systems,users’ perceptions",5,"Copyright - © 2024, The Author(s)",NA,3062248219; 2024-87273-001,Taylor & Francis Lawrence Erlbaum,NA,https://www.proquest.com/scholarly-journals/who-made-that-decision-why-users-perceptions/docview/3062248219/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Who+made+that+decision+and+why%3F+Users%26rsquo%3B+perceptions+of+human+versus+ai+decision-making+and+the+power+of+explainable-ai&title=International+Journal+of+Human-Computer+Interaction&issn=10447318&date=2024-05-18&volume=&issue=&spage=&au=Shulner-Tal%2C+Avital%3BKuflik%2C+Tsvi%3BKliger%2C+Doron%3BMancini%2C+Azzurra&isbn=&jtitle=International+Journal+of+Human-Computer+Interaction&btitle=&rft_id=info:eric/2024-87273-001&rft_id=info:doi/10.1080%2F10447318.2024.2348843,2024,NA,NA
ID-043,article,Choosing human over AI doctors? How comparative trust associations and knowledge relate to risk and benefit perceptions of AI in healthcare,"The development of artificial intelligence (AI) in healthcare is accelerating rapidly. Beyond the urge for technological optimization, public perceptions and preferences regarding the application of such technologies remain poorly understood. Risk and benefit perceptions of novel technologies are key drivers for successful implementation. Therefore, it is crucial to understand the factors that condition these perceptions. In this study, we draw on the risk perception and human‐AI interaction literature to examine how explicit (i.e., deliberate) and implicit (i.e., automatic) comparative trust associations with AI versus physicians, and knowledge about AI, relate to likelihood perceptions of risks and benefits of AI in healthcare and preferences for the integration of AI in healthcare. We use survey data (n = 378) to specify a path model. Results reveal that the path for implicit comparative trust associations on relative preferences for AI over physicians is only significant through risk, but not through benefit perceptions. This finding is reversed for AI knowledge. Explicit comparative trust associations relate to AI preference through risk and benefit perceptions. These findings indicate that risk perceptions of AI in healthcare might be driven more strongly by affect‐laden factors than benefit perceptions, which in turn might depend more on reflective cognition. Implications of our findings and directions for future research are discussed considering the conceptualization of trust as heuristic and dual‐process theories of judgment and decision‐making. Regarding the design and implementation of AI‐based healthcare technologies, our findings suggest that a holistic integration of public viewpoints is warranted. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/choosing-human-over-ai-doctors-how-comparative/docview/3058995617/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Choosing+human+over+AI+doctors%3F+How+comparative+trust+associations+and+knowledge+relate+to+risk+and+benefit+perceptions+of+AI+in+healthcare&title=Risk+Analysis&issn=02724332&date=2024-04-01&volume=44&issue=4&spage=939&au=Kerstan%2C+Sophie%3BBienefeld%2C+Nadine%3BGrote%2C+Gudela&isbn=&jtitle=Risk+Analysis&btitle=&rft_id=info:eric/2024-73239-013&rft_id=info:doi/10.1111%2Frisa.14216,1,1,1,NA,NA,NA,NA,Sophie Kerstan and Nadine Bienefeld and Gudela Grote,https://doi.org/10.1111/risa.14216,"0272-4332, 0272-4332",4,Risk Analysis,"3410:Professional Education & Training,AI in healthcare,AI knowledge,Adulthood (18 yrs & older),Aged (65 yrs & older),Artificial Intelligence,Empirical Study,Explicit Attitudes,Female,Human,Implicit Attitudes,Male,Middle Age (40-64 yrs),Physicians,Preferences,Quantitative Study,Risk Perception,Thirties (30-39 yrs),Trust (Social Behavior),US,Young Adulthood (18-29 yrs),article,trust associations",4,"Copyright - © 2023, The Authors—Risk Analysis. Published by Wiley Periodicals LLC on behalf of Society for Risk Analysis. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.",939-957,3058995617; 2024-73239-013,Wiley-Blackwell Publishing Ltd. Blackwell Publishing,44,https://www.proquest.com/scholarly-journals/choosing-human-over-ai-doctors-how-comparative/docview/3058995617/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Choosing+human+over+AI+doctors%3F+How+comparative+trust+associations+and+knowledge+relate+to+risk+and+benefit+perceptions+of+AI+in+healthcare&title=Risk+Analysis&issn=02724332&date=2024-04-01&volume=44&issue=4&spage=939&au=Kerstan%2C+Sophie%3BBienefeld%2C+Nadine%3BGrote%2C+Gudela&isbn=&jtitle=Risk+Analysis&btitle=&rft_id=info:eric/2024-73239-013&rft_id=info:doi/10.1111%2Frisa.14216,2024,NA,NA
ID-050,article,Employees adhere more to unethical instructions from human than AI supervisors: Complementing experimental evidence with machine learning,"The role of artificial intelligence (AI) in organizations has fundamentally changed from performing routine tasks to supervising human employees. While prior studies focused on normative perceptions of such (n= 1701) and used two state-of-the-art machine learning algorithms (causal forest and transformers). We consistently find that employees adhere less to unethical instructions from an AI than a human supervisor. Further, individual characteristics such as the tendency to comply without dissent or age constitute important boundary conditions. In addition, Study 1 identified that the perceived mind of the supervisors serves as an explanatory mechanism. We generate further insights on this mediator via experimental manipulations in two pre-registered studies by manipulating mind between two AI (Study 2) and two human supervisors (Study 3). In (pre-registered) Study 4, we replicate the resistance to unethical instructions from AI supervisors in an incentivized experimental setting. Our research generates insights into the ‘black box’ of human behavior toward AI supervisors, particularly in the moral domain, and showcases how organizational researchers can use machine learning methods as powerful tools to complement experimental research for the generation of more fine-grained insights. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/employees-adhere-more-unethical-instructions/docview/2794797568/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Employees+adhere+more+to+unethical+instructions+from+human+than+AI+supervisors%3A+Complementing+experimental+evidence+with+machine+learning&title=Journal+of+Business+Ethics&issn=01674544&date=2024-01-01&volume=189&issue=3&spage=625&au=Lanz%2C+Lukas%3BBriker%2C+Roman%3BGerpott%2C+Fabiola+H.&isbn=&jtitle=Journal+of+Business+Ethics&btitle=&rft_id=info:eric/2023-59430-001&rft_id=info:doi/10.1007%2Fs10551-023-05393-1,1,1,1,NA,perceived mind,NA,NA,Lukas Lanz and Roman Briker and Fabiola H Gerpott,https://doi.org/10.1007/s10551-023-05393-1,"0167-4544, 0167-4544",3,Journal of Business Ethics,"3660:Organizational Behavior,AI leadership,Adulthood (18 yrs & older),Artificial Intelligence,Compliance,Empirical Study,Employee Attitudes,Ethics,Female,Human,Leadership,Male,Management Personnel,Mind,Quantitative Study,Unethical leadership,article,artificial intelligence,perceived mind",1,"Copyright - © 2023, The Author(s). This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.",625-646,2794797568; 2023-59430-001,Springer,189,https://www.proquest.com/scholarly-journals/employees-adhere-more-unethical-instructions/docview/2794797568/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Employees+adhere+more+to+unethical+instructions+from+human+than+AI+supervisors%3A+Complementing+experimental+evidence+with+machine+learning&title=Journal+of+Business+Ethics&issn=01674544&date=2024-01-01&volume=189&issue=3&spage=625&au=Lanz%2C+Lukas%3BBriker%2C+Roman%3BGerpott%2C+Fabiola+H.&isbn=&jtitle=Journal+of+Business+Ethics&btitle=&rft_id=info:eric/2023-59430-001&rft_id=info:doi/10.1007%2Fs10551-023-05393-1,2024,NA,NA
ID-053,article,Eyes can tell: Assessment of implicit attitudes toward AI art,"Advances in artificial intelligence (AI) have significantly improved the abilities of machines. Human-unique abilities, such as art creation, are now being challenged by AI. Recent studies have investigated and compared people's attitudes toward human-made and AI-generated artworks. These results suggest that a negative bias may exist toward the latter. However, none of these previous studies has examined the extent of this bias. In this study, we investigate whether a bias against AI art can be found at an implicit level. Viewers’ attitudes toward AI art were assessed using eye-tracking measures and subjective aesthetic evaluations. Visual attention and aesthetic judgments were compared between artworks categorized as human-made and AI-made. The results showed that although it was difficult for individuals to identify AI-generated artwork, they exhibited an implicit prejudice against AI art. Participants looked longer at paintings that they thought were made by humans. No significant effect of categorization of paintings was found in subjective evaluations. These findings suggest that although human and AI art may be perceived as having similar aesthetic values, an implicit negative bias toward AI art exists. Although AI can now perform creative tasks, artistic creativity is still considered a human prerogative. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/eyes-can-tell-assessment-implicit-attitudes/docview/2895387082/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Eyes+can+tell%3A+Assessment+of+implicit+attitudes+toward+AI+art&title=i-Perception&issn=2041-6695&date=2023-10-30&volume=14&issue=5&spage=&au=Zhou%2C+Yizhen%3BKawabata%2C+Hideaki&isbn=&jtitle=i-Perception&btitle=&rft_id=info:eric/2024-23457-001&rft_id=info:doi/10.1177%2F20416695231209846,1,1,1,visual art,NA,NA,NA,Yizhen Zhou and Hideaki Kawabata,https://doi.org/10.1177/20416695231209846,NA,5,i-Perception,"4120:Artificial Intelligence & Expert Systems,AI art,Adulthood (18 yrs & older),Aesthetics,Art,Artificial Intelligence,Cognitive Bias,Computer Attitudes,Creativity,Empirical Study,Female,Human,Implicit Attitudes,Japan,Male,Painting (Art),Quantitative Study,aesthetic evaluation,article,artificial intelligence,attitudes,creativity,implicit attitudes,negative bias,visual attention",10,"Copyright - © 2023, The Author(s)",14,2895387082; 2024-23457-001,Sage Publications Pion,14,https://www.proquest.com/scholarly-journals/eyes-can-tell-assessment-implicit-attitudes/docview/2895387082/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Eyes+can+tell%3A+Assessment+of+implicit+attitudes+toward+AI+art&title=i-Perception&issn=2041-6695&date=2023-10-30&volume=14&issue=5&spage=&au=Zhou%2C+Yizhen%3BKawabata%2C+Hideaki&isbn=&jtitle=i-Perception&btitle=&rft_id=info:eric/2024-23457-001&rft_id=info:doi/10.1177%2F20416695231209846,2023,NA,NA
ID-054,article,Thinking about God increases acceptance of artificial intelligence in decision-making,"Thinking about God promotes greater acceptance of Artificial intelligence (AI)-based recommendations. Eight preregistered experiments (n = 53,563) corroborates the experimental results with data from 21 countries on the usage of robo-advisors in financial decision-making. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/thinking-about-god-increases-acceptance/docview/2889363823/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Thinking+about+God+increases+acceptance+of+artificial+intelligence+in+decision-making&title=PNAS+Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&issn=00278424&date=2023-08-15&volume=120&issue=33&spage=1&au=Karata%C5%9F%2C+Mustafa%3BCutright%2C+Keisha+M.&isbn=&jtitle=PNAS+Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&btitle=&rft_id=info:eric/2024-05973-006&rft_id=info:doi/10.1073%2Fpnas.2218961120,1,1,1,finances,NA,NA,NA,Mustafa Karataş and Keisha M Cutright,https://doi.org/10.1073/pnas.2218961120,"0027-8424, 0027-8424",33,PNAS Proceedings of the National Academy of Sciences of the United States of America,"2340:Cognitive Processes,2920:Religion,Adulthood (18 yrs & older),Artificial Intelligence,Decision Making,Empirical Study,Female,God Concepts,Human,Male,Quantitative Study,Religion,US,algorithm aversion,article,artificial intelligence,decision-making,religion",8,"Copyright - © 2023, The Author(s). Published by PNAS. This article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).",1-10,2889363823; 2024-05973-006,National Academy of Sciences,120,https://www.proquest.com/scholarly-journals/thinking-about-god-increases-acceptance/docview/2889363823/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Thinking+about+God+increases+acceptance+of+artificial+intelligence+in+decision-making&title=PNAS+Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&issn=00278424&date=2023-08-15&volume=120&issue=33&spage=1&au=Karata%C5%9F%2C+Mustafa%3BCutright%2C+Keisha+M.&isbn=&jtitle=PNAS+Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&btitle=&rft_id=info:eric/2024-05973-006&rft_id=info:doi/10.1073%2Fpnas.2218961120,2023,NA,NA
ID-065,article,Better explaining the benefits why ai? Analyzing the impact of explaining the benefits of ai‐supported selection on applicant responses,"Despite the increasing popularity of AI‐supported selection tools, knowledge about the actions that can be taken by organizations to increase AI acceptance is still in its infancy, even though multiple studies point out that applicants react negatively to the implementation of AI‐supported selection tools. Therefore, this study investigates ways to alter applicant reactions to AI‐supported selection. Using a scenario‐based between‐subject design with participants from the working population (N = 200), we varied the information provided by the organization about the reasons for using an AI‐supported selection process (no additional information vs. written information vs. video information) in comparison to a human selection process. Results show that the use of AI without information and with written information decreased perceived fairness, personableness perception, and increased emotional creepiness. In turn, perceived fairness, personableness perceptions, and emotional creepiness mediated the association between an AI‐supported selection process, organizational attractiveness, and the intention to further proceed with the selection process. Moreover, results did not differ for applicants who were provided video explanations of the benefits of AI‐supported selection tools and those who participated in an actual human selection process. Important implications for research and practice are discussed. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/better-explaining-benefits-why-ai-analyzing/docview/2756716290/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Better+explaining+the+benefits+why+ai%3F+Analyzing+the+impact+of+explaining+the+benefits+of+ai%E2%80%90supported+selection+on+applicant+responses&title=International+Journal+of+Selection+and+Assessment&issn=0965075X&date=2022-12-16&volume=&issue=&spage=&au=K%C3%B6chling%2C+Alina%3BWehner%2C+Marius+Claus&isbn=&jtitle=International+Journal+of+Selection+and+Assessment&btitle=&rft_id=info:eric/2023-31185-001&rft_id=info:doi/10.1111%2Fijsa.12412,1,1,1,NA,NA,NA,NA,Alina Köchling and Marius Claus Wehner,https://doi.org/10.1111/ijsa.12412,"0965-075X, 0965-075X",NA,International Journal of Selection and Assessment,"2200:Psychometrics & Statistics & Methodology,AI‐supported selection,No terms assigned,applicant reactions,article,emotional creepiness,fairness,organizational attractiveness,perceptions",12,"Copyright - © 2022, The Authors.",NA,2756716290; 2023-31185-001,Wiley-Blackwell Publishing Ltd. Blackwell Publishing,NA,https://www.proquest.com/scholarly-journals/better-explaining-benefits-why-ai-analyzing/docview/2756716290/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Better+explaining+the+benefits+why+ai%3F+Analyzing+the+impact+of+explaining+the+benefits+of+ai%E2%80%90supported+selection+on+applicant+responses&title=International+Journal+of+Selection+and+Assessment&issn=0965075X&date=2022-12-16&volume=&issue=&spage=&au=K%C3%B6chling%2C+Alina%3BWehner%2C+Marius+Claus&isbn=&jtitle=International+Journal+of+Selection+and+Assessment&btitle=&rft_id=info:eric/2023-31185-001&rft_id=info:doi/10.1111%2Fijsa.12412,2022,NA,NA
ID-067,article,Why do users trust algorithms? A review and conceptualization of initial trust and trust over time,"Algorithms are increasingly playing a pivotal role in organizations' day-to-day operations; however, a general distrust of artificial intelligence-based algorithms and automated processes persists. This aversion to algorithms raises questions about the drivers that lead managers to trust or reject their use. This conceptual paper aims to provide an integrated review of how users experience the encounter with AI-based algorithms over time. This is important for two reasons: first, their functional activities change over the course of time through machine learning; and second, users' trust develops with their level of knowledge of a particular algorithm. Based on our review, we propose an integrative framework to explain how users’ perceptions of trust change over time. This framework extends current understandings of trust in AI-based algorithms in two areas: First, it distinguishes between the formation of initial trust and trust over time in AI-based algorithms, and specifies the determinants of trust in each phase. Second, it links the transition between initial trust in AI-based algorithms and trust over time to representations of the technology as either human-like or system-like. Finally, it considers the additional determinants that intervene during this transition phase. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/why-do-users-trust-algorithms-review/docview/2869683074/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Why+do+users+trust+algorithms%3F+A+review+and+conceptualization+of+initial+trust+and+trust+over+time&title=European+Management+Journal&issn=02632373&date=2022-10-01&volume=40&issue=5&spage=685&au=Cabiddu%2C+Francesca%3BMoi%2C+Ludovica%3BPatriotta%2C+Gerardo%3BAllen%2C+David+G.&isbn=&jtitle=European+Management+Journal&btitle=&rft_id=info:eric/2023-18587-005&rft_id=info:doi/10.1016%2Fj.emj.2022.06.001,1,3,2,NA,NA,NA,RQ: How do trust perceptions change over time?,Francesca Cabiddu and Ludovica Moi and Gerardo Patriotta and David G Allen,https://doi.org/10.1016/j.emj.2022.06.001,"0263-2373, 0263-2373",5,European Management Journal,"3640:Management & Management Training,4120:Artificial Intelligence & Expert Systems,AI algorithms,Algorithms,Artificial Intelligence,Automation,Business Organizations,Human,Initial trust,Integrative review,Literature Review,Machine Learning,Management Personnel,Technology,Trust,Trust over time,article",10,"Copyright - © 2022, Elsevier Ltd. All rights reserved.",685-706,2869683074; 2023-18587-005,Elsevier Science,40,https://www.proquest.com/scholarly-journals/why-do-users-trust-algorithms-review/docview/2869683074/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Why+do+users+trust+algorithms%3F+A+review+and+conceptualization+of+initial+trust+and+trust+over+time&title=European+Management+Journal&issn=02632373&date=2022-10-01&volume=40&issue=5&spage=685&au=Cabiddu%2C+Francesca%3BMoi%2C+Ludovica%3BPatriotta%2C+Gerardo%3BAllen%2C+David+G.&isbn=&jtitle=European+Management+Journal&btitle=&rft_id=info:eric/2023-18587-005&rft_id=info:doi/10.1016%2Fj.emj.2022.06.001,2022,NA,NA
ID-068,article,The searching artificial intelligence: Consumers show less aversion to algorithm‐recommended search product,"Though artificial intelligence (AI) recommendation is a hot topic in recent marketing research, previous research has shown a convergent tendency for aversion to AI recommendation. It is imperative to find ways to promote AI usage and reduce consumers’ AI aversion. This study fills this void by exploring the effect of AI (vs. human) recommenders on consumers’ preferences for search versus experience products in the context of e‐commerce. Two studies provide convergent evidence that consumers show less avoidance of algorithms when recommending search products compared to experience products. A behavioral experiment (Study 1, n = 26) shows that consumers have a higher level of cognitive conflict (i.e., a larger magnitude of N2) when AI (vs. human) recommends experience products, while the effect disappears for search products. This paper shows that for search products, marketers can obtain similar evaluations using AI recommenders, which is relatively cheaper and more time‐saving compared with human recommenders. Therefore, our work provides important implications for theory and practice on e‐commerce and marketing communication. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/searching-artificial-intelligence-consumers-show/docview/2689191297/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=The+searching+artificial+intelligence%3A+Consumers+show+less+aversion+to+algorithm%E2%80%90recommended+search+product&title=Psychology+%26+Marketing&issn=07426046&date=2022-10-01&volume=39&issue=10&spage=1902&au=Xie%2C+Zhaohan%3BYu%2C+Yining%3BZhang%2C+Jing%3BChen%2C+Mingliang&isbn=&jtitle=Psychology+%26+Marketing&btitle=&rft_id=info:eric/2022-81286-001&rft_id=info:doi/10.1002%2Fmar.21706,1,NA,NA,NA,NA,NA,NA,Zhaohan Xie and Yining Yu and Jing Zhang and Mingliang Chen,https://doi.org/10.1002/mar.21706,"0742-6046, 0742-6046",10,Psychology & Marketing,"3920:Consumer Attitudes & Behavior,4120:Artificial Intelligence & Expert Systems,AI recommendation,Adulthood (18 yrs & older),Algorithms,Artificial Intelligence,China,Consumer Attitudes,Consumer Behavior,Electronic Commerce,Empirical Study,Evoked Potentials,Female,Human,Male,Marketing,Neurosciences,Quantitative Study,US,United Kingdom,article,communication,consumer neuroscience,event‐ related potential,search/experience product",10,"Copyright - © 2022, Wiley Periodicals LLC",1902-1919,2689191297; 2022-81286-001,John Wiley & Sons,39,https://www.proquest.com/scholarly-journals/searching-artificial-intelligence-consumers-show/docview/2689191297/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=The+searching+artificial+intelligence%3A+Consumers+show+less+aversion+to+algorithm%E2%80%90recommended+search+product&title=Psychology+%26+Marketing&issn=07426046&date=2022-10-01&volume=39&issue=10&spage=1902&au=Xie%2C+Zhaohan%3BYu%2C+Yining%3BZhang%2C+Jing%3BChen%2C+Mingliang&isbn=&jtitle=Psychology+%26+Marketing&btitle=&rft_id=info:eric/2022-81286-001&rft_id=info:doi/10.1002%2Fmar.21706,2022,NA,NA
ID-069,article,Buffer bots: The role of virtual service agents in mitigating negative effects when service fails,"In recent years, marketers have placed increased reliance upon artificial intelligence (AI) and, subsequently, the use of virtual agents in customer service contexts is on the rise. Despite such service digitalization, service can still fail. While there is an increasing literature on the effect of virtual agents in service settings, questions remain as to how customers react to service failure that results from interactions with virtual service agents. To this end, we deconstruct the effect of virtual agent service failure across two studies: one involving a process service failure and another involving an outcome service failure. We specifically manipulate the type of service agent that causes the service failure (human vs. virtual agent) and the magnitude of the failure (small vs. large). Results show that firms can leverage virtual service agents to mitigate or buffer the negative effects of service failure. From a managerial perspective, our findings suggest that firms could engage virtual service agents in situations where there may be a risk of outcome service failure—particularly in settings where relatively large magnitude failures may be experienced. In such a setting, we find that virtual service agents can mitigate the negative effects of service failure, more so than when the failure results from an interaction with a human service agent. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/buffer-bots-role-virtual-service-agents/docview/2711486972/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Buffer+bots%3A+The+role+of+virtual+service+agents+in+mitigating+negative+effects+when+service+fails&title=Psychology+%26+Marketing&issn=07426046&date=2022-09-07&volume=&issue=&spage=&au=Sands%2C+Sean%3BCampbell%2C+Colin%3BPlangger%2C+Kirk%3BPitt%2C+Leyland&isbn=&jtitle=Psychology+%26+Marketing&btitle=&rft_id=info:eric/2023-00574-001&rft_id=info:doi/10.1002%2Fmar.21723,1,1,1,marketing,NA,NA,NA,Sean Sands and Colin Campbell and Kirk Plangger and Leyland Pitt,https://doi.org/10.1002/mar.21723,"0742-6046, 0742-6046",NA,Psychology & Marketing,"3900:Consumer Psychology,No terms assigned,article,artificial intelligence (AI),human–machine interaction,service failure,service robots,virtual service agents",9,"Copyright - © 2022, The Authors.",NA,2711486972; 2023-00574-001,John Wiley & Sons,NA,https://www.proquest.com/scholarly-journals/buffer-bots-role-virtual-service-agents/docview/2711486972/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Buffer+bots%3A+The+role+of+virtual+service+agents+in+mitigating+negative+effects+when+service+fails&title=Psychology+%26+Marketing&issn=07426046&date=2022-09-07&volume=&issue=&spage=&au=Sands%2C+Sean%3BCampbell%2C+Colin%3BPlangger%2C+Kirk%3BPitt%2C+Leyland&isbn=&jtitle=Psychology+%26+Marketing&btitle=&rft_id=info:eric/2023-00574-001&rft_id=info:doi/10.1002%2Fmar.21723,2022,NA,NA
ID-076,article,Home-tutoring services assisted with technology: Investigating the role of artificial intelligence using a randomized field experiment,"Despite a rising interest in artificial intelligence (AI) technology, research in services marketing has not evaluated its role in helping firms learn about customers’ needs and increasing the adaptability of service employees. Therefore, the authors develop a conceptual framework and investigate whether and to what extent providing AI assistance to service employees improves service outcomes. The randomized controlled trial in the context of tutoring services shows that helping service employees (tutors) adapt to students’ learning needs by providing AI-generated diagnoses significantly improves service outcomes measured by academic performance. However, the authors find that some tutors may not utilize AI assistance (i.e., AI aversion), and factors associated with unforeseen barriers to usage (i.e., technology overload) can moderate its impact on outcomes. Interestingly, tutors who significantly contribute to the firm's revenue relied heavily on AI assistance but unexpectedly benefited little from AI in improving service outcomes. Given the wide applicability of AI assistance in a variety of services marketing contexts, the authors suggest that firms should consider the potential difficulties employees face in using the technology rather than encourage them to use it as it is. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",https://journals-sagepub-com.stanford.idm.oclc.org/doi/full/10.1177/00222437211050351,1,1,1,"tutoring, education",NA,NA,"clarification about the meaning of AI aversion: ""It is worth noting that the literature does not define AI aversion as users experiencing a “feeling” of aversion toward AI. Instead, AI aversion defined by users choosing not to use available AI technology, regardless of the feelings they might have.""",Jun Hyung Kim and Minki Kim and Do Won Kwak and Sol Lee,https://doi.org/10.1177/00222437211050351,"0022-2437, 0022-2437",1,Journal of Marketing Research,"3530:Curriculum & Programs & Teaching Methods,4120:Artificial Intelligence & Expert Systems,AI aversion,Academic Achievement,Adolescence (13-17 yrs),Artificial Intelligence,Aversion,Business Organizations,Childhood (birth-12 yrs),Empirical Study,Female,Field Study,Home Environment,Human,Interview,Korea,Male,Marketing,Mathematical Model,Observation Methods,Preschool Age (2-5 yrs),Quantitative Study,School Age (6-12 yrs),Teachers,Tutoring,article,artificial intelligence,field experiment,one-on-one tutoring services,technology overload",2,"Copyright - © 2021, American Marketing Association",79-96,2786224548; 2022-26028-005,Sage Publications American Marketing Association,59,https://www.proquest.com/scholarly-journals/home-tutoring-services-assisted-with-technology/docview/2786224548/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Home-tutoring+services+assisted+with+technology%3A+Investigating+the+role+of+artificial+intelligence+using+a+randomized+field+experiment&title=Journal+of+Marketing+Research&issn=00222437&date=2022-02-01&volume=59&issue=1&spage=79&au=Kim%2C+Jun+Hyung%3BKim%2C+Minki%3BKwak%2C+Do+Won%3BLee%2C+Sol&isbn=&jtitle=Journal+of+Marketing+Research&btitle=&rft_id=info:eric/2022-26028-005&rft_id=info:doi/10.1177%2F00222437211050351,2022,NA,NA
ID-080,article,Understanding the potential adoption of autonomous vehicles in China: The perspective of behavioral reasoning theory,"Powered by artificial intelligence (AI), autonomous vehicles (AVs) are one of the most disruptive technologies to reduce road incidents, improve road efficiency and enhance mobility for underserved populations. However, existing literature of AV adoption is either based on descriptive analysis or simple applications of conventional behavioral frameworks. Drawing on behavioral reasoning theory (BRT), we examine the effect of reasoning process on consumers' attitudes and intentions toward adopting AVs, and how their psychological traits moderates the relationships. By conducting a nationwide survey in China (n = 849), we identify positive (negative) effects of individuals' reasons for (against) adopting AVs on their adoption intentions for AVs, and positive influence of face consciousness on the reasoning process. More importantly, we find that the psychological trait of need for uniqueness strengthens the association between consumers' reasoning for AVs and their adoption intention, while the risk aversion trait intensifies the negative relationships between consumers' reasoning against AVs and their attitude/adoption intention. We contribute to the literature of AI adoption by establishing the important roles of consumers' reasons for and against such innovations. We also extend the BRT framework by identifying the roles of psychological traits in moderating the relationships among reasoning, attitude, and adoption intention. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/understanding-potential-adoption-autonomous/docview/2496342094/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Understanding+the+potential+adoption+of+autonomous+vehicles+in+China%3A+The+perspective+of+behavioral+reasoning+theory&title=Psychology+%26+Marketing&issn=07426046&date=2021-04-01&volume=38&issue=4&spage=669&au=Huang%2C+Youlin%3BQian%2C+Lixian&isbn=&jtitle=Psychology+%26+Marketing&btitle=&rft_id=info:eric/2021-22842-001&rft_id=info:doi/10.1002%2Fmar.21465,1,NA,NA,NA,NA,NA,need for uniqueness,Youlin Huang and Lixian Qian,https://doi.org/10.1002/mar.21465,"0742-6046, 0742-6046",4,Psychology & Marketing,"3920:Consumer Attitudes & Behavior,Adulthood (18 yrs & older),Aged (65 yrs & older),Autonomous Vehicles,China,Consumer Attitudes,Empirical Study,Female,Human,Innovation,Intention,Male,Middle Age (40-64 yrs),Models,Psychological Theories,Quantitative Study,Reasoning,Test Construction,Thirties (30-39 yrs),Very Old (85 yrs & older),Young Adulthood (18-29 yrs),article,autonomous vehicles,behavioral reasoning theory,consumer attitudes,consumer reasons,face consciousness,innovation adoption,integrated effects,intentions,psychological traits,reasoning process",4,"Copyright - © 2021, Wiley Periodicals LLC",669-690,2496342094; 2021-22842-001,John Wiley & Sons,38,https://www.proquest.com/scholarly-journals/understanding-potential-adoption-autonomous/docview/2496342094/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Understanding+the+potential+adoption+of+autonomous+vehicles+in+China%3A+The+perspective+of+behavioral+reasoning+theory&title=Psychology+%26+Marketing&issn=07426046&date=2021-04-01&volume=38&issue=4&spage=669&au=Huang%2C+Youlin%3BQian%2C+Lixian&isbn=&jtitle=Psychology+%26+Marketing&btitle=&rft_id=info:eric/2021-22842-001&rft_id=info:doi/10.1002%2Fmar.21465,2021,NA,NA
ID-081,article,Artificial intelligence coaches for sales agents: Caveats and solutions,"Firms are exploiting artificial intelligence (AI) coaches to provide training to sales agents and improve their job skills. The authors present several caveats associated with such practices based on a series of randomized field experiments. Experiment 1 shows that the incremental benefit of the AI coach over human managers is heterogeneous across agents in an inverted-U shape: whereas middle-ranked agents improve their performance by the largest amount, both bottom- and top-ranked agents show limited incremental gains. This pattern is driven by a learning-based mechanism in which bottom-ranked agents encounter the most severe information overload problem with the AI versus human coach, while top-ranked agents hold the strongest aversion to the AI relative to a human coach. To alleviate the challenge faced by bottom-ranked agents, Experiment 2 redesigns the AI coach by restricting the training feedback level and shows a significant improvement in agent performance. Experiment 3 reveals that the AI–human coach assemblage outperforms either the AI or human coach alone. This assemblage can harness the hard data skills of the AI coach and soft interpersonal skills of human managers, solving both problems faced by bottom- and top-ranked agents. These findings offer novel insights into AI coaches for researchers and managers alike. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",https://www.proquest.com/scholarly-journals/artificial-intelligence-coaches-sales-agents/docview/2650020572/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Artificial+intelligence+coaches+for+sales+agents%3A+Caveats+and+solutions&title=Journal+of+Marketing&issn=00222429&date=2021-03-01&volume=85&issue=2&spage=14&au=Luo%2C+Xueming%3BQin%2C+Marco+Shaojun%3BFang%2C+Zheng%3BQu%2C+Zhe&isbn=&jtitle=Journal+of+Marketing&btitle=&rft_id=info:eric/2021-16120-002&rft_id=info:doi/10.1177%2F0022242920956676,1,1,1,job coaching,NA,NA,"Sales agents were randomly assigned to receive job coaching from either an AI or a human coach. The coach would process/listen to the sales agent's call, point out mistakes, and give suggestions. Paper analyzes AI aversion relative to the sales agent's baseline performance/competence.",Xueming Luo and Marco Shaojun Qin and Zheng Fang and Zhe Qu,https://doi.org/10.1177/0022242920956676,"0022-2429, 0022-2429",2,Journal of Marketing,"3940:Marketing & Advertising,AI coach,Adulthood (18 yrs & older),Artificial Intelligence,Coaches,Empirical Study,Female,Human,Male,Management Personnel,Management Training,Quantitative Study,Technology,US,article,artificial intelligence,aversion,information overload,new technology,sales force management,sales training",3,"Copyright - © 2020, American Marketing Association",14-32,2650020572; 2021-16120-002,Sage Publications American Marketing Association,85,https://www.proquest.com/scholarly-journals/artificial-intelligence-coaches-sales-agents/docview/2650020572/se-2?accountid=14026 http://library.stanford.edu/sfx?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apsycinfo&atitle=Artificial+intelligence+coaches+for+sales+agents%3A+Caveats+and+solutions&title=Journal+of+Marketing&issn=00222429&date=2021-03-01&volume=85&issue=2&spage=14&au=Luo%2C+Xueming%3BQin%2C+Marco+Shaojun%3BFang%2C+Zheng%3BQu%2C+Zhe&isbn=&jtitle=Journal+of+Marketing&btitle=&rft_id=info:eric/2021-16120-002&rft_id=info:doi/10.1177%2F0022242920956676,2021,NA,NA
ID-108,article,Beyond humans: Consumer reluctance to adopt zoonotic artificial intelligence.,"In addition to humanoid‐robotic designs, an increasing number of artificial intelligence (AI)‐powered services are being represented by animals, referred to as zoonotic design. Yet, little is known about the consequential effects of such zoonotic AI on consumer adoption of these services. Drawing on the concepts of prototypicality, Cognitive Load Theory, and the ""Match‐up"" Hypothesis, the current research uncovers how the use of zoonotic designs, as opposed to robotic ones, may negatively influence consumers' adoption of AI over a human provider. The results of seven studies suggest that consumers are less likely to choose an AI over a human provider for performing tasks when the AI has a zoonotic embodiment rather than a robotic embodiment. This negative effect is mediated by the increased cognitive difficulty associated with linking the AI prototype to the task. However, such a negative effect decreases when the characteristics of the animal are congruent with the task and is even reversed when the congruent task is of a hedonic nature. These findings advance the understanding of consumer–AI interactions in the context of zoonotic embodiment and provide valuable managerial insights into when and how firms should use zoonotic design for AI‐powered services. [ABSTRACT FROM AUTHOR]","https://search.ebscohost.com/login.aspx?direct=true&db=ufh&AN=174690901&site=ehost-live&authtype=ip,sso&custid=s4392798",1,1,1,NA,NA,"zoonotic AI has been predominantly associated with services related to entertainment, companionship (e.g., SONY's AIBO), and playful tasks (Chowdhury et al., 2021; Li et al., 2010). hedonic vs. utilitarian task",zoonotic = making AI seem more lifelike by giving it an animal's embodied form. example: Otter.ai markets itself as an otter typing transcripts.,Sara‐Maude Poirier and Bo Huang and Anshu Suri and Sylvain Sénécal,doi.org/10.1002/mar.21934,7426046,2,Psychology & Marketing,"AI aversion,Adoption of ideas,Artificial intelligence,Aversion,Cognitive load,Consumer behavior,Consumer behavior research,Prototypes,artificial intelligence,cognitive load theory,match‐up hypothesis,prototypicality,zoonotic AI",2,NA,292-307,NA,NA,41,"https://search.ebscohost.com/login.aspx?direct=true&db=ufh&AN=174690901&site=ehost-live&authtype=ip,sso&custid=s4392798",2024,NA,NA
ID-123,article,The effect of required warmth on consumer acceptance of artificial intelligence in service: The moderating role of AI-human collaboration.,"Recent technological advances allow artificial intelligence (AI) to perform tasks that require high warmth, such as caring, understanding others' feelings, and being friendly. However, current consumers may be reluctant to accept AI for such tasks. This research investigates the impact of required warmth to conduct a task on consumer acceptance of AI service and the moderating role of AI-human collaboration. A series of choice-based conjoint experiments and one survey yield two main findings. First, consumers tend to refuse AI for tasks that require high warmth due to the low perceived fit between AI and the task at hand. Second, an AI-human collaboration of AI supporting a human employee increases consumer acceptance of AI service for tasks that require high warmth. This is not the case for AI-human collaboration in which AI performs a task that is supervised by a human employee. Theoretically, this study increases our understanding of how consumer acceptance of AI service varies across tasks and how AI-human collaboration can advance AI acceptance. These findings provide insightful suggestions for managers regarding designing AI service and framing AI-human collaboration. • Required warmth decreases consumer acceptance of AI service. • Task-AI fit mediates the effect of required warmth. • AI supporting humans increases AI acceptance for tasks of high warmth. • AI supervised by humans cannot increase AI acceptance for tasks of high warmth. [ABSTRACT FROM AUTHOR]","https://search.ebscohost.com/login.aspx?direct=true&db=ufh&AN=158402526&site=ehost-live&authtype=ip,sso&custid=s4392798",1,1,1,education,perceived warmth and competence,NA,"discusses task-technology fit. For example, it makes more sense for AI to do administrative work than something like career advising.",Chenming Peng and Jenny van Doorn and Felix Eggers and Jaap E Wieringa,doi.org/10.1016/j.ijinfomgt.2022.102533,2684012,NA,International Journal of Information Management,"AI acceptance,AI service,AI-human collaboration,Artificial intelligence,Intelligence service,Required warmth",10,NA,NA,NA,NA,66,"https://search.ebscohost.com/login.aspx?direct=true&db=ufh&AN=158402526&site=ehost-live&authtype=ip,sso&custid=s4392798",2022,NA,NA
ID-131,article,What factors contribute to the acceptance of artificial intelligence? A systematic review.,"• Reviewed 60 studies on user acceptance of artificial intelligence. • This is the first paper to provide a comprehensive overview of the psychosocial models and factors influencing user acceptance of AI. • In some cultural scenarios, it appears that the need for human contact cannot be replicated or replaced by AI. • Perceived usefulness, performance expectancy, attitudes, trust, and effort expectancy significantly and positively predicted intention, willingness, and use behaviour of AI across multiple industries. • Future studies should examine role of bias (e.g., job security, pre-existing knowledge) in users' intentions to use AI. Artificial Intelligence (AI) agents are predicted to infiltrate most industries within the next decade, creating a personal, industrial, and social shift towards the new technology. As a result, there has been a surge of interest and research towards user acceptance of AI technology in recent years. However, the existing research appears dispersed and lacks systematic synthesis, limiting our understanding of user acceptance of AI technologies. To address this gap in the literature, we conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and meta-Analysis guidelines using five databases: EBSCO host, Embase, Inspec (Engineering Village host), Scopus, and Web of Science. Papers were required to focus on both user acceptance and AI technology. Acceptance was defined as the behavioural intention or willingness to use, buy, or try a good or service. A total of 7912 articles were identified in the database search. Sixty articles were included in the review. Most studies (n = 31) did not define AI in their papers, and 38 studies did not define AI for their participants. The extended Technology Acceptance Model (TAM) was the most frequently used theory to assess user acceptance of AI technologies. Perceived usefulness, performance expectancy, attitudes, trust, and effort expectancy significantly and positively predicted behavioural intention, willingness, and use behaviour of AI across multiple industries. However, in some cultural scenarios, it appears that the need for human contact cannot be replicated or replaced by AI, no matter the perceived usefulness or perceived ease of use. Given that most of the methodological approaches present in the literature have relied on self-reported data, further research using naturalistic methods is needed to validate the theoretical model/s that best predict the adoption of AI technologies. [ABSTRACT FROM AUTHOR]","
What factors contribute to the acceptance of artificial ...

ScienceDirect.com
https://www.sciencedirect.com › science › article › pii",1,3,2,NA,NA,NA,"outlines all the methods, conceptualizations, and theories in the research on acceptance of AI across domains",Sage Kelly and Sherrie-Anne Kaye and Oscar Oviedo-Trespalacios,doi.org/10.1016/j.tele.2022.101925,7365853,NA,Telematics & Informatics,"AI,Artificial intelligence,Human factors,Job security,Machine learning,Psychosocial models,Social robotics,Technological innovations,Technology Acceptance Model,Trust,User acceptance",2,NA,NA,NA,NA,77,"https://search.ebscohost.com/login.aspx?direct=true&db=ufh&AN=161527071&site=ehost-live&authtype=ip,sso&custid=s4392798",2023,NA,NA
ID-132,article,When AI moderates online content: effects of human collaboration and interactive transparency on user trust.,"Given the scale of user-generated content online, the use of artificial intelligence (AI) to flag problematic posts is inevitable, but users do not trust such automated moderation of content. We explore if (a) involving human moderators in the curation process and (b) affording ""interactive transparency,"" wherein users participate in curation, can promote appropriate reliance on AI. We test this through a 3 (Source: AI, Human, Both) × 3 (Transparency: No Transparency, Transparency-Only, Interactive Transparency) × 2 (Classification Decision: Flagged, Not Flagged) between-subjects online experiment (N = 676) involving classification of hate speech and suicidal ideation. We discovered that users trust AI for the moderation of content just as much as humans, but it depends on the heuristic that is triggered when they are told AI is the source of moderation. We also found that allowing users to provide feedback to the algorithm enhances trust by increasing user agency. [ABSTRACT FROM AUTHOR]","https://search.ebscohost.com/login.aspx?direct=true&db=ufh&AN=158340620&site=ehost-live&authtype=ip,sso&custid=s4392798",1,1,1,content moderation,includes a source condition for BOTH human and AI,NA,NA,Maria D Molina and S Shyam Sundar,10.1093/jcmc/zmac010,10836101,4,Journal of Computer-Mediated Communication,"Algorithms,Artificial intelligence,Hate speech,Internet censorship,Internet content,Suicidal ideation,Trust",7,NA,NA,NA,NA,27,"https://search.ebscohost.com/login.aspx?direct=true&db=ufh&AN=158340620&site=ehost-live&authtype=ip,sso&custid=s4392798",2022,NA,NA
ID-135,article,Is algorithm aversion WEIRD? A cross-country comparison of individual-differences and algorithm aversion,"Although algorithms offer superior performance over humans across many tasks, individuals often exhibit al-gorithm aversion, resisting algorithmic advice in favour of human recommendations. However, most algorithm aversion studies rely on American samples, potentially limiting the generalisability of the findings. Given the increasing adoption of algorithms globally, we explore if the impact of two crucial factors driving algorithm aversion, uniqueness neglect and familiarity, differ between culturally different countries. Drawing on the individualism-collectivism cultural dimension, we conducted two online studies comparing algorithm aversion between people in India and the United States in medical and financial services scenarios. While our results suggest that there is no difference in the degree of algorithm aversion between Indians and Americans at an aggregate level, we find important cross-cultural differences: Uniqueness neglect strengthens algorithm aversion for Americans more than Indians, while familiarity weakens algorithm aversion more for Indians than Ameri-cans. Thus, our results reveal generalisability issues within the algorithm aversion literature, as factors influ-encing algorithm aversion can be culturally dependent.",NA,1,1,1,"medical, financial",NA,NA,"focused on cross-cultural differences: the importance of uniqueness in individualist vs. collectivist societies, familiarity with AI. Uniqueness more important to American samples, so that drove algorithm aversion moreso for Americans than Indians",Nicole Tsz Yeung Liu and Samuel N Kirshner and Eric T K Lim,doi.org/10.1016/j.jretconser.2023.103259,0969-6989,NA,JOURNAL OF RETAILING AND CONSUMER SERVICES,NA,NA,NA,NA,WOS:000924039200001,NA,72,NA,2023,NA,NA
ID-139,article,What influences algorithmic decision-making? A systematic literature review on algorithm aversion,"With the continuing application of artificial intelligence (AI) technologies in decision-making, algorithmic decision-making is becoming more efficient, often even outperforming humans. Despite this superior performance, people often consciously or unconsciously display reluctance to rely on algorithms, a phenomenon known as algorithm aversion. Viewed as a behavioral anomaly, algorithm aversion has recently attracted much scholarly attention. With a view to synthesize the findings of existing literature, we systematically review 80 empirical studies identified through searching in seven academic databases and using the snowballing technique. We inductively categorize the influencing factors of algorithm aversion under four main themes: algorithm, individual, task, and high-level. Our analysis reveals that although algorithm and individual factors have been investigated extensively, very little attention has been given to exploring the task and high-level factors. We contribute to algorithm aversion literature by proposing a comprehensive framework, highlighting open issues in existing studies, and outlining several research avenues that could be handled in future research. Our model could guide developers in designing and developing and managers in implementing and using of algorithmic decision.",NA,1,3,2,NA,NA,NA,NA,Hasan Mahmud and A K M Najmul Islam and Syed Ishtiaque Ahmed and Kari Smolander,10.1016/j.techfore.2021.121390,0040-1625,NA,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,NA,NA,NA,NA,WOS:000775946700063,NA,175,NA,2022,NA,NA
ID-140,article,Towards a better understanding on mitigating algorithm aversion in forecasting: an experimental study,"Forecasts serve as the basis for a wide range of managerial decisions. With the potential of new data sources and new techniques for data analysis, human forecasters are increasingly interacting with algorithms. Although algorithms can show better forecasting performance than humans, forecasters do not always accept these algorithms and instead show aversion to them. Algorithm aversion has become a widely known phenomenon. Drawing on the seminal study of Dietvorst et al. (J Exp Psychol Gen 144(1):114-126, 2015), we extend the evidence on algorithm aversion by introducing three environmental variables from the management accounting literature. We argue that time pressure, ""do your best"" goals, and forecasters' data input decision rights on the algorithms input mitigate algorithm aversion. To test our hypotheses, we conducted an experimental study with 1,840 participants overall. We found support for our hypothesis that time pressure mitigates algorithm aversion. We found evidence that the mitigation effect is based on forecasters' loss of confidence in their own forecast when they are under time pressure. We found no support for our hypothesis on ""do your best"" goals or forecasters' data input decision rights.",NA,1,1,1,"finance, predictions",NA,NA,NA,Markus Jung and Mischa Seiter,doi.org/10.1007/s00187-021-00326-3,2191-4761,4,JOURNAL OF MANAGEMENT CONTROL,NA,NA,NA,495-516,WOS:000701632400002,NA,32,NA,2021,NA,NA
ID-141,article,When Self-Humanization Leads to Algorithm Aversion: What Users Want from Decision Support Systems on Prosocial Microlending Platforms,"Decision support systems are increasingly being adopted by various digital platforms. However, prior research has shown that certain contexts can induce algorithm aversion, leading people to reject their decision support. This paper investigates how and why the context in which users are making decisions (for-profit versus prosocial microlending decisions) affects their degree of algorithm aversion and ultimately their preference for more human-like (versus computer-like) decision support systems. The study proposes that contexts vary in their affordances for self-humanization. Specifically, people perceive prosocial decisions as more relevant to self-humanization than for-profit contexts, and, in consequence, they ascribe more importance to empathy and autonomy while making decisions in prosocial contexts. This increased importance of empathy and autonomy leads to a higher degree of algorithm aversion. At the same time, it also leads to a stronger preference for human-like decision support, which could therefore serve as a remedy for an algorithm aversion induced by the need for self-humanization. The results from an online experiment support the theorizing. The paper discusses both theoretical and design implications, especially for the potential of anthropomorphized conversational agents on platforms for prosocial decision-making.",NA,1,1,1,finance,NA,NA,"compared prosocial vs. for-profit contexts. Affordances vary by context: prosocial context (like lending money to an entrepreneur) is though to require more empathy and autonomy from the lender. For-profit (maximizing one's own financial gains) does not require those qualities, so algorithms are more acceptable",Pascal Oliver Hessler and Jella Pfeiffer and Sebastian Hafenbradl,doi.org/10.1007/s12599-022-00754-y,2363-7005,3,BUSINESS & INFORMATION SYSTEMS ENGINEERING,NA,NA,NA,275-292,WOS:000819708600002,NA,64,NA,2022,NA,NA
ID-142,article,Man Versus Machine: Complex Estimates and Auditor Reliance on Artificial Intelligence,"Audit firms are investing billions of dollars to develop artificial intelligence (AI) systems that will help auditors execute challenging tasks (e.g., evaluating complex estimates). Although firms assume AI will enhance audit quality, a growing body of research documents that individuals often exhibit ""algorithm aversion""-the tendency to discount computer-based advice more heavily than human advice, although the advice is identical otherwise. Therefore, we conduct an experiment to examine how algorithm aversion manifests in auditor judgments. Consistent with theory, we find that auditors receiving contradictory evidence from their firm's AI system (instead of a human specialist) propose smaller adjustments to management's complex estimates, particularly when management develops their estimates using relatively objective (vs. subjective) inputs. Our findings suggest auditor susceptibility to algorithm aversion could prove costly for the profession and financial statements users.",NA,1,1,1,finance,NA,NA,NA,Benjamin P Commerford and Sean A Dennis and Jennifer R Joe and Jenny W Ulla,10.1111/1475-679X.12407,0021-8456,1,JOURNAL OF ACCOUNTING RESEARCH,NA,NA,NA,171-201,WOS:000724092600001,NA,60,NA,2022,NA,NA
ID-143,article,What drives managers towards algorithm aversion and how to overcome it? Mitigating the impact of innovation resistance through technology readiness,"Artificial intelligence has been increasingly used in many managerial decision-making contexts for its superior performance. However, many managers are still averse to using AI algorithms in decision-making. The benefits of algorithms cannot be realized optimally if managers are reluctant to use them. Although there are many studies investigating the factors that make an individual averse towards algorithms, studies examining such factors in the managerial decision-making context are scarce. This study, drawing on the innovation resistance theory and the technology readiness concept, proposes a structural model depicting relationships between managers' perceived barriers and algorithm aversion. The model also incorporates the role of managers' technology read-iness in influencing these relationships. To test the model, data were collected from 179 managers of 31 banks/ financial institutions in Bangladesh through an online survey and were analyzed using partial least squares structural equation modeling. The results indicate that barriers related to value, tradition, and image are significantly associated with algorithm aversion, whereas barriers related to usage and risk have a non-significant impact. Moderation analysis suggests that motivating factors of technology readiness mitigate the impact of value barriers on algorithm aversion.",NA,1,1,1,managerial decision making (like loan approval),NA,NA,NA,Hasan Mahmud and A K M Najmul Islam and Ranjan Kumar Mitra,doi.org/10.1016/j.techfore.2023.122641,0040-1625,NA,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,NA,NA,NA,NA,WOS:001015450800001,NA,193,NA,2023,NA,NA
ID-145,article,Algorithm Aversion: Evidence from Ridesharing Drivers,"The low rate of adoption by human users often hinders AI algorithms from achieving their intended efficiency gains. This is particularly true for algorithms that prioritize system-wide objectives because they can create misalignment of incentives and cause confusion among potential users. We provide one of the first large-scale field studies on algorithm aversion by leveraging an algorithmic recommendation rollout on a large ride sharing platform. We identify contextual experience and herding as two important factors that explain ridesharing drivers' aversion to an algorithm that is designed to help drivers make better location choices. Specifically, we find that drivers are less likely to follow the algorithm when the algorithmic recommendation does not align with their past experience at a given location-time unit and when their peers' actions contradict the algorithmic recommendations. We discuss the managerial implications of these findings.",https://pubsonline-informs-org.stanford.idm.oclc.org/doi/epdf/10.1287/mnsc.2022.02475,1,1,1,NA,NA,NA,"no comparison to human recommender, but we should keep because it's explicitly about algorithm aversion.",Meng Liu and Xiaocheng Tang and Siyuan Xia and Shuo Zhang and Yuting Zhu and Qianying Meng,doi.org/10.1287/mnsc.2022.02475,0025-1909,NA,MANAGEMENT SCIENCE,NA,NA,NA,NA,WOS:001082744200001,NA,NA,https://pubsonline-informs-org.stanford.idm.oclc.org/doi/epdf/10.1287/mnsc.2022.02475,2023,NA,NA
ID-147,article,"Algorithm aversion, emotions, and investor reaction: Does disclosing the use of AI influence investment decisions?","Businesses are increasingly using artificial intelligence (AI) in accounting systems to reduce uncertainty and improve accuracy. However, algorithm aversion (Dietvorst et al., 2015) indicates that individuals often avoid information provided by automated systems as compared to that provided by humans. This paper is an exploratory step towards documenting an emotional response to AI. We experimentally investigate how disclosing the use of AI rather than human staff for estimating the fair value of an asset influences investment decisions through lower levels of emotional response, particularly in pleasantness and attentiveness. Consistent with algorithm aversion, we find that disclosing the use of AI to estimate the asset's fair value reduces the effect of information valence on nonprofessional investor responses. Specifically, when a company's AI usage is disclosed, investors make smaller additional investments when fair value information is positive and smaller investment withdrawals when fair value information is negative, as compared to when human staff usage is disclosed. Importantly, we also find that emotions mediate the effect of information source (AI versus human staff) and moderate the effect of information valence on investment decisions.",NA,1,1,1,finance,NA,NA,NA,Tom Downen and Sarah Kim and Lorraine Lee,10.1016/j.accinf.2023.100664,1467-0895,NA,INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,NA,NA,NA,NA,WOS:001145854000001,NA,52,NA,2024,NA,NA
ID-148,article,Reducing algorithm aversion through experience,"In the context of an experiment, we examine the persistence of aversion towards algorithms in relation to learning processes. The subjects of the experiment are asked to make one share price forecast (rising or falling) in each of 40 rounds. A forecasting computer (algorithm) is available to them which has a success rate of 70%. Intuitive forecasts made by the subjects usually lead to a significantly poorer success rate. Feedback provided after each round of forecasts and a clear financial incentive lead to the subjects becoming better able to estimate their own forecasting abilities. At the same time, their aversion to algorithms also decreases significantly. (C) 2021 Elsevier B.V. All rights reserved.",NA,1,1,1,NA,NA,NA,delegation (or not) of forecasting to an algorithm,Ibrahim Filiz and Jan Rene Judek and Marco Lorenz and Markus Spiwoks,doi.org/10.1016/j.jbef.2021.100524,2214-6350,NA,JOURNAL OF BEHAVIORAL AND EXPERIMENTAL FINANCE,NA,NA,NA,NA,WOS:000700620900026,NA,31,NA,2021,NA,NA
ID-150,article,AI is not careful: approach to the stock market and preference for AI advisor,"PurposeFinancial institutions actively seek to leverage the capabilities of artificial intelligence (AI) across diverse operations in the field. Especially, the adoption of AI advisors has a significant impact on trading and investing in the stock market. The purpose of this paper is to test whether AI advisors are less preferred compared to human advisors for investing and whether this algorithm aversion diminishes for trading.Design/methodology/approachThe four hypotheses regarding the direct and indirect relationships between variables are tested in five experiments that collect data from Prolific.FindingsThe results of the five experiments reveal that, for investing, consumers are less likely to use AI advisors in comparison to human advisors. However, this reluctance to AI advisors decreases for trading. The author identifies the perceived importance of careful decision-making for investing and trading as the psychological mechanism. Specifically, the greater emphasis on careful decision-making in investing, as compared to trading, leads to consumers' tendency to avoid AI advisors.Originality/valueThis research is the first to investigate whether algorithm aversion varies based on whether one's approach to the stock market is investing or trading. Furthermore, it contributes to the literature on carefulness by exploring the interaction between a stock market approach and the lay belief that algorithms lack the capability to deliberate carefully.",NA,1,1,1,finance,NA,NA,NA,Jieun Koo,doi.org/10.1108/IJBM-10-2023-0568,0265-2323,NA,INTERNATIONAL JOURNAL OF BANK MARKETING,NA,NA,NA,NA,WOS:001270144700001,NA,NA,NA,2024,NA,NA
ID-152,article,Algorithm aversion? On the influence of advice accuracy on trust in algorithmic advice,"There is empirical evidence that decision makers show negative behaviours towards algorithmic advice compared to human advice, termed as algorithm aversion. Taking a trust theoretical perspective,this study broadens the quite monolithic view on behaviour to its cognitive antecedent: cognitive trust, i.e. trusting beliefs and trusting intentions. We examine initial trust (cognitive trust and behaviour) as well as its development after performance feedback by conducting an online experiment that asked participants to forecast the expected demand for a product. Advice accuracy was manipulated by +/- 5 % relative to the participant's initial forecasting accuracy determined in a pre-test. Results show that initial behaviour towards algorithmic advice is not influenced by cognitive trust. Furthermore, the decision maker's initial forecasting accuracy indicates a threshold between near-perfect and bad advice. When advice accuracy is at this threshold, we observe behavioural algorithm appreciation, particularly due to higher trusting integrity beliefs in algorithmic advice.",NA,1,1,1,"forecasting, predicting demand for a product",NA,NA,NA,Stefan Daschner and Robert Obermaier,doi.org/10.1080/12460125.2022.2070951,1246-0125,NA,JOURNAL OF DECISION SYSTEMS,NA,NA,NA,77-97,WOS:000792024100001,NA,31,NA,2022,NA,NA
ID-153,article,Rage Against the Artificial Intelligence? Understanding Contextuality of Algorithm Aversion and Appreciation,"People tend to be hesitant toward algorithmic tools, and this aversion potentially affects how innovations in artificial intelligence (AI) are effectively implemented. Explanatory mechanisms for aversion are based on individual or structural issues but often lack reflection on real-world contexts. Our study addresses this gap through a mixed-method approach, analyzing seven cases of AI deployment and their public reception on social media and in news articles. Using the Contextual Integrity framework, we argue that most often it is not the AI technology that is perceived as problematic, but that processes related to transparency, consent, and lack of influence by individuals raise aversion. Future research into aversion should acknowledge that technologies cannot be extricated from their contexts if they aim to understand public perceptions of AI innovation.",https://pure.eur.nl/ws/portalfiles/portal/123825372/20809-80191-1-PB.pdf,1,"1, 2",1,NA,NA,NA,"mixed-methods. How do transparency, consent, and lack of human intervention relate to AI aversion?",Tessa Oomen and Joao Goncalves and Anouk Mols,NA,1932-8036,NA,INTERNATIONAL JOURNAL OF COMMUNICATION,NA,NA,NA,609-633,WOS:001150575800039,NA,18,https://pure.eur.nl/ws/portalfiles/portal/123825372/20809-80191-1-PB.pdf,2024,NA,NA
ID-154,article,Beware the performance of an algorithm before relying on it: Evidence from a stock price forecasting experiment,"We experimentally investigated the relationship between participants' reliance on algorithms, their familiarity with the task, and the performance level of the algorithm. We found that when participants were given the freedom to submit any number as their final forecast after observing the one produced by the algorithm (a condition found to mitigate algorithm aversion), the average degree of reliance on high and low performing algorithms did not significantly differ when there was no practice stage. Participants relied less on the algorithm when there was practice stage, regardless of its performance level. The reliance on the low performing algorithm was positive even when participants could infer that they outperformed the algorithm. Indeed, participants would have done better without relying on the low performing algorithm at all. Our results suggest that, at least in some domains, excessive reliance on algorithms, rather than algorithm aversion, should be a concern.",NA,1,1,1,NA,NA,NA,Ps evaluate their stock market forecasts relative to the algorithm's,Tiffany Tsz Kwan Tse and Nobuyuki Hanaki and Bolin Mao,doi.org/10.1016/j.joep.2024.102727,0167-4870,NA,JOURNAL OF ECONOMIC PSYCHOLOGY,NA,NA,NA,NA,WOS:001220214300001,NA,102,NA,2024,NA,NA
ID-155,article,Do you create your content yourself? Using generative artificial intelligence for social media content creation diminishes perceived brand authenticity,"Recent studies have demonstrated the potential of generative artificial intelligence (GenAI) in enhancing marketing content. However, its impact on consumer behavior has remained empirically untested. In response to social media platforms mandating the disclosure of GenAI content, we investigate how followers perceive brands that use GenAI for content creation. Drawing from literature on algorithm aversion and brand authenticity, the results of three experimental studies indicate that brands' GenAI adoption induces negative attitudinal and behavioral follower reactions. These effects are mediated by followers' perceptions of brand authenticity and can be triggered by GenAI disclosure. Negative reactions are attenuated if GenAI is used to assist humans in content creation rather than to replace them through automation. Our findings underscore the need for nuance in brands' GenAI adoption to unlock economic benefits without compromising on relationships with consumers.",NA,1,1,1,"content creation, marketing",NA,NA,NA,Jasper David Bruens and Martin Meissner,doi.org/10.1016/j.jretconser.2024.103790,0969-6989,NA,JOURNAL OF RETAILING AND CONSUMER SERVICES,NA,NA,NA,NA,WOS:001218856500001,NA,79,NA,2024,NA,NA
ID-156,article,Consumer responses to human-AI collaboration at organizational frontlines: strategies to escape algorithm aversion in content creation,"Although Artificial Intelligence can offer significant business benefits, many consumers have negative perceptions of AI, leading to negative reactions when companies act ethically and disclose its use. Based on the pervasive example of content creation (e.g., via tools like ChatGPT), this research examines the potential for human-AI collaboration to preserve consumers' message credibility judgments and attitudes towards the company. The study compares two distinct forms of human-AI collaboration, namely AI-supported human authorship and human-controlled AI authorship, with traditional human authorship or full automation. Building on the compensatory control theory and the algorithm aversion concept, the study evaluates whether disclosing a high human input share (without explicit control) or human control over AI (with lower human input share) can mitigate negative consumer reactions. Moreover, this paper investigates the moderating role of consumers' perceived morality of companies' AI use. Results from two experiments in different contexts reveal that human-AI collaboration can alleviate negative consumer responses, but only when the collaboration indicates human control over AI. Furthermore, the effects of content authorship depend on consumers' moral acceptance of a company's AI use. AI authorship forms without human control lead to more negative consumer responses in case of low perceived morality (and no effects in case of high morality), whereas messages from AI with human control were not perceived differently to human authorship, irrespective of the morality level. These findings provide guidance for managers on how to effectively integrate human-AI collaboration into consumer-facing applications and advises to take consumers' ethical concerns into account.",NA,1,1,1,business,NA,NA,focuses on types of human-AI collaboration and whether AI is supporting the human or human is controlling the AI,Martin Haupt and Jan Freidank and Alexander Haas,10.1007/s11846-024-00748-y,1863-6683,NA,REVIEW OF MANAGERIAL SCIENCE,NA,NA,NA,NA,WOS:001196870800001,NA,NA,NA,2024,NA,NA
ID-158,article,PREJUDICED AGAINST THE MACHINE? IMPLICIT ASSOCIATIONS AND THE TRANSIENCE OF ALGORITHM AVERSION,"Algorithm aversion is an important and persistent issue that prevents harvesting the benefits of advancements in artificial intelligence. The literature thus far has provided explanations that primarily focus on conscious reflective processes. Here, we supplement this view by taking an unconscious perspective that can be highly informative. Building on theories of implicit prejudice, in a preregistered study, we suggest that people develop an implicit bias (i.e., prejudice) against artificial intelligence (AI) systems, as a different and threatening ""species,"" the behavior of which is unknown. Like in other contexts of prejudice, we expected people to be guided by this implicit bias but try to override it. This leads to some willingness to rely on algorithmic advice (appreciation), which is reduced as a function of people's implicit prejudice against the machine. Next, building on the somatic marker hypothesis and the accessibility-diagnosticity perspective, we provide an explanation as to why aversion is ephemeral. As people learn about the performance of an algorithm, they depend less on primal implicit biases when deciding whether to rely on the AI's advice. Two studies (n(1) = 675, n(2) = 317) that use the implicit association test consistently support this view. Two additional studies (n(3) = 255, n(4) = 332) rule out alternative explanations and provide stronger support for our assertions. The findings ultimately suggest that moving the needle between aversion and appreciation depends initially on one's general unconscious bias against AI because there is insufficient information to override it. They further suggest that in later use stages, this shift depends on accessibility to diagnostic information about the AI's performance, which reduces the weight given to unconscious prejudice.",NA,1,1,1,NA,NA,NA,NA,Ofir Turel and Shivam Kalhan,10.25300/MISQ/2022/17961,0276-7783,4,MIS QUARTERLY,NA,NA,NA,1369-1394,WOS:001179740000003,NA,47,NA,2023,NA,NA
ID-162,article,Mixed-effects regression weights for advice taking and related phenomena of information sampling and utilization,"Advice taking and related research is dominated by deterministic weighting indices, specifically ratio-of-differences-based formulas for investigating informational influence. Their arithmetic is intuitively simple, but they pose several measurement problems and restrict research to a particular paradigmatic approach. As a solution, we propose to specify how strongly peoples' judgments are influenced by externally provided evidence by fitting corresponding mixed-effects regression models. Our approach explicitly distinguishes between endogenous components, such as updated beliefs, and exogenous components, such as independent initial judgments and advice. Crucially, mixed-effects regression coefficients of various exogenous sources of information also reflect individual weighting but are based on a conceptually consistent representation of the endogenous judgment process. The formal derivation of the proposed weighting measures is accompanied by a detailed elaboration on their most important technical and statistical subtleties. We use this modeling approach to revisit empirical findings from several paradigms investigating algorithm aversion, sequential collaboration, and advice taking. In summary, we replicate and extend the original finding of algorithm appreciation and initially demonstrate a lack of evidence for both systematic order effects in sequential collaboration as well as differential weighting of multiple pieces of advice. In addition to opening new avenues for innovative research, appropriate modeling of information sampling and utilization has the potential to increase the reproducibility and replicability of behavioral science. Furthermore, the proposed method is relevant beyond advice taking, as mixed-effects regression weights can also inform research on related cognitive phenomena such as multidimensional belief updating, anchoring effects, hindsight bias, or attitude change.",NA,1,3,1,NA,NA,NA,NA,Tobias R Rebholz and Marco Biella and Mandy Huetter,doi.org/10.1002/bdm.2369,0894-3257,2,JOURNAL OF BEHAVIORAL DECISION MAKING,NA,NA,NA,NA,WOS:001184124000001,NA,37,NA,2024,NA,NA
ID-163,article,"Patients prefer artificial intelligence to a human provider, provided the AI is better than the human: A commentary on Longoni, Bonezzi and Morewedge (2019)","We call attention to an important, but overlooked finding in research reported by Longoni, Bonezzi and Morewedge (2019). Longoni et al. claim that people always prefer a human to an artificially intelligent (AI) medical provider. We show that this was only the case when the historical performance of the human and AI providers was equal. When the AI is known to outperform the human, their data showed a clear preference for the automated provider. We provide additional statistical analyses of their data to support this claim.",https://www.cambridge.org/core/journals/judgment-and-decision-making/article/patients-prefer-artificial-intelligence-to-a-human-provider-provided-the-ai-is-better-than-the-human-a-commentary-on-longoni-bonezzi-and-morewedge-2019/FDBF96A9CA1C5C88E163DDD5EF6E4115,1,1,1,healthcare,NA,NA,NA,Mark Pezzo V and Jason W Beckstead,NA,1930-2975,3,JUDGMENT AND DECISION MAKING,NA,NA,NA,443-445,WOS:000537942500011,NA,15,https://www.cambridge.org/core/journals/judgment-and-decision-making/article/patients-prefer-artificial-intelligence-to-a-human-provider-provided-the-ai-is-better-than-the-human-a-commentary-on-longoni-bonezzi-and-morewedge-2019/FDBF96A9CA1C5C88E163DDD5EF6E4115,2020,NA,NA
ID-164,article,"AI Agents as Team Members: Effects on Satisfaction, Conflict, Trustworthiness, and Willingness to Work With","Organizations are beginning to deploy artificial intelligence (AI) agents as members of virtual teams to help manage information, coordinate team processes, and perform simple tasks. How will team members perceive these AI team members and will they be willing to work with them? We conducted a 2 x 2 x 2 lab experiment that manipulated the type of team member (human or AI), their performance (high or low), and the performance of other team members (high or low). AI team members were perceived to have higher ability and integrity but lower benevolence, which led to no differences in trustworthiness or willingness to work with them. However, the presence of an AI team member resulted in lower process satisfaction. When the AI team member performed well, participants perceived less conflict compared to a human team member with the same performance, but there were no differences in perceived conflict when it performed poorly. There were no other interactions with performance, indicating that the AI team member was judged similarly to humans, irrespective of variations in performance; there was no evidence of algorithm aversion. Our research suggests that AI team members are likely to be accepted into teams, meaning that many old collaboration research questions may need to be reexamined to consider AI team members.",NA,1,1,1,team management,"performance of AI, performance of human team members",NA,NA,Alan R Dennis and Akshat Lakhiwal and Agrim Sachdeva,doi.org/10.1080/07421222.2023.2196773,0742-1222,2,JOURNAL OF MANAGEMENT INFORMATION SYSTEMS,NA,NA,NA,307-337,WOS:001012871800002,NA,40,NA,2023,NA,NA
ID-165,article,On the Motivations to Seek Information From Artificial Intelligence Agents Versus Humans: A Risk Information Seeking and Processing Perspective,"This study investigates how anticipating an artificial intelligence agent versus human information source moderates the risk information seeking and processing model. It focuses on a behavioral proxy of seeking intention-how long a participant waited for an online consultant whose identity was manipulated. In two samples (N1 = 182 students and N2 = 800 mturkers), the source identity consistently moderated the model in two ways: First, informational subjective norms encouraged seeking from humans but discouraged seeking from AI agents. Second, information insufficiency drove favoritism toward humans-when perceived information-gathering capacity was high. When the capacity was low, AI agents were favored.",https://journals-sagepub-com.stanford.idm.oclc.org/doi/full/10.1177/10755470241232993,1,1,1,healthcare,NA,NA,NA,Wang Liao and William Weisman and Arti Thakur,doi.org/10.1177/10755470241232993,1075-5470,NA,SCIENCE COMMUNICATION,NA,NA,NA,NA,WOS:001182010000001,NA,NA,https://journals-sagepub-com.stanford.idm.oclc.org/doi/full/10.1177/10755470241232993,2024,NA,NA
ID-166,article,May robots be held responsible for service failure and recovery? The role of robot service provider agents' human-likeness,"This research investigates how consumers attribute service failure and recovery responsibilities and respond to them differently based on the service provider agent type (human, humanoid, and non-humanoid robot). Two experiments show that, first, consumers attribute more service failure responsibility to the firm when the agent is less human-like. Second, they attribute more recovery responsibility to the agent and less to the firm when those agents are human, rather than robots. Third, failure (recovery) attribution to the firm reduces (enhances) consumer forgiveness and satisfaction. This study identifies the impact of human-likeness and humanness on responsibility attribution processes in interaction with robotic and human agents.",NA,1,1,1,NA,NA,NA,This study compares perceptions of humans vs. robots (humanoid and non-humanoid). Should robots be included in meta-analysis? Probably not? But 1st round of coding will include robots,Esra Arikan and Nesenur Altinigne and Ebru Kuzgun and Mehmet Okan,doi.org/10.1016/j.jretconser.2022.103175,0969-6989,NA,JOURNAL OF RETAILING AND CONSUMER SERVICES,NA,NA,NA,NA,WOS:000889125900015,NA,70,NA,2023,NA,NA
ID-167,article,Managerial advice-taking-Sharing responsibility with (non)human advisors trumps decision accuracy,"Organizations are increasingly implementing algorithmic decision aids to advise managerial decision-making. We study managers' motives behind using advice (human and nonhuman), particularly sharing responsibility versus increasing decision accuracy motives. We conduct an online experiment with experienced managers in a sales forecasting setting and find that managers focus on increasing decision accuracy (sharing responsibility) when they are unable (able) to share responsibility with advisors. Moreover, managers prefer to share responsibility with blamable human advisors over nonhuman advisors unless they perceive algorithms as socially competent. Consequently, the results show that managers are not solely motivated to minimize forecast errors but also to reduce personal responsibility when taking advice. We contribute to the literature by highlighting the opportunistic motives of managers when taking (non)human advice. Our findings also bear important implications for practice. Specifically, firms should be aware of managers' opportunistic advice-taking motives when implementing algorithmic decision aids.",NA,1,1,1,marketing,NA,NA,NA,Florian Aschauer and Matthias Sohn and Bernhard Hirsch,doi.org/10.1111/emre.12575,1740-4754,1,EUROPEAN MANAGEMENT REVIEW,NA,NA,NA,186-203,WOS:000987156400001,NA,21,NA,2024,NA,NA
ID-169,article,Who guards the guards with AI-driven robots? The ethicalness and cognitive neutralization of police violence following AI-robot advice,"We investigate whether the perceived ethicalness of police actions changes when police follow an AI-robot's advice. We assess whether perceived ethicalness of police violence is higher when police follow robot advice to arrest a passer-by, compared to no robot advice to arrest the passer-by. Using neutralization theory, we test how blame-shifting occurs. When police violently arrest an innocent passer-by, the violence is neutralized when the decision was made following the AI-robot. Perceived ethicalness of police violence is higher when the passer-by is a terrorist, and police violence against a passer-by is neutralized through 'denial of victim' and 'denial of injury'.",https://www.tandfonline.com/doi/full/10.1080/14719037.2023.2269203#d1e577,1,1,1,advice: whether or not to arrest,NA,NA,NA,Lisa Hohensinn and Jurgen Willems and Meikel Soliman and Dieter Vanderelst and Jonathan Stoll,doi.org/10.1080/14719037.2023.2269203,1471-9037,8,PUBLIC MANAGEMENT REVIEW,NA,NA,NA,2355-2379,WOS:001087366100001,NA,26,https://www.tandfonline.com/doi/full/10.1080/14719037.2023.2269203#d1e577,2024,NA,NA
ID-172,article,Ruled by robots: preference for algorithmic decision makers and perceptions of their choices,"As technology-assisted decision-making is becoming more widespread, it is important to understand how the algorithmic nature of the decision maker affects how decisions are perceived by those affected. We use an online experiment to study the preference for human or algorithmic decision makers in redistributive decisions. In particular, we consider whether an algorithmic decision maker will be preferred because of its impartiality. Contrary to previous findings, the majority of participants (over 60%) prefer the algorithm as a decision maker over a human-but this is not driven by concerns over biased decisions. However, despite this preference, the decisions made by humans are regarded more favorably. Subjective ratings of the decisions are mainly driven by participants' own material interests and fairness ideals. Participants tolerate any explainable deviation between the actual decision and their ideals but react very strongly and negatively to redistribution decisions that are not consistent with any fairness principles.",https://link.springer.com/article/10.1007/s11127-024-01178-w#Sec3,1,1,1,NA,impartiality,NA,NA,Marina Chugunova and Wolfgang J Luhan,doi.org/10.1007/s11127-024-01178-w,0048-5829,NA,PUBLIC CHOICE,NA,NA,NA,NA,WOS:001251048900001,NA,NA,https://link.springer.com/article/10.1007/s11127-024-01178-w#Sec3,2024,NA,NA
ID-173,article,"Algorithms in selection decisions: Effective, but unappreciated","Selection decisions are often affected by irrelevant variables such as gender or race. People can discount this irrelevant information by adjusting their predictions accordingly, yet they fail to do so intuitively. In five online studies (N = 1077), participants were asked to make selection decisions in which the selection test was affected by irrelevant attributes. We examined whether in such decisions people are willing to be advised by algorithms, human advisors or prefer to decide without advice. We found that people fail to adjust for irrelevant information by themselves, and those who received advice from an algorithm or human advisor made better decisions. Interestingly, although most participants stated they prefer advice from human advisors, they tend to rely equally on algorithms in actual selection tasks. The sole exception is when they are forced to choose between an algorithm and a human advisor. In that case, they pick human advisors. We conclude that while algorithms may not be people's preferred source of advice in selection decisions, they are equally useful and can be implemented.",NA,1,1,1,"recruitment, candidate selection",NA,NA,NA,Hagai Rabinovitch and David V Budescu and Yoella Bereby Meyer,doi.org/10.1002/bdm.2368,0894-3257,2,JOURNAL OF BEHAVIORAL DECISION MAKING,NA,NA,NA,NA,WOS:001160280600001,NA,37,NA,2024,NA,NA
ID-176,article,Understanding the Influence Discrepancy Between Human and Artificial Agent in Advice Interactions: The Role of Stereotypical Perception of Agency,"This paper proposes that artificial agents' underperformance in interpersonal influence situations can be explained by stereotypical perceptions of such agents' lack of capacity to act and accomplish goals (i.e., agency), triggered by their non-human identity. In two experiments of text-based conversations (N = 305 and 309), the identity of a human advice giver was manipulated to be either a human or a chatbot. The chatbot identity resulted in less perceived agency of the giver, which then mediated the identity's effects on the advice seeker's (a) perceived advice effectiveness and (b) intention to follow advice after the conversation, as well as (c) adherence to advice and (d) trouble relief after a week. A positive correlation between perceived agency of the artificial agent and the seeker's self-efficacy was identified as part of this mediation. In contrast, perceived emotional capacity (i.e., experience), despite a discrepancy between the two identities, had non-significant mediating effects.",NA,1,1,1,advice-giving,"perceived emotional capacity, agency",NA,"conversational AI, chatbot",Wang Liao and Yoo Jung Oh and Bo Feng and Jingwen Zhang,doi.org/10.1177/00936502221138427,0093-6502,5,COMMUNICATION RESEARCH,NA,NA,NA,633-664,WOS:000958055200001,NA,50,NA,2023,NA,NA
ID-178,article,Perceived opacity leads to algorithm aversion in the workplace,"With algorithms standing out and influencing every aspect of human society, people's attitudes toward algorithmic invasion have become a vital topic to be discussed. Recently, algorithms as alternatives and enhancements to human decision-making have become ubiquitously applied in the workplace. Despite algorithms offering numerous advantages, such as vast data storage and anti-interference performance, previous research has found that people tend to reject algorithmic agents across different applications. Especially in the realm of human resources, the increasing utilization of algorithms forces us to focus on users' attitudes. Thus, the present study aimed to explore public attitudes toward algorithmic decision-making and probe the underlying mechanism and potential boundary conditions behind the possible difference.To verify our research hypotheses, four experiments (N = 1211) were conducted, which involved various kinds of human resource decisions in the daily workplace, including resume screening, recruitment and hiring, allocation of bonuses, and performance assessment. Experiment 1 used a single-factor, two-level, between-subjects design. 303 participants were randomly assigned to two conditions (agent of decision-making: human versus algorithm) and measured their permissibility, liking, and willingness to utilize the agent. Experiment 1 was designed to be consistent with Experiment 2. The only difference was an additional measurement of perceived transparency to test the mediating role. Experiment 3 aimed to establish a causal chain between the mediator and dependent variables by manipulating the perceived transparency of the algorithm. In Experiment 4, a single-factor three-level between-subjects design ( non-anthropomorphism algorithm versus anthropomorphism algorithm versus human) was utilized to explore the boundary condition of this effect.As anticipated, the present research revealed a pervasive algorithmic aversion across diverse organizational settings. Specifically, we conceptualized algorithm aversion as a tripartite framework encompassing cognitive, affective, and behavioral dimensions. We found that compared with human managers, participants demonstrated significantly lower permissibility (Experiments: 1, 2, and 4), liking (Experiments: 1, 2, and 4), and willingness to utilize (Experiment 2) algorithmic management. And the robustness of this result was demonstrated by the diversity of our scenarios and samples. Additionally, this research discovered perceived transparency as an interpretation mechanism explaining participants' psychological reactions to different decision-making agents. That is to say, participants were opposed to algorithmic management because they thought its decision processes were more incomprehensible and inaccessible than humans (noted in Experiment 2). Addressing this "" black box"" phenomenon, Experiment 3 showed that providing more information and principles about algorithmic management positively influenced participants' attitudes. Crucially, the result also demonstrated the moderating effect of anthropomorphism. The result showed that participants exhibited greater permissibility and liking for the algorithm with human-like characteristics, such as a human-like name and communication style, over more than a mechanized form of the algorithm. This observation underlined the potential of anthropomorphism to ameliorate resistance to algorithmic management.These results bridge the gap between algorithmic aversion and decision transparency from the social-psychological perspective. Firstly, the present research establishes a three-dimensional (cognitive, affective, and behavioral) dual-perspective (employee and employer) model to elucidate the negative responses toward algorithmic management. Secondly, it reveals that perceived opacity acts as an obstacle to embracing algorithmic decision-making. This finding lays the theoretical foundation of Explainable Artificial Intelligence (XAI) which is conceptualized as a ""glass box"". Ultimately, the study highlights the moderating effect of anthropomorphism on algorithmic aversion. This suggests that anthropomorphizing algorithms could be a feasible approach to facilitate the integration of intelligent management systems.",NA,1,1,1,NA,NA,NA,NA,Zhao Yijun and Xu Liying and Yu Feng and Jin Wanglong,10.3724/SP.J.1041.2024.00497,0439-755X,4,ACTA PSYCHOLOGICA SINICA,NA,NA,NA,497-514,WOS:001208692100010,NA,56,NA,2024,NA,NA
ID-179,article,Does human-AI collaboration lead to more creative art? Aesthetic evaluation of human-made and AI-generated haiku poetry,"With the development of technology, the quality of AI-generated text has improved. This is relevant in the AI art field, where AI generates literature or poetry that is appreciated. This study compared human-made and AI -generated haiku poetry, which is composed with 17 syllables and the world's shortest and clearest rules, to examine aesthetic evaluations of AI art and people's beliefs about it. AI-generated haiku were divided into those with and without human intervention. Two tasks were completed by 385 participants. The first involved eval-uating human-made and AI-generated haiku on 21 items, such as beauty. The second involved determining whether the haiku were human-made or AI-generated. The results showed that the beauty rating of the AI -generated haiku with the human intervention was the highest, and those of the human-made and AI -generated haiku without human intervention were equal. Furthermore, participants could not distinguish be-tween human-made and AI-generated haiku. These results suggest that human-AI collaboration has better creativity in haiku production. Moreover, a negative correlation was found between discrimination performance and beauty rating in AI-generated haiku, suggesting that high-quality AI-generated work is believed to be human -made. This study indicates the potential of human-AI collaboration in haiku and the underestimation of AI art due to algorithm aversion.",NA,1,1,1,poetry,NA,NA,NA,Jimpei Hitsuwari and Yoshiyuki Ueda and Woojin Yun and Michio Nomura,doi.org/10.1016/j.chb.2022.107502,0747-5632,NA,COMPUTERS IN HUMAN BEHAVIOR,NA,NA,NA,NA,WOS:000877585600009,NA,139,NA,2023,NA,NA
ID-180,article,"No Thanks, Dear AI! Understanding the Effects of Disclosure and Deployment of Artificial Intelligence in Public Sector Recruitment","Applications based on artificial intelligence (AI) play an increasing role in the public sector and invoke political discussions. Research gaps exist regarding the disclosure effects-reactions to disclosure of the use of AI applications-and the deployment effect-efficiency gains in data savvy tasks. This study analyzes disclosure effects and explores the deployment of an AI application in a preregistered field experiment (n = 2,000) co-designed with a public organization in the context of employer-driven recruitment. The linear regression results show that disclosing the use of the AI application leads to significantly less interest in an offer among job candidates. The explorative analysis of the deployment of the AI application indicates that the person-job fit determined by the leaders can be predicted by the AI application. Based on the literature on algorithm aversion and digital discretion, this study provides a theoretical and empirical disentanglement of the disclosure effect and the deployment effect to inform future evaluations of AI applications in the public sector. It contributes to the understanding of how AI applications can shape public policy and management decisions, and discusses the potential benefits and downsides of disclosing and deploying AI applications in the public sector and in employer-driven recruitment.",NA,1,1,1,recruitment,NA,NA,NA,Florian Keppeler,10.1093/jopart/muad009,1053-1858,1,JOURNAL OF PUBLIC ADMINISTRATION RESEARCH AND THEORY,NA,NA,NA,39-52,WOS:000999487300001,NA,34,NA,2024,NA,NA
ID-181,article,Trust in self-driving vehicles is lower than in human drivers when both drive almost perfectly,"Studies have investigated the determinants of trust in self-driving vehicles (SDVs) and confirmed that the ability to execute the driving task flawlessly is essential to promote trust. However, little is known about the extent to which errors decrease trust in SDVs. This study conducted four experiments (N = 2221) and tested whether people's trust in SDVs was lower than that in human drivers when they made errors without causing negative events. In Experiments 1 and 2, which manipulated the driving accuracy of the drivers, the participants checked nine different pieces of information that showed accuracy. The results demonstrated that the SDV was less trusted than humans only when there was a slight possibility of making an error. This study did not identify factors explaining lower trust in the SDV. Experiments 3 and 4 consisted of participants watching videos showing that the SDV and human driver made minor errors, such as taking a long time to park. This study showed that the minor errors largely reduced trust, regardless of whether the vehicle was self-driven or driven by humans. These findings imply that errors without describing severe accidents are less likely to cause a gap in trust between SDVs and human drivers.",NA,1,1,1,self-driving cars,NA,NA,NA,Ryosuke Yokoi,10.1016/j.trf.2024.03.019,1369-8478,NA,TRANSPORTATION RESEARCH PART F-TRAFFIC PSYCHOLOGY AND BEHAVIOUR,NA,NA,NA,1-17,WOS:001262059200001,NA,103,NA,2024,NA,NA
ID-182,article,Baseball Fans’ Evaluations of Robot Umpire: The Perspective of Human-Robot Interaction,"PURPOSE The current study examined how baseball fans evaluate a robot umpire from the perspective of the human-robot interaction. In particular, this study examined whether baseball fans evaluated more positively a robot umpire depending on whether a robot or a human umpire has the final authority to make ball-strike decisions, when the ball-strike decision contradicts with each other. Furthermore, the different types of expertise (baseball vs. image analysis) embedded in robot umpire and fans’ levels of technology adoption were used to examine the relationship between umpire type and evaluations of robot umpire. METHODS This study used 2 (final decision maker: robot vs. human umpire) by 2 (different types of expertise embedded in robot umpire: baseball vs. image analysis) by 2 (different levels of technology adoption: low vs. high) between-subjects design. RESULTS The results indicated that when a robot umpire had the final authority to make ball-strike decisions, baseball fans evaluated more positively the adoption of robot umpire, when image analysis expertise was embedded in the robot umpire rather than baseball expertise. Meanwhile, baseball fans evaluated more positively the adoption of robot umpire when baseball expertise was embedded in a robot umpire rather than image analysis expertise. CONCLUSIONS The current study provides meaningful implications regarding how to design an effective system for the operation of robot umpire in baseball.",NA,1,1,1,sports decision-making (robot vs. human umpire),NA,NA,NA,Min-Young Lee and Se-Young Park and JANG， WONSEOK and 김보민,NA,1598-2920,3,Korean Journal of Sport Science,NA,NA,NA,440-450,KJD:ART002887676,NA,33,NA,2022,NA,NA
ID-184,article,Can AI close the gender gap in the job market? Individuals' preferences for AI evaluations,"Gender imbalances in the labor market continue to be an economic and social problem that could be reduced by artificial intelligence (AI), which is being promoted as a means for fairer and less biased hiring practices. To examine whether these supposed benefits of AI are perceived as such, we have investigated the preferences of individuals, particularly women, for an AI-based evaluation process in a competitive situation. The results of our experimental study (N = 152) show that individuals generally prefer a human evaluator over an AI evaluator-but only if the human evaluator is female. Whereas we demonstrate that women's beliefs in AI to reduce bias and perceived personal discrimination have a positive direct effect, we find no direct effect of the competitors' gender on women's preference for an AI evaluation. However, we find that the belief in AI moderates the other two relationships, which highlights the crucial role of people's general perception of AI tools in realizing AI's full potential and reduce anticipated biases. Our findings provide an initial indication that the use of AI technology in hiring could encourage women to apply for jobs in male-dominated fields and serve as a starting point for future research in this field.",NA,1,1,1,"recruitment, candidate screening",NA,NA,NA,Anna Lena Schulte Steinberg and Christoph Hohenberger,doi.org/10.1016/j.chbr.2023.100287,2451-9588,NA,COMPUTERS IN HUMAN BEHAVIOR REPORTS,NA,NA,NA,NA,WOS:000988509800001,NA,10,NA,2023,NA,NA
ID-187,article,Managerial overreliance on AI-augmented decision-making processes: How the use of AI-based advisory systems shapes choice behavior in R&D investment decisions,"AI-augmented decision-making processes promise to transform strategic decisions around innovation management. However, despite a growing body of research on algorithmic management, very little is known about the behavioral effects of the AI-augmented decision-making process. This article utilizes a psychological perspective to research the interaction of artificial intelligence and human judgment, suggesting that AI-based advice affects human decision-making behavior and skews perceptions of decision outcomes. We present a vignette-based decision experiment involving 150 senior executives to examine the perception of AI-augmented decisionmaking at the individual level. In contrast to earlier research on algorithm aversion, we find that employing AIbased advisory systems positively affects choice behavior and amplifies decision quality perception. We further show how this overreliance on an AI-augmented decision-making process can be explained through both a higher degree of trust in the advisor and the attribution of a more structured process. This paper contributes to the emerging discussion as to the role of AI in management and the novel phenomenon of algorithm appreciation by investigating the interplay of human and artificial intelligence in strategic decision-making to show that AI-based advice is perceived as more trustworthy than human advice in an R&D investment context.",NA,1,1,1,innovation management,NA,NA,NA,Christoph Keding and Philip Meissner,doi.org/10.1016/j.techfore.2021.120970,0040-1625,NA,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,NA,NA,NA,NA,WOS:000681245200011,NA,171,NA,2021,NA,NA
ID-193,article,Who should decide how limited healthcare resources are prioritized? Autonomous technology as a compelling alternative to humans,"Who should decide how limited resources are prioritized? We ask this question in a healthcare context where patients must be prioritized according to their need and where advances in autonomous artificial intelligence-based technology offer a compelling alternative to decisions by humans. Qualitative (Study 1a; N = 50) and quantitative (Study 1b; N = 800) analysis identified agency, emotional experience, bias-free, and error-free as four main qualities describing people's perceptions of autonomous computer programs (ACPs) and human staff members (HSMs). Yet, the qualities were not perceived to be possessed equally by HSMs and ACPs. HSMs were endorsed with human qualities of agency and emotional experience, whereas ACPs were perceived as more capable than HSMs of bias- and error-free decision-making. Consequently, better than average (Study 2; N = 371), or relatively better (Studies 3, N = 181; & 4, N = 378), ACP performance, especially on qualities characteristic of ACPs, was sufficient to reverse preferences to favor ACPs over HSMs as the decision makers for how limited healthcare resources should be prioritized. Our findings serve a practical purpose regarding potential barriers to public acceptance of technology, and have theoretical value for our understanding of perceptions of autonomous technologies.",NA,1,"1, 2",1,healthcare,"intelligent/effective communicator, emotional experience, bias-free, error-free",NA,NA,Jonathan J Rolison and Peter L T Gooding and Riccardo Russo and Kathryn E Buchanan,doi.org/10.1371/journal.pone.0292944,1932-6203,2,PLOS ONE,NA,NA,NA,NA,WOS:001181714500071,NA,19,NA,2024,NA,NA
ID-194,article,"Mirror, Mirror on the Wall: Algorithmic Assessments, Transparency, and Self-Fulfilling Prophecies","Predictive algorithmic scores can significantly impact the lives of assessed individuals by shaping decisions of organizations and institutions that affect them, for example, influencing the hiring prospects of job applicants or the release of defendants on bail. To better protect people and provide them the opportunity to appeal their algorithmic assessments, data privacy advocates and regulators increasingly push for disclosing the scores and their use in decision-making processes to scored individuals. Although inherently important, the response of scored individuals to such algorithmic transparency is understudied and therefore demands further research. Inspired by psychological and economic theories of information processing, we aim to fill this gap. We conducted a comprehensive experimental study with five treatment conditions to explore how and why disclosing the use of algorithmic scoring processes to (involuntarily) scored individuals affects their behaviors. Our results provide strong evidence that the disclosure of fundamentally erroneous algorithmic scores evokes selffulfilling prophecies that endogenously steer the behavior of scored individuals toward their assessment, enabling algorithms to help produce the world they predict. Occurring selffulfilling prophecies are consistent with an anchoring effect and the exploitation of available moral wiggle room. Because scored individuals interpret others' motives for overriding human expert and algorithmic scores differently, self-fulfilling prophecies occur in part only when disclosing algorithmic scores. Our results emphasize that isolated transparency measures can have considerable side effects with noticeable implications for the development of automation bias, the occurrence of feedback loops, and the design of transparency regulations.",NA,1,1,1,NA,"transparency (public, private)",NA,NA,Kevin Bauer and Andrej Gill,doi.org/10.1287/isre.2023.1217,1047-7047,1,INFORMATION SYSTEMS RESEARCH,NA,NA,NA,NA,WOS:000986721900001,NA,35,NA,2024,NA,NA
ID-197,article,The AI-augmented crowd: How human crowdvoters adopt AI (or not),"To date, innovation management research on idea evaluation has focused on human experts and crowd evaluators. With recent advances in artificial intelligence (AI), idea evaluation and selection processes need to keep up. As a result, the potential role of AI-enabled systems in idea evaluation has become an important topic in innovation management research and practice. While AI can help overcome human capacity constraints and biases, prior research has identified also aversive behaviors of humans toward AI. However, research has also shown lay people's appreciation of AI. This study focuses on human crowdvoters' AI adoption behavior. More precisely, we focus on gig workers, who despite often lacking expert knowledge are frequently engaged in crowdvoting. To investigate crowdvoters' AI adoption behavior, we conducted a behavioral experimental study (n = 629) with incentive-compatible rewards in a human-AI augmentation scenario. The participants had to predict the success or failure of crowd-generated ideas. In multiple rounds, participants could opt to delegate their decisions to an AI-enabled system or to make their own evaluations. Our findings contribute to the innovation management literature on open innovation, more specifically crowdvoting, by observing how human crowdvoters engage with AI. In addition to showing that the lay status of gig workers does not lead to an appreciation of AI, we identify factors that foster AI adoption in this specific innovation context. We hereby find mixed support for influencing factors previously identified in other contexts, including financial incentives, social incentives, and the provision of information about AI-enabled system's functionality. A second novel contribution of our empirical study is, however, the fading of crowdvoters' aversive behavior over time.",NA,1,1,1,innovation management,NA,NA,whether or not to delegate decision to algorithm,Elena Freisinger and Matthias Unfried and Sabrina Schneider,doi.org/10.1111/jpim.12708,0737-6782,4,JOURNAL OF PRODUCT INNOVATION MANAGEMENT,NA,NA,NA,865-889,WOS:001120853900001,NA,41,NA,2024,NA,NA
ID-198,article,"Humanoid Robots - Artificial. Human-like. Credible? Empirical Comparisons of Source Credibility Attributions Between Humans, Humanoid Robots, and Non-human-like Devices","Source credibility is known as an important prerequisite to ensure effective communication (Pornpitakpan, 2004). Nowadays not only humans but also technological devices such as humanoid robots can communicate with people and can likewise be rated credible or not as reported by Fogg and Tseng (1999). While research related to the machine heuristic suggests that machines are rated more credible than humans (Sundar, 2008), an opposite effect in favor of humans' information is supposed to occur when algorithmically produced information is wrong (Dietvorst, Simmons, and Massey, 2015). However, humanoid robots may be attributed more in line with humans because of their anthropomorphically embodied exterior compared to non-human-like technological devices. To examine these differences in credibility attributions a 3 (source-type) x 2 (information's correctness) online experiment was conducted in which 338 participants were asked to either rate a human's, humanoid robot's, or non-human-like device's credibility based on either correct or false communicated information. This between-subjects approach revealed that humans were rated more credible than social robots and smart speakers in terms of trustworthiness and goodwill. Additionally, results show that people's attributions of theory of mind abilities were lower for robots and smart speakers on the one side and higher for humans on the other side and in part influence the attribution of credibility next to people's reliance on technology, attributed anthropomorphism, and morality. Furthermore, no main or moderation effect of the information's correctness was found. In sum, these insights offer hints for a human superiority effect and present relevant insights into the process of attributing credibility to humanoid robots.",NA,1,1,1,NA,NA,NA,humanoid robot,Marcel Finkel and Nicole C Kraemer,doi.org/10.1007/s12369-022-00879-w,1875-4791,6,INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS,NA,NA,NA,1397-1411,WOS:000785943600001,NA,14,NA,2022,NA,NA
ID-201,article,Gain-loss separability in human- but not computer-based changes of mind,"The effect of human-based advice on decision-making represents a ""gain-loss asymmetry,"" as people tend to conform to others' advice in the loss than in the gain domain; however, it is unknown whether the same is true for automatically generated advice. To address a research gap in the literature created by ignoring the gain-loss dimension, we compared the utilization of human- and computer-based advices in the gain and loss domains, separately. Sixty-seven college volunteers were given an opportunity to change their initial decision in a gain- or loss-related context after receiving human- or computer-based advice. Event-related potentials were recorded including the N2 (reflecting psychological conflict) and P3 (reflecting subjective confidence) components. Behavioral data revealed a classic ""gain-loss asymmetry"" effect in the human-based condition, but not in the computer-based condition, indicating that computerized advice utilization remained prominent across different domains. Moreover, the human-based condition showed a larger option-evoked P3 in the gain than in the loss domain, but no difference was found for the computer-based condition; P3 latency was longer in the human-than in the computer-based condition. These findings support the ""automation bias"" hypothesis (i.e., automations are trusted more than humans), and may help develop automated advice systems.",NA,1,1,1,NA,NA,decision context: gains vs. losses,EEG study,Yongling Lin and Pengfei Xu and Jiayu Fan and Ruolei Gu and Yue-jia Luo,doi.org/10.1016/j.chb.2023.107712,0747-5632,NA,COMPUTERS IN HUMAN BEHAVIOR,NA,NA,NA,NA,WOS:000951514900001,NA,143,NA,2023,NA,NA
ID-204,article,The Human Black-Box: The Illusion of Understanding Human Better Than Algorithmic Decision-Making,"As algorithms increasingly replace human decision-makers, concerns have been voiced about the black-box nature of algorithmic decision-making. These concerns raise an apparent paradox. In many cases, human decision-makers are just as much of a black-box as the algorithms that are meant to replace them. Yet, the inscrutability of human decision-making seems to raise fewer concerns. We suggest that one of the reasons for this paradox is that people foster an illusion of understanding human better than algorithmic decision-making, when in fact, both are black-boxes. We further propose that this occurs, at least in part, because people project their own intuitive understanding of a decision-making process more onto other humans than onto algorithms, and as a result, believe that they understand human better than algorithmic decision-making, when in fact, this is merely an illusion.",NA,1,1,1,NA,NA,NA,interesting perspective. People think they understand human decision-making better than algorithmic decision-making because they apply their own processes,Andrea Bonezzi and Massimiliano Ostinelli and Johann Melzner,doi.org/10.1037/xge0001181,0096-3445,9,JOURNAL OF EXPERIMENTAL PSYCHOLOGY-GENERAL,NA,NA,NA,2250-2258,WOS:000753125600001,NA,151,NA,2022,NA,NA
ID-206,article,Does it pay to be honest? The effect of retailer-provided negative feedback on consumers' product choice and shopping experience,"This research aims at investigating the potential double-edged sword effect of a retailer's negative feedback, which may not only lead consumers to alter their purchase decisions, but also provide a shopping experience that is more effortful and thus be of less utilitarian value. Three experiments were performed involving 678 participants. Overall, results suggest a double-edged sword effect of negative feedback in online and offline retail contexts. When compared to no feedback, neutral feedback, or positive feedback, negative feedback leads consumers to change their initial product choice whatever their choice uncertainty and whatever the feedback source (human advisor or algorithmic advisor). However, it also leads to the perception of more cognitive effort and reduced utilitarian value, resulting in lower purchase and word-of-mouth intentions. The only situation in which negative feedback does not degrade the utilitarian value of the shopping experience is when consumers are highly uncertain about their initial product choice.",NA,1,1,1,marketing,NA,NA,NA,Aurelie Merle and Anik St-Onge and Sylvain Senecal,doi.org/10.1016/j.jbusres.2022.03.031,0148-2963,NA,JOURNAL OF BUSINESS RESEARCH,NA,NA,NA,532-543,WOS:000799247400020,NA,147,NA,2022,NA,NA
ID-207,article,Threat of racial and economic inequality increases preference for algorithm decision-making,"Artificial intelligence (AI) algorithms hold promise to reduce inequalities across race and socioeconomic status. One of the most important domains of racial and economic inequalities is medical outcomes; Black and lowincome people are more likely to die from many diseases. Algorithms can help reduce these inequalities because they are less likely than human doctors to make biased decisions. Unfortunately, people are generally averse to algorithms making important moral decisions-including in medicine-undermining the adoption of AI in healthcare. Here we use the COVID-19 pandemic to examine whether the threat of racial and economic inequality increases the preference for algorithm decision-making. Four studies (N = 2819) conducted in the United States and Singapore show that emphasizing inequality in medical outcomes increases the preference for algorithm decision-making for triage decisions. These studies suggest that one way to increase the acceptance of AI in healthcare is to emphasize the threat of inequality and its negative outcomes associated with human decision-making.",NA,1,1,1,healthcare,NA,NA,"manipulated the salience of racial and economic equality. impartiality of AI seen as an advantage over human decision=making, leading to icnreased AI acceptance",Yochanan E Bigman and Kai Chi Yam and Deborah Marciano and Scott J Reynolds and Kurt Gray,doi.org/10.1016/j.chb.2021.106859,0747-5632,NA,COMPUTERS IN HUMAN BEHAVIOR,NA,NA,NA,NA,WOS:000663722100006,NA,122,NA,2021,NA,NA
ID-208,article,Preference for human or algorithmic forecasting advice does not predict if and how it is used,"Past research has found that people treat advice differently depending on its source. In many cases, people seem to prefer human advice to algorithms, but in others, there is a reversal, and people seem to prefer algorithmic advice. Across two studies, we examine the persuasiveness of, and judges' preferences for, advice from different sources when forecasting geopolitical events. We find that judges report domain-specific preferences, preferring human advice in the domain of politics and algorithmic advice in the domain of economics. In Study 2, participants report a preference for hybrid advice, that combines human and algorithmic sources, to either one on it's own regardless of domain. More importantly, we find that these preferences did not affect persuasiveness of advice from these different sources, regardless of domain. Judges were primarily sensitive to quantitative features pertaining to the similarity between their initial beliefs and the advice they were offered, such as the distance between them and the relative advisor confidence, when deciding whether to revise their initial beliefs in light of advice, rather than the source that generated the advice.",NA,1,1,1,"forecasting geopolitical events, an impersonal task",NA,NA,"2 × 2 × 3 × 6 design, manipulated the advice source (algorithm vs. human; within subject), time horizon (short vs. long; within subject), question domain (economics, politics, other; within subject), and description of advice source (between subjects). self-reported preferences for algorithmic or human advice did not predict actual WOA",Mark Himmelstein and David Budescu V,doi.org/10.1002/bdm.2285,0894-3257,1,JOURNAL OF BEHAVIORAL DECISION MAKING,NA,NA,NA,NA,WOS:000798093300001,NA,36,NA,2023,NA,NA
ID-213,article,"AI in human teams: effects on technology use, members' interactions, and creative performance under time scarcity","Time and technology permeate the fabric of teamwork across a variety of settings to affect outcomes which have a wide range of consequences. However, there is a limited understanding about the interplay between these factors for teams, especially as applied to artificial intelligence (AI) technology. With the increasing integration of AI into human teams, we need to understand how environmental factors such as time scarcity interact with AI technology to affect team behaviors. To address this gap in the literature, we investigated the interaction between the availability of intelligent technology and time scarcity in teams. Drawing from the theoretical perspective of computers are social actors and extant research on the use of heuristics and human-AI interaction, this study uses behavioral data from 56 teams who participated in a between-subjects 2 (intelligent assistant available x control/no intelligent assistant) x 2 (time scarcity x control/no time scarcity) lab experiment. Results show that teams working under time scarcity used the intelligent assistant more often and underperformed on a creative task compared to teams without the temporal constraints. Further, teams who had an intelligent assistant available to them had fewer interactions between members compared to teams who did not have the technology. Implications for research and applications are discussed.",NA,1,1,1,team behaviors,NA,NA,"time scarcity, whether or not to adopt AI assistant",Sonia Jawaid Shaikh and Ignacio F Cruz,10.1007/s00146-021-01335-5,0951-5666,4,AI & SOCIETY,NA,NA,NA,1587-1600,WOS:000745359700001,NA,38,NA,2023,NA,NA
ID-214,article,A longitudinal approach for understanding algorithm use,"Research suggests that algorithms-based on artificial intelligence or linear regression models-make better predictions than humans in a wide range of domains. Several studies have examined the degree to which people use algorithms. However, these studies have been mostly cross-sectional and thus have failed to address the dynamic nature of algorithm use. In the present paper, we examined algorithm use with a novel longitudinal approach outside the lab. Specifically, we conducted two ecological momentary assessment studies in which 401 participants made financial predictions for 18 days in two tasks. Relying on the judge-advisor system framework, we examined how time interacted with advice source (human vs. algorithm) and advisor accuracy to predict advice taking. Our results showed that when the advice was inaccurate, people tended to use algorithm advice less than human advice across the period studied. Inaccurate algorithms were penalized logarithmically; the effect was initially strong but tended to fade over time. This suggests that first impressions are crucial and produce significant changes in advice taking at the beginning of the interaction, which later tends to stabilize as days go by. Therefore, inaccurate algorithms are more likely to accrue a negative reputation than inaccurate humans, even when having the same level of performance.",https://content.ebscohost.com/cds/retrieve?content=AQICAHiylJ_bvOB56hI8UzTN6Ryruh7a0kiIBN_ANwtaWYjmxwGhCAhlrjHA5qzvq0tf8oTtAAAA4zCB4AYJKoZIhvcNAQcGoIHSMIHPAgEAMIHJBgkqhkiG9w0BBwEwHgYJYIZIAWUDBAEuMBEEDC1FNwwlRhT3oq__dwIBEICBm4AVO0_q_SboehIxXCiOsYwX4hq_kfVVZ-Etp0HHONw-FY1Gd1Drim2iSNvJ-qpZGRcIW0OwWkSUG4JF_ssnjqX8TdarY2v5J-fBQmaA0vnwZSOk9Izr4vjJQMptTDukxowZJGURUq0El03LPPXiawyDMj7UzDkpN5iYWiUSU-tNyllxS_vQyy_j-OUFB2SFS_mwXmWX2rWFU1Lx,1,1,1,NA,NA,NA,"how does algorithm use change over time? importance of first impressions. inaccurate algorithms are more likely to gain a negative reputation than inaccurate humans, even when they have the same overall performance",Alvaro Chacon and Edgar E Kausel and Tomas Reyes,doi.org/10.1002/bdm.2275,0894-3257,4,JOURNAL OF BEHAVIORAL DECISION MAKING,NA,NA,NA,NA,WOS:000746449600001,NA,35,NA,2022,NA,NA
ID-222,article,Bending the Automation Bias Curve: A Study of Human and AI-Based Decision Making in National Security Contexts,"Uses of artificial intelligence (AI) are growing around the world. What will influence AI adoption in the international security realm? Research on automation bias suggests that humans can often be overconfident in AI, whereas research on algorithm aversion shows that, as the stakes of a decision rise, humans become more cautious about trusting algorithms. We theorize about the relationship between background knowledge about AI, trust in AI, and how these interact with other factors to influence the probability of automation bias in the international security context. We test these in a preregistered task identification experiment across a representative sample of 9,000 adults in nine countries with varying levels of AI industries. The results strongly support the theory, especially concerning AI background knowledge. A version of the Dunning-Kruger effect appears to be at play, whereby those with the lowest level of experience with AI are slightly more likely to be algorithm-averse, then automation bias occurs at lower levels of knowledge before leveling off as a respondent's AI background reaches the highest levels. Additional results show effects from the task's difficulty, overall AI trust, and whether a human or AI decision aid is described as highly competent or less competent.Los usos de la inteligencia artificial (IA) estan creciendo en todo el mundo. Que es lo que influira en la adopcion de la IA en el ambito de la seguridad internacional? La investigacion en materia del sesgo de la automatizacion sugiere que los humanos pueden confiar, con frecuencia, demasiado en la IA. Por el contrario, la investigacion relativa a la aversion a los algoritmos demuestra que, a medida que aumenta lo que esta en juego en una decision, los humanos se vuelven mas cautelosos a la hora de confiar en los algoritmos. Teorizamos sobre la relacion entre el conocimiento previo en materia de IA y la confianza en la IA, asi como sobre las formas en que estos interactuan con otros factores para influir sobre la probabilidad de que exista un sesgo con relacion a la automatizacion en el contexto de la seguridad internacional. Ponemos a prueba esta teoria mediante un experimento de identificacion de tareas prerregistrado con una muestra representativa de 9000 adultos en 9 paises con diferentes niveles de industrias de IA. Los resultados respaldan firmemente esta teoria, especialmente en lo que respecta a los conocimientos previos en materia de IA. Observamos que aqui entra en juego una version del efecto Dunning Kruger, segun el cual aquellos con el nivel mas bajo de experiencia con IA son ligeramente mas propensos a ser reacios a los algoritmos. Por lo tanto, el sesgo de automatizacion se produce en niveles mas bajos de conocimiento antes de estabilizarse a medida que los antecedentes con relacion a la IA de un encuestado alcanzan unos niveles mas altos. Los resultados adicionales muestran los efectos de la dificultad de cada tarea, de la confianza general en la IA y de si existe un humano o una ayuda para la toma de decisiones de la IA que se describan como altamente competentes o como menos competentes.Les utilisations de l'intelligence artificielle (IA) se developpent dans le monde entier. Quels seront les facteurs qui influenceront l'adoption de l'IA dans le domaine de la securite internationale ? La recherche sur les penchants pour l'automatisation suggere que les humains font souvent preuve d'une confiance excessive a l'egard de l'IA, alors que la recherche sur l'aversion aux algorithmes montre que, lorsque les enjeux d'une decision montent, les humains se montrent plus prudents quand il s'agit de faire confiance aux algorithmes. Nous formulons une theorie a propos de la relation entre les connaissances de fond quant a l'IA, la confiance envers l'IA et leurs interactions avec d'autres facteurs pour influencer la probabilite d'un penchant pour l'automatisation dans un contexte de securite internationale. Nous testons ces interactions dans une experience d'identification de taches preenregistrees appliquee a un echantillon de 9 000 adultes dans 9 pays dont le secteur de l'IA se trouve a des niveaux differents. Les resultats appuient fortement la theorie, notamment concernant les connaissances de fond sur l'IA. Une version de l'effet Dunning Kruger semblerait entrer en consideration : les personnes les moins experimentees avec l'IA ont legerement plus de chances de s'opposer aux algorithmes, puis le penchant pour l'automatisation intervient quand les connaissances sont legerement meilleures avant de se stabiliser quand l'experience de la personne sondee quant a l'IA atteint les niveaux les plus eleves. Des resultats complementaires montrent les effets de la difficulte de la tache, de la confiance globale en l'IA et de la description du niveau de competence de l'aide a la decision humaine ou d'IA.",NA,1,1,1,NA,NA,NA,"Dunning-Kruger effect: lowest level of experience with AI are more likely to be averse to algorithms, aversion decreases as knowledge increases, then aversion increases again at the highest levels of AI knowledge",Michael C Horowitz and Lauren Kahn,doi.org/10.1093/isq/sqae020,0020-8833,2,INTERNATIONAL STUDIES QUARTERLY,NA,NA,NA,NA,WOS:001196939100002,NA,68,NA,2024,NA,NA
ID-228,article,Attitudes toward artificial intelligence: combining three theoretical perspectives on technology acceptance,"Evidence on AI acceptance comes from a diverse field comprising public opinion research and largely experimental studies from various disciplines. Differing theoretical approaches in this research, however, imply heterogeneous ways of studying AI acceptance. The present paper provides a framework for systematizing different uses. It identifies three families of theoretical perspectives informing research on AI acceptance-user acceptance, delegation acceptance, and societal adoption acceptance. These models differ in scope, each has elements specific to them, and the connotation of technology acceptance thus changes when shifting perspective. The discussion points to a need for combining the three perspectives as they have all become relevant for AI. A combined approach serves to systematically relate findings from different studies. And as AI systems affect people in different constellations and no single perspective can accommodate them all, building blocks from several perspectives are needed to comprehensively study how AI is perceived in society.",NA,1,3,2,NA,NA,NA,"overview of the research on algorithm aversion. breaks down into (1) domains/disciplines, (2), context/task/stakes, (3) theoretical frameworks",P D Koenig,doi.org/10.1007/s00146-024-01987-z,0951-5666,NA,AI & SOCIETY,NA,NA,NA,NA,WOS:001242207100001,NA,NA,NA,2024,NA,NA
ID-242,article,Not a good judge of talent: the influence of subjective socioeconomic status on AI aversion,The current research constructs a framework to understand how subjective socioeconomic status (SES) affects consumers' AI aversion in the evaluation context. Three experiments show that subjective SES has a negative impact on consumers' willingness to accept AI evaluation. Consumers with higher subjective SES are more likely to resist AI evaluation because they perceive that AI agents are not as capable as human agents of identifying their talents. This effect is moderated by the agent type-the impact of subjective SES on resistance to the AI agent is attenuated when the AI agent is non-evaluative. This research is of great significance in enriching research on improving AI services efficiency across various social classes.,NA,1,1,1,recruiting,NA,NA,NA,C Y Xie and T H Fu and C Yang and E C Chang and M Y Zhao,doi.org/10.1007/s11002-024-09725-7,0923-0645,NA,MARKETING LETTERS,NA,NA,NA,NA,WOS:001184411000001,NA,NA,NA,2024,NA,NA
ID-246,article,"Understanding how personality traits, experiences, and attitudes shape negative bias toward AI-generated artworks","The study primarily aimed to understand whether individual factors could predict how people perceive and evaluate artworks that are perceived to be produced by AI. Additionally, the study attempted to investigate and confirm the existence of a negative bias toward AI-generated artworks and to reveal possible individual factors predicting such negative bias. A total of 201 participants completed a survey, rating images on liking, perceived positive emotion, and believed human or AI origin. The findings of the study showed that some individual characteristics as creative personal identity and openness to experience personality influence how people perceive the presented artworks in function of their believed source. Participants were unable to consistently distinguish between human and AI-created images. Furthermore, despite generally preferring the AI-generated artworks over human-made ones, the participants displayed a negative bias against AI-generated artworks when subjective perception of source attribution was considered, thus rating as less preferable the artworks perceived more as AI-generated, independently on their true source. Our findings hold potential value for comprehending the acceptability of products generated by AI technology.",NA,1,1,1,visual art,NA,NA,"investigated the role of user's personality traits (empathy, openness to experience)",S Grassini and M Koivisto,doi.org/10.1038/s41598-024-54294-4,2045-2322,1,SCIENTIFIC REPORTS,NA,NA,NA,NA,WOS:001167137100044,NA,14,NA,2024,NA,NA
ID-253,article,"Decoding algorithm appreciation: Unveiling the impact of familiarity with algorithms, tasks, and algorithm performance","Algorithm appreciation, defined as an individual's reliance or tendency to rely on algorithms in decision-making, has emerged as a subject of growing scholarly interest. Inquiries into this subject are crucial to understanding human decision-making processes as in the era of artificial intelligence, algorithms are increasingly being integrated into decision-making. To contribute to this evolving field, this study examines three factors that might play significant roles in enhancing trust in algorithms: familiarity with algorithms, familiarity with tasks, and familiarity with algorithm performance. Drawing upon prior studies, a conceptual model was developed and empirically tested using a scenario study. Data on 327 individuals showed a strong positive association between familiarity with algorithms and trust in algorithms. In contrast, task familiarity appeared to have no significant influence on trust. Trust, in turn, was identified as a key driver of algorithm appreciation. The study also revealed the moderating role of familiarity with algorithm performance in the relationship between familiarity with algorithms and trust in algorithms. Post hoc analysis highlighted that trust fully mediates the relationship between algorithm familiarity and algorithm appreciation. The study underscores the significance of algorithm familiarity and performance transparency in shaping trust in algorithms. The study contributes theoretically by offering important insights about the influences of different forms of familiarity on trust and practically by prescribing practical guidelines to enhance algorithm appreciation.",NA,1,1,1,NA,NA,NA,WOA,H Mahmud and AKMN Islam and X Luo and P Mikalef,doi.org/10.1016/j.dss.2024.114168,0167-9236,NA,DECISION SUPPORT SYSTEMS,NA,NA,NA,NA,WOS:001155974900001,NA,179,NA,2024,NA,NA
ID-271,article,The augmentation effect of artificial intelligence: can AI framing shape customer acceptance of AI-based services?,"Although Artificial Intelligence is a big revolution in the tourism and hospitality industry, prior research provides little insight into how customers respond to AI replacement and how providers can mitigate AI aversion. Drawing on the Feeling Economy framework, three studies examine how customers react to a different framing of AI replacement (augmentation vs. substitution) compared to using only human employees, affecting their acceptance of AI-based services. The findings contribute to the tourism and hospitality literature by revealing that framing AI as augmentation (vs. substitution) can increase enjoyment and ease of use and improve AI acceptance. Consistent with the Feeling Economy account, the findings highlight the proposed mechanism of enjoyment and perceived ease of use underlying the AI framing effects. This research provides important theoretical and managerial implications for tourism and hospitality providers, helping them understand how to effectively introduce AI-based services to win customers' acceptance.",NA,1,1,1,"tourism, hospitality",NA,NA,framed how AI is involved. Does it augment human's abilities or substitute? How do those framings compare to control group?,D Vorobeva and D C Pinto and N Antonio and A S Mattila,doi.org/10.1080/13683500.2023.2214353,1368-3500,10,CURRENT ISSUES IN TOURISM,NA,NA,NA,1551-1571,WOS:000995146100001,NA,27,NA,2024,NA,NA
ID-278,article,Defending humankind: Anthropocentric bias in the appreciation of AI art,"We argue that recent advances of artificial intelligence (AI) in the domain of art (e.g., music, painting) pose a profound ontological threat to anthropocentric worldviews because they challenge one of the last frontiers of the human uniqueness narrative: artistic creativity. Four experiments (N = 1708), including a high-powered pre-registered experiment, consistently reveal a pervasive bias against AI-made artworks and shed light on its psy-chological underpinnings. The same artwork is preferred less when labeled as AI-made (vs. human-made) because it is perceived as less creative and subsequently induces less awe, an emotional response typically associated with the aesthetic appreciation of art. These effects are more pronounced among people with stronger anthropocentric creativity beliefs (i.e., who believe that creativity is a uniquely human characteristic). Sys-tematic depreciation of AI-made art (assignment of lower creative value, suppression of emotional reactions) appears to serve a shaken anthropocentric worldview whereby creativity is exclusively reserved for humans.",https://www.sciencedirect.com/science/article/pii/S0747563223000584?via%3Dihub,1,1,1,artwork,NA,NA,NA,K Millet and F Buehler and G Z Du and M D Kokkoris,doi.org/10.1016/j.chb.2023.107707,0747-5632,NA,COMPUTERS IN HUMAN BEHAVIOR,NA,NA,NA,NA,WOS:001009394200001,NA,143,https://www.sciencedirect.com/science/article/pii/S0747563223000584?via%3Dihub,2023,NA,NA
ID-293,article,"How do people react to AI failure? Automation bias, algorithmic aversion, and perceived controllability","AI can make mistakes and cause unfavorable consequences. It is important to know how people react to such AI-driven negative consequences and subsequently evaluate the fairness of AI's decisions. This study theorizes and empirically tests two psychological mechanisms that explain the process: (a) heuristic expectations of AI's consistent performance (automation bias) and subsequent frustration of unfulfilled expectations (algorithmic aversion) and (b) heuristic perceptions of AI's controllability over negative results. Our findings from two experimental studies reveal that these two mechanisms work in an opposite direction. First, participants tend to display more sensitive responses to AI's inconsistent performance and thus make more punitive assessments of AI's decision fairness, when compared to responses to human experts. Second, as participants perceive AI has less control over unfavorable outcomes than human experts, they are more tolerant in their assessments of AI.",NA,1,1,1,NA,NA,NA,NA,S M Jones-Jang and Y J Park,10.1093/jcmc/zmac029,1083-6101,1,JOURNAL OF COMPUTER-MEDIATED COMMUNICATION,NA,NA,NA,NA,WOS:000885646100001,NA,28,NA,2022,NA,NA
ID-321,article,Acceptance of Medical Treatment Regimens Provided by AI vs. Human,"Along with the increasing development of information technology, the interaction between artificial intelligence and humans is becoming even more frequent. In this context, a phenomenon called ""medical AI aversion"" has emerged, in which the same behaviors of medical AI and humans elicited different responses. Medical AI aversion can be understood in terms of the way that people attribute mind capacities to different targets. It has been demonstrated that when medical professionals dehumanize patients-making fewer mental attributions to patients and, to some extent, not perceiving and treating them as full human-it leads to more painful and effective treatment options. From the patient's perspective, will painful treatment options be unacceptable when they perceive the doctor as a human but disregard his or her own mental abilities? Is it possible to accept a painful treatment plan because the doctor is artificial intelligence? Based on the above, the current study investigated the above questions and the phenomenon of medical AI aversion in a medical context. Through three experiments it was found that: (1) human doctor was accepted more when patients were faced with the same treatment plan; (2) there was an interactional effect between the treatment subject and the nature of the treatment plan, and, therefore, affected the acceptance of the treatment plan; and (3) experience capacities mediated the relationship between treatment provider (AI vs. human) and treatment plan acceptance. Overall, this study attempted to explain the phenomenon of medical AI aversion from the mind perception theory and the findings are revealing at the applied level for guiding the more rational use of AI and how to persuade patients.",NA,1,1,1,"medical, healthcare",NA,NA,"AI algorithms lack the enthusiasm and responsibility that many patients see as necessary for a medical advisor. So even when the algorithm proposes the same treatment plan as a human doctor, it is less likely to be accepted",J H Wu and L Y Xu and F Yu and K P Peng,doi.org/10.3390/app12010110,2076-3417,1,APPLIED SCIENCES-BASEL,NA,NA,NA,NA,WOS:000752218400001,NA,12,NA,2022,NA,NA
ID-324,article,Drivers and social implications of Artificial Intelligence adoption in healthcare during the COVID-19 pandemic,"The COVID-19 pandemic continues to impact people worldwide-steadily depleting scarce resources in healthcare. Medical Artificial Intelligence (AI) promises a much-needed relief but only if the technology gets adopted at scale. The present research investigates people's intention to adopt medical AI as well as the drivers of this adoption in a representative study of two European countries (Denmark and France, N = 1068) during the initial phase of the COVID-19 pandemic. Results reveal AI aversion; only 1 of 10 individuals choose medical AI over human physicians in a hypothetical triage-phase of COVID-19 pre-hospital entrance. Key predictors of medical AI adoption are people's trust in medical AI and, to a lesser extent, the trait of open-mindedness. More importantly, our results reveal that mistrust and perceived uniqueness neglect from human physicians, as well as a lack of social belonging significantly increase people's medical AI adoption. These results suggest that for medical AI to be widely adopted, people may need to express less confidence in human physicians and to even feel disconnected from humanity. We discuss the social implications of these findings and propose that successful medical AI adoption policy should focus on trust building measures-without eroding trust in human physicians.",NA,1,1,1,"medical, healthcare",NA,NA,"self-report questions, perceived trust and uniqueness of AI/human doctor",D A Frank and C T Elbæk and C K Borsting and P Mitkidis and T Otterbring and S Borau,doi.org/10.1371/journal.pone.0259928,1932-6203,11,PLOS ONE,NA,NA,NA,NA,WOS:000755579000017,NA,16,NA,2021,NA,NA
ID-337,article,Use and Control of Artificial Intelligence in Patients Across the Medical Workflow: Single-Center Questionnaire Study of Patient Perspectives,"Background: Artificial intelligence (AI) is gaining increasing importance in many medical specialties, yet data on patients' opinions on the use of AI in medicine are scarce. Objective: This study aimed to investigate patients' opinions on the use of AI in different aspects of the medical workflow and the level of control and supervision under which they would deem the application of AI in medicine acceptable. Methods: Patients scheduled for computed tomography or magnetic resonance imaging voluntarily participated in an anonymized questionnaire between February 10, 2020, and May 24, 2020. Patient information, confidence in physicians vs AI in different clinical tasks, opinions on the control of AI, preference in cases of disagreement between AI and physicians, and acceptance of the use of AI for diagnosing and treating diseases of different severity were recorded. Results: In total, 229 patients participated. Patients favored physicians over AI for all clinical tasks except for treatment planning based on current scientific evidence. In case of disagreement between physicians and AI regarding diagnosis and treatment planning, most patients preferred the physician's opinion to AI (96.2% [153/159] vs 3.8% [6/159] and 94.8% [146/154] vs 5.2% [8/154], respectively; P=.001). AI supervised by a physician was considered more acceptable than AI without physician supervision at diagnosis (confidence rating 3.90 [SD 1.20] vs 1.64 [SD 1.03], respectively; P=.001) and therapy (3.77 [SD 1.18] vs 1.57 [SD 0.96], respectively; P=.001). Conclusions: Patients favored physicians over AI in most clinical tasks and strongly preferred an application of AI with physician supervision. However, patients acknowledged that AI could help physicians integrate the most recent scientific evidence into medical care. Application of AI in medicine should be disclosed and controlled to protect patient interests and meet ethical standards.",https://web-p-ebscohost-com.stanford.idm.oclc.org/ehost/detail/detail?vid=0&sid=ee68696e-d2c1-4204-ada0-30978c10771e%40redis&bdata=JnNpdGU9ZWhvc3QtbGl2ZSZzY29wZT1zaXRl#AN=149133698&db=aph,1,1,1,"medical, healthcare",NA,NA,NA,S Lennartz and T Dratsch and D Zopfs and T Persigehl and D Maintz and N G Hokamp and D P dos Santos,10.2196/24221,1438-8871,2,JOURNAL OF MEDICAL INTERNET RESEARCH,NA,NA,NA,NA,WOS:000618984600006,NA,23,https://web-p-ebscohost-com.stanford.idm.oclc.org/ehost/detail/detail?vid=0&sid=ee68696e-d2c1-4204-ada0-30978c10771e%40redis&bdata=JnNpdGU9ZWhvc3QtbGl2ZSZzY29wZT1zaXRl#AN=149133698&db=aph,2021,NA,NA
ID-396,article,Delegation of purchasing tasks to AI: The role of perceived choice and decision autonomy,"Although artificial intelligence (AI) outperforms humans in many tasks, research suggests some consumers are still averse to having AI perform tasks on their behalf. Informed by the literature of customer decision-making process, we propose and show that consumer autonomy is a significant predictor of customers' decision to adopt AI in the purchasing context. Across three experiments, we found that the delegation of purchasing tasks to AI, which restricts choice and decision dimensions of consumers' perceived autonomy, reduces the likelihood of AI adoption. Our results show that the effects of choice and decision autonomy on AI adoption holds even when product choice evaluation is complex. We also found that identity-relevant consumption moderates this relationship, such that it interacts with choice and decision autonomy. Specifically, despite lacking choice and decision autonomy, those who identify strongly with a given activity are more likely to use an AI-enabled app to purchase the product needed to perform this activity. With these insights into when and why consumers are likely to use AI-enabled technology, firms might effectively increase its adoption. © 2024 Elsevier B.V.",https://www-sciencedirect-com.stanford.idm.oclc.org/science/article/pii/S0167923623002415?via%3Dihub,1,1,1,shopping,NA,"varied choice autonomy and decision autonomy. The scenarios in the experiment describe whether participants can see all grocery choices available; i.e., choice autonomy present (vs. not able to see all the available choices; i.e., choice autonomy absent) and whether they make the final choices and execute the purchases themselves; i.e., decision autonomy present (vs. the app makes the final choices and executes the purchases; i.e., decision autonomy absent)",NA,M Ahmad Husairi and P Rossi,doi.org/10.1016/j.dss.2023.114166,NA,NA,Decision Support Systems,NA,NA,Cited By :1,NA,NA,NA,179,https://www-sciencedirect-com.stanford.idm.oclc.org/science/article/pii/S0167923623002415?via%3Dihub,2024,NA,NA
ID-399,article,"Artificial intelligence in Jordanian education: Assessing acceptance via perceived cybersecurity, novelty value, and perceived trust","The growing significance of Artificial Intelligence (AI) across different fields highlights the es-sential role of user acceptance, as the success of this technology largely depends on its adoption and practical use by individuals. This research aims to examine how perceived cybersecurity, novelty value, and perceived trust affect students' willingness to accept AI in educational settings. The study's theoretical basis is the AI Device Use Acceptance (AIDUA) model. Using structural equation modeling, the study tested hypothesized relationships using data from 526 students at Jordanian universities. The results showed that social influence is positively associated with performance expectancy, while perceived cybersecurity is positively related to both performance and effort expectancy. Novelty value is positively associated with performance expectancy but a neg-ative one with effort expectancy. Additionally, effort and performance expectancy significantly influence perceived trust and the willingness to accept AI. Moreover, perceived trust has a notable positive effect on the willingness to accept AI in education. These findings provide valuable guid-ance for the creation and improvement of AI-driven educational systems in universities, contrib-uting to the broader understanding of AI technology acceptance in the educational field. © 2024 by the authors; licensee Growing Science, Canada.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185688119&doi=10.5267%2fj.ijdns.2023.12.022&partnerID=40&md5=804a18984aa2e5f226fa73c5082aa3fe,1,1,1,NA,NA,NA,"authors make the point that user acceptance is a key aspect of the success of a technology. The technology could (objectively) be strong, but if people aren't willing to use it it has failed",M Alzyoud and N Al-Shanableh and S Alomar and A M As’adalnaser and A Mustafa and A Al-Momani and S I S Al-Hawary,10.5267/j.ijdns.2023.12.022,NA,2,International Journal of Data and Network Science,NA,NA,Cited By :7,823-834,NA,NA,8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185688119&doi=10.5267%2fj.ijdns.2023.12.022&partnerID=40&md5=804a18984aa2e5f226fa73c5082aa3fe,2024,NA,NA
ID-401,article,Toward human-centered AI management: Methodological challenges and future directions,"As algorithms powered by Artificial Intelligence (AI) are increasingly involved in the management of organizations, it becomes imperative to conduct human-centered AI management research and understand people's feelings and behaviors when machines gain power over humans. The two mainstream methods – vignette studies and case studies – reveal important but inconsistent insights. Here we discuss the respective limitations of vignette studies (affective forecasting errors, biased media coverage, and question substitution) and case studies (social desirability biases and lack of random assignment and control conditions), which may lead them to overrate negative and positive reactions to AI management, respectively. We further discuss the advantages of a third method for mitigating these limitations: field experiments on crowdsourced marketplaces. A proof-of-concept study on Amazon Mechanical Turk (Mturk; as a world-leading crowdsourcing platform) showed unique human reactions to AI management, which were not perfectly aligned with those in vignette or case studies. Participants (N = 504) did not differ significantly under AI versus human management, in terms of performance, intrinsic motivation, fairness perception, and commitment. We suggest that crowdsourced marketplaces can go beyond human research subject pools and become models of AI-managed workplaces, facilitating timely behavioral research and robust predictions on human-centered work designs and organizations. © 2024 The Authors",https://www-sciencedirect-com.stanford.idm.oclc.org/science/article/pii/S0166497224000038?via%3Dihub#sec4,1,1,1,management,fairness,2 (manager: algorithm vs. human) by 2 (evaluation: correct or incorrect) between-participants design,methodological contribution. showing how measurement via vignette studies and case studies yields distorted conclusions about algorithm aversion,M Dong and J.-F. Bonnefon and I Rahwan,10.1016/j.technovation.2024.102953,NA,NA,Technovation,NA,NA,Cited By :1,NA,NA,NA,131,https://www-sciencedirect-com.stanford.idm.oclc.org/science/article/pii/S0166497224000038?via%3Dihub#sec4,2024,NA,NA
ID-410,article,How Gender and Type of Algorithmic Group Discrimination Influence Ratings of Algorithmic Decision Making,"Algorithms frequently discriminate against certain groups, and people generally reject such unfairness. However, people sometimes display an egocentric bias when choosing between fairness rules. Two online experiments were conducted to explore whether egocentric biases influence the judgment of biased algorithms. In Experiment 1, an unbiased algorithm was compared with an algorithm favoring males and an algorithm favoring married people. Experiment 2 focused only on the first two conditions. Instead of the expected gender difference in the condition in which the algorithm favored males, a gender difference in the unbiased condition was found in both experiments. Women perceived the unfair algorithm as less fair than men did. Women also perceived the algorithm favoring married people as the least fair. Fairness ratings, however, did not directly translate into permissibility ratings. The results show that egocentric biases are subtle and that women take the social context more into account than men do.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189483022&partnerID=40&md5=87c5f3c0886967121e9e01004c4596fe,1,1,1,loan approval,NA,NA,"unbiased algorithm vs. two biased algorithms (biased against women, biased against single people). Observed egocentric bias. For example, a woman would find the sexist algorithm less fair but a man would not recognize the algorithm as unfair",S Utz,NA,NA,NA,International Journal of Communication,NA,NA,Export Date: 30 July 2024,570-589,NA,NA,18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189483022&partnerID=40&md5=87c5f3c0886967121e9e01004c4596fe,2024,NA,NA
ID-420,article,EmoBot: Artificial emotion generation through an emotional chatbot during general-purpose conversations,"Emotion modeling has always been intriguing to researchers, where detecting emotion is highly focused and generating emotion is much less focused to date. Therefore, in this paper, we aim to exploring emotion generation, particularly for general-purpose conversations. Based on the Cognitive Appraisal Theory and focusing on audio and textual inputs, we propose a novel method to calculate informative variables to evaluate a particular emotion-generating event and six primary emotions. Incorporating such a method of artificial emotion generation, we implement an emotional chatbot, namely EmoBot. Accordingly, EmoBot analyzes continuous audio and textual inputs, calculates the informative variables to evaluate the current situation, generates appropriate emotions, and responds accordingly. An objective evaluation indicates that EmoBot could generate more accurate emotional and semantic responses than a traditional chatbot that does not consider emotion. Additionally, a subjective evaluation of EmoBot demonstrates the appreciation of users for EmoBot over a traditional chatbot that does not consider emotion.",https://www-sciencedirect-com.stanford.idm.oclc.org/science/article/pii/S138904172300102X?via%3Dihub,1,1,1,NA,NA,NA,Section 6 shows the results of users' subjective evaluations of EmoBot compared to a non-emotional bot,M Ehtesham-Ul-Haque and J D'Rozario and R Adnin and F T Utshaw and F Tasneem and I J Shefa and A.B.M.A. Al Islam,10.1016/j.cogsys.2023.101168,NA,NA,Cognitive Systems Research,NA,NA,Export Date: 30 July 2024,NA,NA,NA,83,https://www-sciencedirect-com.stanford.idm.oclc.org/science/article/pii/S138904172300102X?via%3Dihub,2024,NA,NA
ID-424,article,"Cultural Differences in People s Reactions and Applications of Robots, Algorithms, and Artificial Intelligence","Although research in cultural psychology has established that virtually all human behaviors and cognitions are in some ways shaped by culture, culture has been surprisingly absent from the emerging literature on the psychology of technology. In this perspective article, we first review recent findings on machine aversion versus appreciation. We then offer a cross-cultural perspective in understanding how people might react differently to machines. We propose three frameworks - historical, religious, and exposure - to explain how Asians might be more accepting of machines than their Western counterparts.We end the article by discussing three exciting human-machine applications found primarily in Asia and provide future research directions.",https://www.cambridge.org/core/journals/management-and-organization-review/article/cultural-differences-in-peoples-reactions-and-applications-of-robots-algorithms-and-artificial-intelligence/EE491FDF4C4773AB97D71C89545DF07C,1,1,2,NA,NA,NA,"considers cultural differences between Asians and Westerners to try to explain why Asians may be more algorithm-appreciative. For example, Asian cultures may emphasize less human exceptionalism, more exposure to robots, acceptance that spirits can exist in non-humans",K C Yam and T Tan and J C Jackson and A Shariff and K Gray,10.1017/mor.2023.21,NA,5,Management and Organization Review,NA,NA,Cited By :4,859-875,NA,NA,19,https://www.cambridge.org/core/journals/management-and-organization-review/article/cultural-differences-in-peoples-reactions-and-applications-of-robots-algorithms-and-artificial-intelligence/EE491FDF4C4773AB97D71C89545DF07C,2023,NA,NA
ID-428,article,Perceptions and Acceptance of Artificial Intelligence: A Multi-Dimensional Study,"In this comprehensive study, insights from 1389 scholars across the US, UK, Germany, and Switzerland shed light on the multifaceted perceptions of artificial intelligence (AI). AI’s burgeoning integration into everyday life promises enhanced efficiency and innovation. The Trustworthy AI principles by the European Commission, emphasising data safeguarding, security, and judicious governance, serve as the linchpin for AI’s widespread acceptance. A correlation emerged between societal interpretations of AI’s impact and elements like trustworthiness, associated risks, and usage/acceptance. Those discerning AI’s threats often view its prospective outcomes pessimistically, while proponents recognise its transformative potential. These inclinations resonate with trust and AI’s perceived singularity. Consequently, factors such as trust, application breadth, and perceived vulnerabilities shape public consensus, depicting AI as humanity’s boon or bane. The study also accentuates the public’s divergent views on AI’s evolution, underlining the malleability of opinions amidst polarising narratives.",https://www.mdpi.com/2076-0760/12/9/502,1,"1, 2",1,NA,NA,NA,large-scale study on predictors of AI acceptance,M Gerlich,10.3390/socsci12090502,NA,9,Social Sciences,NA,NA,Cited By :9,NA,NA,NA,12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172093963&doi=10.3390%2fsocsci12090502&partnerID=40&md5=a2d29577e00da611648228fe1986a3fe,2023,NA,NA
ID-429,article,Task-specific algorithm advice acceptance: A review and directions for future research,"Due to digitalization resulting in artificial intelligence advice, there are increasing studies on advice taking, exploring individual and task-relevant factors associated with the acceptance of algorithm advice. However, to our notice, there are no reviews of studies on the acceptance of algorithm advice that focus explicitly on a task level that consider methodological features and provide a quantitative measure of algorithm acceptance. Our review closes these research gaps. We evaluated 44 studies, 122 tasks, and 89,751 participants. Our review shows that algorithm aversion is present in 75% of the 122 considered tasks. In addition, our quantified measures underscore some shortcomings by the underrepresented individual, task, or methodological characteristics—for example, the expertise of advice takers and longitudinal studies. Finally, we provide valuable recommendations to continue research on algorithm acceptance.",https://www.sciencedirect.com/science/article/pii/S2543925123000141?via%3Dihub,1,3,1,NA,NA,"The paper consideres (i) domain, (ii) objectivity, (iii) advice asking, (iv) type of human advice source, (v) algorithm information, and (vi) pre-advice opinion",NA,E Kaufmann and A Chacon and E E Kausel and N Herrera and T Reyes,doi.org/10.1016/j.dim.2023.100040,NA,3,Data and Information Management,NA,NA,Cited By :6,NA,NA,NA,7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167615787&doi=10.1016%2fj.dim.2023.100040&partnerID=40&md5=30026fa2dd1b3c032fb616a8782dfcdc,2023,NA,NA
ID-437,article,Appreciation vs. apology: Research on the influence mechanism of chatbot service recovery based on politeness theory,"Failures in human-chatbot interactions are becoming inevitable such as failure of chatbots to maintain contextual awareness. Thus, an effective service recovery strategy is essential for e-commerce enterprises to restore customers and resolve their complaints. Therefore, based on politeness theory, this study explores the fundamental mechanism and boundary conditions for a chatbot politeness strategy (appreciation vs. apology) on consumers' post-recovery satisfaction using four scenario-based experiments. The results indicate that establishing a good human-chatbot relationship (appreciation) is a more effective recovery strategy than admitting the chatbot's limited competence (apology) in redressing service failures; face concern mediates the effect of the politeness strategy on post-recovery satisfaction; and time pressure plays a moderating role in the effect of the politeness strategy on face concern and post-recovery satisfaction. This study extends the research of politeness theory and face concern in the field of chatbot marketing, and provide practical guidance for e-commerce enterprises to deal effectively with chatbot failure.",https://www.sciencedirect.com/science/article/pii/S096969892300070X?casa_token=nT1WacRBHs0AAAAA:b0Bw04_qawg_mEan5MPJpa8WBhPwKkSCaJijxka_-WaMb765yOtrT-gcS3XbmX0v_pS_m51oZw,1,1,1,"shopping, e-commerce","chatbot politeness (in the form of appreciation, apology)",the role of time pressure,no comparison to human assistant,M Song and H Zhang and X Xing and Y Duan,10.1016/j.jretconser.2023.103323,NA,NA,Journal of Retailing and Consumer Services,NA,NA,Cited By :23,NA,NA,NA,73,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149419966&doi=10.1016%2fj.jretconser.2023.103323&partnerID=40&md5=2906695d484b7ac597d5a3d9b3aa3242,2023,NA,NA
ID-442,article,Artificial Intelligence and Its Ethical Implications for Marketing,"Despite the recent developments in AI, ethical questions arise when consumers contemplate how their data is being treated. This paper develops a conceptual model building on the theory of acceptance, risk, trust, and attitudes towards AI to understand the drivers that lead consumers to accept AI, considering consumers' ethical concerns. The model was empirically tested with 200 consumers of AI marketing services. The findings reveal that perceived risk significantly impacts attitudes toward AI, ethical concerns, and perceived trust and suggest a significant association between perceived risk, ethical concerns, and social norms. This research provides important theoretical and managerial implications for the ethical aspects of AI in marketing by highlighting the ethical and moral questions surrounding AI's acceptance.",https://research.unl.pt/ws/portalfiles/portal/53855603/Artificial_Intelligence_Its_Ethical_Implications_for_Marketing.pdf,1,3,1,NA,"perceived trust, risk, usefulness",NA,NA,A R Gonçalves and D C Pinto and P Rita and T Pires,10.28991/ESJ-2023-07-02-01,NA,2,Emerging Science Journal,NA,NA,Cited By :4,313-327,NA,NA,7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149197198&doi=10.28991%2fESJ-2023-07-02-01&partnerID=40&md5=de23ba9870c7260eb7b49c122f93655c,2023,NA,NA
ID-444,article,Human-AI Interactions in Public Sector Decision Making: “Automation Bias” and “Selective Adherence” to Algorithmic Advice,"Artificial intelligence algorithms are increasingly adopted as decisional aides by public bodies, with the promise of overcoming biases of human decision-makers. At the same time, they may introduce new biases in the human-algorithm interaction. Drawing on psychology and public administration literatures, we investigate two key biases: overreliance on algorithmic advice even in the face of “warning signals” from other sources (automation bias), and selective adoption of algorithmic advice when this corresponds to stereotypes (selective adherence). We assess these via three experimental studies conducted in the Netherlands: In study 1 (N = 605), we test automation bias by exploring participants' adherence to an algorithmic prediction compared to an equivalent human-expert prediction. We do not find evidence for automation bias. In study 2 (N = 904), we replicate these findings, and also test selective adherence. We find a stronger propensity for adherence when the advice is aligned with group stereotypes, with no significant differences between algorithmic and human-expert advice. In study 3 (N = 1,345), we replicate our design with a sample of civil servants. This study was conducted shortly after a major scandal involving public authorities' reliance on an algorithm with discriminatory outcomes (the “childcare benefits scandal”). The scandal is itself illustrative of our theory and patterns diagnosed empirically in our experiment, yet in our study 3, while supporting our prior findings as to automation bias, we do not find patterns of selective adherence. We suggest this is driven by bureaucrats' enhanced awareness of discrimination and algorithmic biases in the aftermath of the scandal. We discuss the implications of our findings for public sector decision making in the age of automation. Overall, our study speaks to potential negative effects of automation of the administrative state for already vulnerable and disadvantaged citizens.",https://content.ebscohost.com/cds/retrieve?content=AQICAHiylJ_bvOB56hI8UzTN6Ryruh7a0kiIBN_ANwtaWYjmxwH68UsVoqohxzlRTnqL8XYEAAAA4zCB4AYJKoZIhvcNAQcGoIHSMIHPAgEAMIHJBgkqhkiG9w0BBwEwHgYJYIZIAWUDBAEuMBEEDI9FmU6IICPGtllESgIBEICBm5SY442gpp_ZfQsLSWFh2u5eWytBkbYRcwibKwPSCJrw3KX_XQ0lDDPVWSt33hc8jLXDmDWB1XGYB4P418GHH6rOPWGEtJ7nvQ_q5qUiEGI_dB4bcuvV8gnZgxEWCLuVFgEHKPoWoQC5C8KtoZxbA3RCBvMAJx4cwWIA6r4daPI4tU5bMJoDU4BaqKiNuD-AK7EV-vJWOI45eXoS,1,1,1,HR evaluation of teacher,PS randomly assigned to human evaluator or machine learning algorithm evaluator. DV: How likely are you to renew the teacher's contract? Do Ps make decisions consistent with the evaluator?,NA,"automation bias = ""undue deference to automated systems by human actors that disregard contradictory information from other sources or do not (thoroughly) search for
additional information""; selective adherence to algorithmic advice = rather than accepting automated decision outcomes by default,  adhere when it matches stereotypical views of the decision subject",S Alon-Barkat and M Busuioc,10.1093/jopart/muac007,NA,1,Journal of Public Administration Research and Theory,NA,NA,Cited By :55,153-169,NA,NA,33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193223068&doi=10.1093%2fjopart%2fmuac007&partnerID=40&md5=22d8f72fd2ebd9285594db057e9bb5aa,2023,NA,NA
ID-449,article,Exploring moral algorithm preferences in autonomous vehicle dilemmas: an empirical study,"Introduction: This study delves into the ethical dimensions surrounding autonomous vehicles (AVs), with a specific focus on decision-making algorithms. Termed the “Trolley problem,” an ethical quandary arises, necessitating the formulation of moral algorithms grounded in ethical principles. To address this issue, an online survey was conducted with 460 participants in China, comprising 237 females and 223 males, spanning ages 18 to 70. Methods: Adapted from Joshua Greene’s trolley dilemma survey, our study employed Yes/No options to probe participants’ choices and Likert scales to gauge moral acceptance. The primary objective was to assess participants’ inclinations toward four distinct algorithmic strategies—Utilitarianism, Rawlsianism, Egoism, and a Hybrid approach—in scenarios involving AVs Results: Our findings revealed a significant disparity between participants’ preferences in scenarios related to AV design and those focused on purchase decisions. Notably, over half of the respondents expressed reluctance to purchase AVs equipped with an “egoism” algorithm, which prioritizes the car owner’s safety. Intriguingly, the rejection rate for “egoism” was similar to that of “utilitarianism,” which may necessitate self-sacrifice. Discussion: The hybrid approach, integrating “Utilitarianism” and “Egoism,” garnered the highest endorsement. This highlights the importance of balancing self-sacrifice and harm minimization in AV moral algorithms. The study’s insights are crucial for ethically and practically advancing AV technology in the continually evolving realm of autonomous vehicles.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10713701/,1,1,1,self-driving cars,NA,NA,"compared four different moral algorithms: utilitarian, rawlsian, egoist, hybrid. utilitarian = sacrifice the few to save the many; rawlsian = minimize maximum fatality rate by calculating death rates of car owner vs. other people; egoist = prioritize the safety of the driver; hybrid = utilitiarian + egoist (different priorities depending on the driving scenario)",T Sui,10.3389/fpsyg.2023.1229245,NA,NA,Frontiers in Psychology,NA,NA,Cited By :1,NA,NA,NA,14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179336829&doi=10.3389%2ffpsyg.2023.1229245&partnerID=40&md5=07676216eac475c1dd4ec44eb92d9cfd,2023,NA,NA
ID-454,article,ARTIFICIAL INTELLIGENCE (AI) ADOPTION: AN EXTENDED COMPENSATORY LEVEL OF ACCEPTANCE,"This paper uses a new measurement of AI’s intelligence related to task performance to examine how expectations about the operating efficiency of an AI technology influence the intention to adopt it. We suggest four levels of user acceptance/rejection of AI services, including the level of compensatory acceptance. Our conceptual model is specifically designed for the AI context, with two key variables: cybersecurity and anthropomorphism, and three mediating constructs: i) perceived level of AI’s intelligence, ii) perceived performance expectancy, and iii) perceived effort expectancy. The hypotheses were tested by surveying 494 potential virtual banking users in Hong Kong and analyzing the data with Structural Equation Modelling (SEM). We find that consumer acceptance of AI services is positively related to perceived performance expectancy and effort expectancy and to the perceived level of AI’s intelligence. These findings support an extended behavioral intention: the compensatory level of AI acceptance. Our empirically tested and generalizable results have implications for academics and practitioners © 2023, Journal of Electronic Commerce Research.All Rights Reserved.",https://www.proquest.com/docview/2791348425?accountid=14026&sourcetype=Scholarly%20Journals,1,1,1,e-commerce,"anthropomorphism, perceived intelligence, perceived performance expectancy, perceived effort expectancy",NA,NA,C S K Chow and G Zhan and H Wang and M He,NA,NA,1,Journal of Electronic Commerce Research,NA,NA,Cited By :3,84-106,NA,NA,24,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153773955&partnerID=40&md5=ae223e5e6ae5ae1fa8ccef81568e5267,2023,NA,NA
ID-456,article,Investigating the negative bias towards artificial intelligence: Effects of prior assignment of AI-authorship on the aesthetic appreciation of abstract paintings,"Art, regarded as one of the last bulwarks of human prerogatives, is a valid model for investigating the relationship between humans and Artificial Intelligence (AI). Recent studies investigated the response to human-made vs. AI-made artworks, reporting evidence of either a negative bias towards the latter or no difference. Here, we investigated whether prior knowledge of authorship can influence the aesthetic appreciation of two abstract paintings by manipulating the pre-assignment of human- vs. AI-authorship. In the ecological setting of an art fair, participants were asked to explicitly rate their aesthetic appreciation, while psychophysiological measure - electrodermal activity (EDA) and heart rate (HR) - were recorded during the observation of the two paintings. Presentation order was balanced among participants and artworks. Results show that when the human-declared painting was shown as first, aesthetic judgement on the AI-declared painting were lower, while with the opposite presentation order judgements were equal. Furthermore, although no modulation of HR was found, EDA activation was always higher during the second presentation. In line with literature, the results showed that looking at abstract artworks reduces the negative bias towards AI. However, the negative bias still emerges when AI-artworks are implicitly compared to human-artworks. Implications are discussed.",https://www-sciencedirect-com.stanford.idm.oclc.org/science/article/pii/S074756322200228X?via%3Dihub,1,1,1,visual art,NA,NA,"manipulated the authorship assignment by pre-assigning “Human” or “AI” before each painting's presentation; DVs: psychophysiological response (skin conductance, heart rate), subjective evaluations (how much do you like this painting?)",S G Chiarella and G Torromino and D M Gagliardi and D Rossi and F Babiloni and G Cartocci,10.1016/j.chb.2022.107406,NA,NA,Computers in Human Behavior,NA,NA,Cited By :23,NA,NA,NA,137,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135394696&doi=10.1016%2fj.chb.2022.107406&partnerID=40&md5=b80ffe2d9a323e92f0f47058544866e6,2022,NA,NA
ID-460,article,Artificial intelligence in public services: When and why citizens accept its usage,"Interest in implementing artificial intelligence (AI)–based software in the public sector is growing. First implementations and research in individual public services have already been carried out; however, a better understanding of citizens' acceptance of this technology is missing in the public sector, as insights from the private sector cannot be transferred directly. For this purpose, we conduct policy-capturing experiments to analyze AI's acceptance in six representative scenarios. Based on behavioral reasoning theory, we gather evidence from 329 participants. The results show that AI solutions in general public services are preferred over those provided by humans, but specific services are still a human domain. Further analyses show that the major drivers toward acceptance are the reasons against AI. The results contribute to understanding of when and why AI is accepted in public services. Public administration can use the results to identify AI-based software to invest in and communicate their usage to perceive such investments' high acceptance rates. © 2022 The Authors",https://www.sciencedirect.com/science/article/pii/S0740624X22000375,1,1,1,"public services:  (1) specific administration, social, and education: exhibition; (2) specific security and health: life-threatening disease; (3) specific infrastructure: waste management; (4) general administration, social, and education: youth aid in hot spots; (5) general security and health: bushfires; and (6) general infrastructure: bridge",NA,NA,NA,T S Gesk and M Leyer,doi.org/10.1016/j.giq.2022.101704,NA,3,Government Information Quarterly,NA,NA,Cited By :39,NA,NA,NA,39,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127463083&doi=10.1016%2fj.giq.2022.101704&partnerID=40&md5=f2252d57e3fb2ab79b44200686af9486,2022,NA,NA
ID-464,article,Comparison of Cognitive Differences of Artworks between Artist and Artistic Style Transfer,"This study explores how audiences responded to perceiving and distinguishing the paintings created by AI or human artists. The stimuli were six paintings which were completed by AI and human artists. A total of 750 subjects participated to identify which ones were completed by human artists or by AI. Results revealed that most participants could correctly distinguish between paintings made by AI or human artists and that accuracy was higher for those who used “intuition” as the criterion for judgment. The participants preferred the paintings created by human artists. Furthermore, there were big differences in the perception of the denotation and connotation of paintings between audiences of different backgrounds. The reasons for this will be analyzed in subsequent research.",https://www.mdpi.com/2076-3417/12/11/5525,1,1,1,visual art,NA,NA,NA,Y Sun and Y Lyu and P.-H. Lin and R Lin,10.3390/app12115525,NA,11,Applied Sciences (Switzerland),NA,NA,Cited By :3,NA,NA,NA,12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131569122&doi=10.3390%2fapp12115525&partnerID=40&md5=abd0bd028f04c2ee5ee0717c46c50e9b,2022,NA,NA
ID-468,article,People Prefer Moral Discretion to Algorithms: Algorithm Aversion Beyond Intransparency,"We explore aversion to the use of algorithms in moral decision-making. So far, this aversion has been explained mainly by the fear of opaque decisions that are potentially biased. Using incentivized experiments, we study which role the desire for human discretion in moral decision-making plays. This seems justified in light of evidence suggesting that people might not doubt the quality of algorithmic decisions, but still reject them. In our first study, we found that people prefer humans with decision-making discretion to algorithms that rigidly apply exogenously given human-created fairness principles to specific cases. In the second study, we found that people do not prefer humans to algorithms because they appreciate flesh-and-blood decision-makers per se, but because they appreciate humans’ freedom to transcend fairness principles at will. Our results contribute to a deeper understanding of algorithm aversion. They indicate that emphasizing the transparency of algorithms that clearly follow fairness principles might not be the only element for fostering societal algorithm acceptance and suggest reconsidering certain features of the decision-making process. © 2022, The Author(s).",https://link.springer.com/article/10.1007/s13347-021-00495-y?utm_source=getftr&utm_medium=getftr&utm_campaign=getftr_pilot,1,1,1,NA,"empathy, moral discretion/autonomy (ability to override a fairness principle in the case of an exception that requires human empathy)",NA,Is there more to algorithm aversion than a fear of opacity? Study used monetary incentives to assess preferences for human vs. algorithmic decision-makers in moral domain,J Jauernig and M Uhl and G Walkowitz,10.1007/s13347-021-00495-y,NA,1,Philosophy and Technology,NA,NA,Cited By :15,NA,NA,NA,35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123779996&doi=10.1007%2fs13347-021-00495-y&partnerID=40&md5=b936efc27defd893924946514c66482d,2022,NA,NA
ID-469,article,AI invading the workplace: negative emotions towards the organizational use of personal virtual assistants,"Personal virtual assistants (PVAs) based on artificial intelligence are frequently used in private contexts but have yet to find their way into the workplace. Regardless of their potential value for organizations, the relentless implementation of PVAs at the workplace is likely to run into employee resistance. To understand what motivates such resistance, it is necessary to investigate the primary motivators of human behavior, namely emotions. This paper uncovers emotions related to organizational PVA use, primarily focusing on threat emotions. To achieve our goal, we conducted an in-depth qualitative study, collecting data from 45 employees in focus-group discussions and individual interviews. We identified and categorized emotions according to the framework for classifying emotions Beaudry and Pinsonneault (2010) designed. Our results show that loss emotions, such as dissatisfaction and frustration, as well as deterrence emotions, such as fear and worry, constitute valuable cornerstones for the boundaries of organizational PVA use. © 2021, The Author(s).",https://link.springer.com/article/10.1007/s12525-021-00493-0?utm_source=getftr&utm_medium=getftr&utm_campaign=getftr_pilot,1,2,2,personal virtual assistants in the workplace,NA,NA,"focus groups, qualitative data. appraisal of virtual assistance can be conceptualized in terms of threat vs. opportunity and perceived sense of control, resulting in different emotions. See Figure 1",O Hornung and S Smolnik,10.1007/s12525-021-00493-0,NA,1,Electronic Markets,NA,NA,Cited By :23,123-138,NA,NA,32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115141762&doi=10.1007%2fs12525-021-00493-0&partnerID=40&md5=902663f30f7461ffa79ba4912d2f9e37,2022,NA,NA
ID-471,article,Examining perceptions towards hiring algorithms,"Companies are increasingly turning to AI software to select candidates, despite concerns that hiring algorithms may produce biased evaluations. This study explores the public perceptions of algorithms used in resume and video interview screening. In addition, the effects of individual characteristics on these perceptions are examined. Using a nationally representative sample, we find that the public generally has a negative attitude towards the use of algorithms in hiring, and the majority do not consider them fair and effective. We also find clear individual differences regarding the perceptions towards algorithms. Specifically, males, people with higher education level and people with higher income have more positive perceptions towards hiring algorithms than their counterparts. The findings contribute to the emerging body of research on hiring algorithms and suggest strategies to increase public acceptance of hiring algorithms. © 2022",https://www.sciencedirect.com/science/article/pii/S0160791X21003237?via%3Dihub,1,1,1,recruiting,"fairness, effectiveness",NA,NA,L Zhang and C Yencha,10.1016/j.techsoc.2021.101848,NA,NA,Technology in Society,NA,NA,Cited By :18,NA,NA,NA,68,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122190388&doi=10.1016%2fj.techsoc.2021.101848&partnerID=40&md5=1d4f4647c1e83e1764fd96360bdbf547,2022,NA,NA
ID-477,article,Algorithmic versus Human Advice: Does Presenting Prediction Performance Matter for Algorithm Appreciation?,"We propose a theoretical model based on the judge-advisor system (JAS) and empirically examine how algorithmic advice, compared to identical advice from humans, influences human judgment. This effect is contingent on the level of transparency, which varies with whether and how the prediction performance of the advice source is presented. In a series of five controlled behavioral experiments, we show that individuals largely exhibit algorithm appreciation; that is, they follow algorithmic advice to a greater extent than identical human advice due to a higher trust in an algorithmic than human advisor. Interestingly, neither the extent of higher trust in algorithmic advisors nor the level of algorithm appreciation decreases when individuals are informed of the algorithm’s prediction errors (i.e., upon presenting prediction performance in an aggregated format). By contrast, algorithm appreciation declines when the transparency of the advice source’s prediction performance further increases through an elaborated format. This is plausibly because the greater cognitive load imposed by the elaborated format impedes advice taking. Finally, we identify a boundary condition: algorithm appreciation is reduced for individuals with a lower dispositional need for cognition. Our findings provide key implications for research and managerial practice. © 2022 Taylor & Francis Group, LLC.",https://content.ebscohost.com/cds/retrieve?content=AQICAHiylJ_bvOB56hI8UzTN6Ryruh7a0kiIBN_ANwtaWYjmxwHnjlgOIar36sj-nFfaFVTKAAAA4zCB4AYJKoZIhvcNAQcGoIHSMIHPAgEAMIHJBgkqhkiG9w0BBwEwHgYJYIZIAWUDBAEuMBEEDLpFqLdwctpCv_9wkQIBEICBm4VBKoo48peciUVJa2LhnAO2cvvW6E-i5fSTZac9wdmgutN6oR3W-ma__udfROfLnjFAeAq3qZkYtU-BT53HQpcrawRAvweh8grJfbcs6JOETiVHorLr9K0kqb7xMYrOf4GWqO7ylzS8DqkigGIq82DNA3MqPAMyy92ETIqKtdORjGcrglIbo4C1tgsxwe4IOi4P1OXDy15PKkSs,1,1,1,predictions,transparency,NA,"WOA to measure algorithm appreciation; transparency as a predictor of trust, which predicts algorithm appreciation. Note that transparency doesn't necessarily mean a technical explanation of how the algorithm works. ""recent studies on human–AI collaboration imply that transparency in why an algorithm is trustworthy from a user-centric perspective—instead of how an algorithm works from a technic-centric perspective—is more relevant to individuals’ use of algorithmic decision aids""",S You and C L Yang and X Li,10.1080/07421222.2022.2063553,NA,2,Journal of Management Information Systems,NA,NA,Cited By :19,336-365,NA,NA,39,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131373656&doi=10.1080%2f07421222.2022.2063553&partnerID=40&md5=10754faa10a8bdbed230ab6783107230,2022,NA,NA
ID-480,article,Drivers of salespeople’s AI acceptance: what do managers think?,"This research is among the first to examine salespeople’s acceptance of AI (artificial intelligence) and we investigate the drivers of their AI acceptance from the perspective of the managers. In this study, we propose and empirically demonstrate that perceived ease of use, self-efficacy, perceived management support, and digitalization are positively related to salespeople’s acceptance of AI. Moreover, we show that digitalization mediates the relationship between salespeople’s prospecting/adaptive selling capabilities and their AI acceptance. The results suggest that in order to incentivize AI acceptance, managers need to build adequate digital infrastructure, cultivate organizational support to encourage AI adoption and usage, provide professional training to educate salespeople on the proper usage of AI, and reduce salespeople’s perceived risk of AI usage. Theoretical and managerial implications are discussed subsequently. © 2022 Pi Sigma Epsilon National Educational Foundation.",https://www.tandfonline.com/doi/epdf/10.1080/08853134.2021.2016058?src=getftr,1,1,1,sales,NA,NA,NA,J Chen and W Zhou,10.1080/08853134.2021.2016058,NA,2,Journal of Personal Selling and Sales Management,NA,NA,Cited By :11,107-120,NA,NA,42,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122922683&doi=10.1080%2f08853134.2021.2016058&partnerID=40&md5=7f1712a64f0d0e5ba5e8e5858154ec7c,2022,NA,NA
ID-481,article,Persuasive robots should avoid authority: The effects of formal and real authority on persuasion in human-robot interaction,"Social robots must take on many roles when interacting with people in everyday settings, some of which may be authoritative, such as a nurse, teacher, or guard. It is important to investigate whether and how authoritative robots can influence people in applications ranging from health care and education to security and in the home. Here, we present a human-robot interaction study that directly investigates the effect of a robot's peer or authority role (formal authority) and control of monetary rewards and penalties (real authority) on its persuasive influence. The study consisted of a social robot attempting to persuade people to change their answers to the robot's suggestion in a series of challenging attention and memory tasks. Our results show that the robot in a peer role was more persuasive than when in an authority role, contrary to expectations from human-human interactions. The robot was also more persuasive when it offered rewards over penalties, suggesting that participants perceived the robot's suggestions as a less risky option than their own estimates, in line with prospect theory. In general, the results show an aversion to the persuasive influence of authoritative robots, potentially due to the robot's legitimacy as an authority figure, its behavior being perceived as dominant, or participant feelings of threatened autonomy. This paper explores the importance of persuasion for robots in different social roles while providing critical insight into the perception of robots in these roles, people's behavior around these robots, and the development of human-robot relationships. © 2021 American Association for the Advancement of Science. All rights reserved.",https://www.science.org/doi/epdf/10.1126/scirobotics.abd5186?src=getftr,1,1,1,NA,NA,NA,social robots. may be excluded upon 2nd round of coding. varied perceived role of robot (peer or authority) and whether the robot offered penalties or rewards. DV: whether participant was persuaded by the robot to change their answers in attention and memory tasks,S P Saunderson and G Nejat,10.1126/scirobotics.abd5186,NA,58,Science Robotics,NA,NA,Cited By :21,NA,NA,NA,6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116816421&doi=10.1126%2fscirobotics.abd5186&partnerID=40&md5=78cf72ef88c295eb4fa20c9926d683e9,2021,NA,NA
ID-489,article,The most human bot: Female gendering increases humanness perceptions of bots and acceptance of AI,"Companies have repeatedly launched Artificial Intelligence (AI) products such as intelligent chatbots and robots with female names, voices, and bodies. Previous research posits that people intuitively favor female over male bots, mainly because female bots are judged as warmer and more likely to experience emotions. We present five online studies, including four preregistered, with a total sample of over 3,000 participants that go beyond this longstanding perception of femininity. Because warmth and experience (but not competence) are seen as fundamental qualities to be a full human but are lacking in machines, we argue that people prefer female bots because they are perceived as more human than male bots. Using implicit, subtle, and blatant scales of humanness, our results consistently show that women (Studies 1A and 1B), female bots (Studies 2 and 3), and female chatbots (Study 4) are perceived as more human than their male counterparts when compared with non-human entities (animals and machines). Study 4 investigates explicitly the acceptance of gendered algorithms operated by AI chatbots in a health context. We found that the female chatbot is preferred over the male chatbot because it is perceived as more human and more likely to consider our unique needs. These results highlight the ethical quandary faced by AI designers and policymakers: Women are said to be transformed into objects in AI, but injecting women's humanity into AI objects makes these objects seem more human and acceptable. © 2021 Wiley Periodicals LLC",https://content.ebscohost.com/cds/retrieve?content=AQICAHiylJ_bvOB56hI8UzTN6Ryruh7a0kiIBN_ANwtaWYjmxwG0p2ExQD8alRwUQDhMmWorAAAA4zCB4AYJKoZIhvcNAQcGoIHSMIHPAgEAMIHJBgkqhkiG9w0BBwEwHgYJYIZIAWUDBAEuMBEEDFA5iVeLpYp7AIHH-QIBEICBm78q9R831nm2Z93Q_icsCa5Y64a546w6DaxazaElGEXF2o7wV7gKDkIy9b4CGXU7RfHslVpFgNiFtJmxK1REUlz2MdP7toVtovCUsLPFoIyrq7YztAZ6vpy_LTO3-XpNoAh9Y2XFb5yLbz3kPgyzGZi_jmdmRr5-C_KZydNhTWZd_eBSxVBHM4y7L8XCpKkJ4wkusDFR6H4HZQqO,1,1,1,NA,perceived humanness,NA,"chatbots and robots with female presentations are seen as more human-like because they are judged as warmer and more likely to experience emotions. Female chatbots and robots perceived as more likely to consider humans' ""uniqueness""",S Borau and T Otterbring and S Laporte and S Fosso Wamba,10.1002/mar.21480,NA,7,Psychology and Marketing,NA,NA,Cited By :101,1052-1068,NA,NA,38,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102815956&doi=10.1002%2fmar.21480&partnerID=40&md5=328ea7857dcebd7d0840622ade8a4f7b,2021,NA,NA
ID-492,article,Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer,"Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in ‘simulated’ environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n = 50) and a prospective clinical deployment (n = 50) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47 h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake. © 2021, The Author(s), under exclusive licence to Springer Nature America, Inc.",https://www.nature.com/articles/s41591-021-01359-w,1,1,1,medical,NA,"compared actual human-generated vs. ML-generated treatment plans. Physicians assessed these plans based on a priori defined criteria. On average, ML-generated plans were considered more clinically acceptable than human-generated plans (89% vs. 72%). Contribution: This study demonstrates the superior effectiveness of ML-generated treatment plans, gives reason for it to be used as primary treatment plan rather than just an assistant",NA,C McIntosh and L Conroy and M C Tjong and T Craig and A Bayley and C Catton and M Gospodarowicz and J Helou and N Isfahanian and V Kong and T Lam and S Raman and P Warde and P Chung and A Berlin and T G Purdie,10.1038/s41591-021-01359-w,NA,6,Nature Medicine,NA,NA,Cited By :98,999-1005,NA,NA,27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107276314&doi=10.1038%2fs41591-021-01359-w&partnerID=40&md5=17a398a4f1952d01860e902e08178f35,2021,NA,NA
ID-497,article,When will workers follow an algorithm? A field experiment with a retail business,"This paper develops a new algorithm for increasing the revenue in a dynamic product assortment problem. Then, it identifies the challenges faced by managers in practice and discusses the conditions under which workers follow the algorithm. To do so, I conducted a field experiment with a beverage vending machine business. The experiment shows that, on average, workers are reluctant to follow the algorithmic advice; however, the workers are more willing to conform once their forecasts are integrated into the algorithm. Analyses using nonexperimental variations highlight the importance of taking worker and context heterogeneity into account to maximize the benefit from adopting a new algorithm. Higher worker’s regret, sales volatility, and fewer delegations increase the conformity, while they mitigate the effects of integration. Workers avoid high-traffic vending machines and focus on machines with high sales volatility when adopting the algorithm. The effects on the sales are largely similar to the effects on product assortments. The results emphasize the gap between nominal and actual performance of an algorithm and several practical issues to be resolved. Copyright: © 2020 INFORMS",https://pubsonline-informs-org.stanford.idm.oclc.org/doi/epdf/10.1287/mnsc.2020.3599,1,1,1,business,NA,NA,NA,K Kawaguchi,10.1287/mnsc.2020.3599,NA,3,Management Science,NA,NA,Cited By :37,1670-1695,NA,NA,67,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102106344&doi=10.1287%2fmnsc.2020.3599&partnerID=40&md5=8c8b05098a1fcf89e0b3657fe379afa7,2021,NA,NA
ID-499,article,Two distinct and separable processes underlie individual differences in algorithm adherence: Differences in predictions and differences in trust thresholds,"Algorithms play an increasingly ubiquitous and vitally important role in modern society. However, recent findings suggest substantial individual variability in the degree to which people make use of such algorithmic systems, with some users preferring the advice of algorithms whereas others selectively avoid algorithmic systems. The mechanisms that give rise to these individual differences are currently poorly understood. Previous studies have suggested two possible effects that may underlie this variability: users may differ in their predictions of the efficacy of algorithmic systems, and/or in the relative thresholds they hold to place trust in these systems. Based on a novel judgment task with a large number of within-subject repetitions, here we report evidence that both mechanisms exert an effect on experimental participant's degree of algorithm adherence, but, importantly, that these two mechanisms are independent from each-other. Furthermore, participants are more likely to place their trust in an algorithmically managed fund if their first exposure to the task was with an algorithmic manager. These findings open the door for future research into the mechanisms driving individual differences in algorithm adherence, and allow for novel interventions to increase adherence to algorithms. © 2021 Fenneman et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0247084,1,1,1,investment,NA,NA,"downsides to algorithm aversion: missing out in areas where algorithms are objectively superior to humans, eventually people who are irrationally algorithm-averse will be at a disadvantage in the workforce. How do (1) users' predictions about the efficacy of an algorithm and (2) their thresholds of trust affect their degree of algorithm adherence? DV: difference in investment rates between human and algorithmic fund managers",A Fenneman and J Sickmann and T Pitz and A G Sanfey,10.1371/journal.pone.0247084,NA,2 February,PLoS ONE,NA,NA,Cited By :7,NA,NA,NA,16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102097401&doi=10.1371%2fjournal.pone.0247084&partnerID=40&md5=700cd9eda6f2c9679e0ca7795831d66d,2021,NA,NA
ID-500,article,Watch Me Improve—Algorithm Aversion and Demonstrating the Ability to Learn,"Owing to advancements in artificial intelligence (AI) and specifically in machine learning, information technology (IT) systems can support humans in an increasing number of tasks. Yet, previous research indicates that people often prefer human support to support by an IT system, even if the latter provides superior performance – a phenomenon called algorithm aversion. A possible cause of algorithm aversion put forward in literature is that users lose trust in IT systems they become familiar with and perceive to err, for example, making forecasts that turn out to deviate from the actual value. Therefore, this paper evaluates the effectiveness of demonstrating an AI-based system’s ability to learn as a potential countermeasure against algorithm aversion in an incentive-compatible online experiment. The experiment reveals how the nature of an erring advisor (i.e., human vs. algorithmic), its familiarity to the user (i.e., unfamiliar vs. familiar), and its ability to learn (i.e., non-learning vs. learning) influence a decision maker’s reliance on the advisor’s judgement for an objective and non-personal decision task. The results reveal no difference in the reliance on unfamiliar human and algorithmic advisors, but differences in the reliance on familiar human and algorithmic advisors that err. Demonstrating an advisor’s ability to learn, however, offsets the effect of familiarity. Therefore, this study contributes to an enhanced understanding of algorithm aversion and is one of the first to examine how users perceive whether an IT system is able to learn. The findings provide theoretical and practical implications for the employment and design of AI-based systems. © 2020, The Author(s).",https://link.springer.com/article/10.1007/s12599-020-00678-5?utm_source=getftr&utm_medium=getftr&utm_campaign=getftr_pilot,1,1,1,IT,"ability to learn, familiarity to the user",NA,"no main effect of identity of the advisor (human vs. algorithmic advisor), but there was an interaction between the identity and its ability to learn. DV: WOA, self-reported familiarity. Increased WOA for algorithmic advisor when it learned (compared to when it did not). decreased WOA for human advisor. See Figure 4",B Berger and M Adam and A Rühr and A Benlian,10.1007/s12599-020-00678-5,NA,1,Business and Information Systems Engineering,NA,NA,Cited By :73,55-68,NA,NA,63,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097167269&doi=10.1007%2fs12599-020-00678-5&partnerID=40&md5=7c3e8ae4b05f7babf40ab6957e3aacb8,2021,NA,NA
ID-502,article,From satisficing to artificing: The evolution of administrative decision-making in the age of the algorithm,"Algorithmic decision tools (ADTs) are being introduced into public sector organizations to support more accurate and consistent decision-making. Whether they succeed turns, in large part, on how administrators use these tools. This is one of the first empirical studies to explore how ADTs are being used by Street Level Bureaucrats (SLBs). The author develops an original conceptual framework and uses in-depth interviews to explore whether SLBs are ignoring ADTs (algorithm aversion); deferring to ADTs (automation bias); or using ADTs together with their own judgment (an approach the author calls artificing). Interviews reveal that artificing is the most common use-type, followed by aversion, while deference is rare. Five conditions appear to influence how practitioners use ADTs: (a) understanding of the tool (b) perception of human judgment (c) seeing value in the tool (d) being offered opportunities to modify the tool (e) alignment of tool with expectations. © The Author(s) 2021. Published by Cambridge University Press in association with Data for Policy.",https://www.cambridge.org/core/journals/data-and-policy/article/from-satisficing-to-artificing-the-evolution-of-administrative-decisionmaking-in-the-age-of-the-algorithm/8962400DADAC3C740AC023A20B38E285,1,2,2,public sector,NA,NA,"interesting quotes from administrators, exemplify reasons for algorithm aversion (belief in superiority of human judgment, poor understanding of the tool, no opportunity to modify) and automation bias (recognize limitations of human judgment, solid understanding of the tool, opportunity to modify).",T Snow,10.1017/dap.2020.25,NA,2,Data and Policy,NA,NA,Cited By :6,NA,NA,NA,3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138788421&doi=10.1017%2fdap.2020.25&partnerID=40&md5=99bbae9b9bfd7ece6a77d7b94ebe868d,2021,NA,NA
ID-503,article,Artificial intelligence in the fashion industry: consumer responses to generative adversarial network (GAN) technology,"Purpose: This study examines consumers' evaluations of product consumption values, purchase intentions and willingness to pay for fashion products designed using generative adversarial network (GAN), an artificial intelligence technology. This research investigates differences between consumers' evaluations of a GAN-generated product and a non-GAN-generated product and tests whether disclosing the use of GAN technology affects consumers' evaluations. Design/methodology/approach: Sample products were developed as experimental stimuli using cycleGAN. Data were collected from 163 members of Generation Y. Participants were assigned to one of the three experimental conditions (i.e. non-GAN-generated images, GAN-generated images with disclosure and GAN-generated images without disclosure). Regression analysis and ANOVA were used to test the hypotheses. Findings: Functional, social and epistemic consumption values positively affect willingness to pay in the GAN-generated products. Relative to non-GAN-generated products, willingness to pay is significantly higher for GAN-generated products. Moreover, evaluations of functional value, emotional value and willingness to pay are highest when GAN technology is used, but not disclosed. Originality/value: This study evaluates the utility of GANs from consumers' perspective based on the perceived value of GAN-generated product designs. Findings have practical implications for firms that are considering using GANs to develop products for the retail fashion market. © 2020, Emerald Publishing Limited.",https://www.proquest.com/docview/2473332983/fulltextPDF/F7AE6E77C6AF4E70PQ/1?accountid=14026&sourcetype=Scholarly%20Journals,1,1,1,fashion industry,NA,NA,NA,K Sohn and C E Sung and G Koo and O Kwon,10.1108/IJRDM-03-2020-0091,NA,1,International Journal of Retail and Distribution Management,NA,NA,Cited By :34,61-80,NA,NA,49,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091022473&doi=10.1108%2fIJRDM-03-2020-0091&partnerID=40&md5=12f57718da666f72e6b0ccd343347ada,2021,NA,NA
ID-504,article,The Effects of Situational and Individual Factors on Algorithm Acceptance in COVID-19-Related Decision-Making: A Preregistered Online Experiment,"In times of the COVID-19 pandemic, difficult decisions such as the distribution of ventilators must be made. For many of these decisions, humans could team up with algorithms; however, people often prefer human decision-makers. We examined the role of situational (morality of the scenario; perspective) and individual factors (need for leadership; conventionalism) for algorithm preference in a preregistered online experiment with German adults (n = 1,127). As expected, algorithm preference was lowest in the most moral-laden scenario. The effect of perspective (i.e., decision-makers vs. decision targets) was only significant in the most moral scenario. Need for leadership predicted a stronger algorithm preference, whereas conventionalism was related to weaker algorithm preference. Exploratory analyses revealed that attitudes and knowledge also mattered, stressing the importance of individual factors. © 2021 Authors.",https://stars.library.ucf.edu/hmc/vol3/iss1/3/,1,1,1,medical decision-making,NA,NA,NA,S Utz and L N Wolfers and A S Göritz,10.30658/hmc.3.3,NA,1,Human-Machine Communication,NA,NA,Cited By :6,27-45,NA,NA,3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146426439&doi=10.30658%2fhmc.3.3&partnerID=40&md5=fea2c995f8649afd4b9d01b2fcf1fd53,2021,NA,NA
ID-505,article,To App or Not to App? Understanding Public Resistance to COVID-19 Digital Contact Tracing and its Criminological Relevance,"In the context of the COVID-19 pandemic, digital contact tracing has been developed and promoted in many countries as a valuable tool to help the fight against the virus, allowing health authorities to react quickly and limit contagion. Very often, however, these tracing apps have faced public resistance, making their use relatively sparse and ineffective. Our study relies on an interdisciplinary approach that brings together criminological and computational expertise to consider the key social dynamics underlying people’s resistance to using the NHS contact-tracing app in England and Wales. The present study analyses a large Twitter dataset to investigate interactions between relevant user accounts and identify the main narrative frames (lack of trust and negative liberties) and mechanisms (polluted information, conspiratorial thinking and reactance) to explain resistance towards use of the NHS contact-tracing app. Our study builds on concepts of User eXperience (UX) and algorithm aversion and demonstrates the relevance of these elements to the key criminological problem of resistance to official technologies. © The Author/s 2021",https://lthj.qut.edu.au/article/view/2012/1154,1,"1, 2",1,medical,NA,NA,NA,A Lavorgna and P Ugwudike and L Carr and Y S Benitez and G S Rekha,10.5204/lthj.2012,NA,2,"Law, Technology and Humans",NA,NA,Cited By :2,28-45,NA,NA,3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129298507&doi=10.5204%2flthj.2012&partnerID=40&md5=c5e1f846d0081c5c20f41959e2d8a88b,2021,NA,NA
ID-507,article,Algorithm appreciation or aversion? Comparing in-service and pre-service teachers’ acceptance of computerized expert models,"Although computerized expert models (i.e., algorithms) could improve educational decisions and judgments, initial research has demonstrated that teachers, like other professional groups, tend to be “algorithm averse.” In the current study, we use behavioral and questionnaire data to examine the extent to which in-service and pre-service (i.e., students in training to become) teachers accept advice from expert models and investigate how teachers' acceptance of expert models could be improved. Although it is often presumed that younger generations are less algorithm averse, we demonstrate that both in-service and pre-service teachers prefer advice from a human source (school counselor) than from an expert model, to a similar extent. Furthermore, we find that advice acceptance depends on the difficulty of the decision task, but we find no evidence that pre-service teachers’ acceptance of computerized advice depends on their numeracy or the Big Five traits of openness and neuroticism. Finally, we find that in-service teachers lacked knowledge of computerized expert models but indicated that advice from expert models would be superior to human advice in certain kinds of tasks. Our results indicate that both in- and pre-service teachers could profit from training about the definition and value of computerized expert models, and we provide suggestions for training and future research. © 2021 The Author",https://www.sciencedirect.com/science/article/pii/S2666920X21000229?pes=vor,1,1,1,education,NA,task difficulty,NA,E Kaufmann,10.1016/j.caeai.2021.100028,NA,NA,Computers and Education: Artificial Intelligence,NA,NA,Cited By :12,NA,NA,NA,2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123935753&doi=10.1016%2fj.caeai.2021.100028&partnerID=40&md5=2017d40bc3349fac0e9ed83b0063764d,2021,NA,NA
ID-508,article,Artificial intelligence acceptance in services: connecting with Generation Z,"This paper aims to examine willingness to accept artificial intelligence (AI) devices, focusing on the so-called Gen Z population. This study presumes that specific knowledge of a business process is important for AI adoption in hospitality services. A research model, grounded in the artificially intelligent device use acceptance (AIDUA) framework, used data collected from 786 respondents. The model was tested using PLS-SEM methodology. The modified framework was supported by Gen Z, with hedonic motivation having the greatest effect on Gen Z members’ emotions and their willingness to use AI devices in hospitality. The frequency of smartphone usage played a significant moderating role between the perceived effort of AI usage and emotions. This study helps AI designers and business managers when designing and implementing AI devices in a hospitality environment. Based on this study’s findings, policymakers and educational institutions can try to advance their curricula, emphasizing the importance of new technologies. © 2021 Informa UK Limited, trading as Taylor & Francis Group.",https://content.ebscohost.com/cds/retrieve?content=AQICAHiylJ_bvOB56hI8UzTN6Ryruh7a0kiIBN_ANwtaWYjmxwHinR9-Y5KqoZzezE1-_CZLAAAA4zCB4AYJKoZIhvcNAQcGoIHSMIHPAgEAMIHJBgkqhkiG9w0BBwEwHgYJYIZIAWUDBAEuMBEEDO3Zqoo73uxhmiC-DAIBEICBmwai-xQj60sfStaB0qIrh3gv2eNPlu6fwczL1sr8AFZMU2NeBYyZFl2FGDahPjL_feq2BgUZbXEN6oDs5Io8dQpSWP6N6ljXcn2as1J4U7sZMxQ70ewE-jrrKVJTNbpjudD5C5gZg5D-OBDrx_pvd4xjpCrr_dihTfe7OLb7ZJXz0lrtlx2VJXxtM3OK_agngUry8jGhkBjbYG_4,1,1,1,hopsitality,anthropomorphism,NA,"survey with self-report measures. considered hedonic motivation (pleasure derived from using AI), social influence, smartphone use, perceived effort and competence",V Vitezić and M Perić,10.1080/02642069.2021.1974406,NA,13-14,Service Industries Journal,NA,NA,Cited By :55,926-946,NA,NA,41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114399370&doi=10.1080%2f02642069.2021.1974406&partnerID=40&md5=b30e3e8730c4ea0f8fe227f503b8e4e0,2021,NA,NA
ID-509,article,Can Algorithms Legitimize Discrimination?,"Algorithms have been the subject of a heated debate regarding their potential to yield biased decisions. Prior research has focused on documenting algorithmic bias and discussing its origins from a technical standpoint. We look at algorithmic bias from a psychological perspective, raising a fundamental question that has received little attention: are people more or less likely to perceive decisions that yield disparities as biased, when such decisions stem from algorithms as opposed to humans? We find that algorithmic decisions that yield gender or racial disparities are less likely to be perceived as biased than human decisions. This occurs because people believe that algorithms, unlike humans, decontextualize decision-making by neglecting individual characteristics and blindly applying rules and procedures irrespective of whom they are judging. In situations that entail the potential for discrimination, this belief leads people to think that algorithms are more likely than humans to treat everyone equally, thus less likely to yield biased decisions. This asymmetrical perception of bias, which occurs both in the general population and among members of stigmatized groups, leads people to endorse stereotypical beliefs that fuel discrimination and reduces their willingness to act against potentially discriminatory outcomes © 2021 American Psychological Association",https://psycnet-apa-org.stanford.idm.oclc.org/fulltext/2021-28943-001.pdf,1,1,1,"university admissions, hiring, parole",NA,NA,Ps viewed algorithms as less biased for these high-stakes decisions precisely because they neglect uniqueness,A Bonezzi and M Ostinelli,10.1037/xap0000294,NA,2,Journal of Experimental Psychology: Applied,NA,NA,Cited By :30,447-459,NA,NA,27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112125462&doi=10.1037%2fxap0000294&partnerID=40&md5=da2d32b40f56d3631c9272b0b28dc2f4,2021,NA,NA
ID-516,article,Artificial intelligence versus Maya Angelou: Experimental evidence that people cannot differentiate AI-generated from human-written poetry,"The release of openly available, robust natural language generation algorithms (NLG) has spurred much public attention and debate. One reason lies in the algorithms' purported ability to generate humanlike text across various domains. Empirical evidence using incentivized tasks to assess whether people (a) can distinguish and (b) prefer algorithm-generated versus human-written text is lacking. We conducted two experiments assessing behavioral reactions to the state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal = 830). Using the identical starting lines of human poems, GPT-2 produced samples of poems. From these samples, either a random poem was chosen (Human-out-of-theloop) or the best one was selected (Human-in-the-loop) and in turn matched with a human-written poem. In a new incentivized version of the Turing Test, participants failed to reliably detect the algorithmicallygenerated poems in the Human-in-the-loop treatment, yet succeeded in the Human-out-of-the-loop treatment. Further, people reveal a slight aversion to algorithm-generated poetry, independent on whether participants were informed about the algorithmic origin of the poem (Transparency) or not (Opacity). We discuss what these results convey about the performance of NLG algorithms to produce human-like text and propose methodologies to study such learning algorithms in human-agent experimental settings. © 2020 The Authors",https://ct.prod.getft.io/c2NvcHVzLGVsc2V2aWVyLGh0dHBzOi8vd3d3LnNjaWVuY2VkaXJlY3QuY29tL3NjaWVuY2UvYXJ0aWNsZS9waWkvUzA3NDc1NjMyMjAzMDMwMzQ_cGVzPXZvcg.I5da3U7QTZDr6KBrC0O5lQofcbjObQ5dNDuyh_d8p14,1,1,1,poetry,NA,NA,included an incentivized version of the Turing Test. Human judges were paid for every accurately judged poem (human-written or AI-generated),N Köbis and L D Mossink,10.1016/j.chb.2020.106553,NA,NA,Computers in Human Behavior,NA,NA,Cited By :129,NA,NA,NA,114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091228234&doi=10.1016%2fj.chb.2020.106553&partnerID=40&md5=20ddb9ea059a981591f42da39df6b3b7,2021,NA,NA
ID-521,article,"Matter over mind? How the acceptance of digital entities depends on their appearance, mental prowess, and the interaction between both","Digital technologies are advancing rapidly, growing to be more human-like and intelligent by the day. However, research shows that a machine's resemblance to humans can reach a critical level, which makes it seem uncanny to observers. While scholars have discussed this effect in terms of both human-like appearances and mental abilities, a potential interaction between the two aspects has hardly been addressed in literature. We designed a two-factorial experiment to overcome the identified research gap, introducing participants to digital agents with varying embodiment (text interface/human rendering) and mental capacity (simple algorithms/complex artificial intelligence). Our results show that the interaction of both factors indeed affects participants’ experience in a crucial way: Whereas an agent based on simple algorithms only evokes discomfort when embedded in a human-like body, the artificial intelligence is always perceived as eerie, regardless of its embodiment. Yet, additional findings raise doubts on the unidimensionality of participants’ affective response. © 2020 Elsevier Ltd",https://www.sciencedirect.com/science/article/pii/S1071581920300653?via%3Dihub,1,1,1,NA,human embodiment (made to look like a human or not) and actual ability of the algorithm (simple or complex),NA,uncanny valley,J.-P. Stein and M Appel and A Jost and P Ohler,10.1016/j.ijhcs.2020.102463,NA,NA,International Journal of Human Computer Studies,NA,NA,Cited By :35,NA,NA,NA,142,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084793463&doi=10.1016%2fj.ijhcs.2020.102463&partnerID=40&md5=84c4ce9583bb47fce8330aaab458d165,2020,NA,NA
ID-524,article,In AI we trust? Perceptions about automated decision-making by artificial intelligence,"Fueled by ever-growing amounts of (digital) data and advances in artificial intelligence, decision-making in contemporary societies is increasingly delegated to automated processes. Drawing from social science theories and from the emerging body of research about algorithmic appreciation and algorithmic perceptions, the current study explores the extent to which personal characteristics can be linked to perceptions of automated decision-making by AI, and the boundary conditions of these perceptions, namely the extent to which such perceptions differ across media, (public) health, and judicial contexts. Data from a scenario-based survey experiment with a national sample (N = 958) show that people are by and large concerned about risks and have mixed opinions about fairness and usefulness of automated decision-making at a societal level, with general attitudes influenced by individual characteristics. Interestingly, decisions taken automatically by AI were often evaluated on par or even better than human experts for specific decisions. Theoretical and societal implications about these findings are discussed. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",https://link.springer.com/article/10.1007/s00146-019-00931-w,1,1,1,"media, public health, judicial",NA,NA,vignette-experiment with 2 (decision-maker: Human vs. AI) × 2 (subject of the decision: self vs. others) × 2 (impact of the decision: high vs. low),T Araujo and N Helberger and S Kruikemeier and C H de Vreese,10.1007/s00146-019-00931-w,NA,3,AI and Society,NA,NA,Cited By :346,611-623,NA,NA,35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077189754&doi=10.1007%2fs00146-019-00931-w&partnerID=40&md5=f1661ebef5893d62b6e97e4ea35b8b31,2020,NA,NA
ID-529,article,Beyond user experience: What constitutes algorithmic experiences?,"Algorithms are progressively transforming human experience, especially, the interaction with businesses, governments, education, and entertainment. As a result, people are growingly seeing the outside world, in a sense, through the lens of algorithms. Despite the importance of algorithmic experience (AX), few studies had been devoted to investigating the nature and processes through which users perceive and actualize the potential for algorithm affordance. This study proposes the Algorithm Acceptance Model to conceptualize the notion of AX as part of the analytic framework for human-algorithm interaction. It then tests how AX shapes the satisfaction with and acceptance of algorithm services. The results show that AX is inherently related to human understanding of fairness, transparency, and other conventional components of user-experience, indicating the heuristic roles of transparency and fairness regarding their underlying relations of user experience and trust. AX can influence the user perception of algorithmic systems in the context of algorithm ecology, offering useful insights into the design of human-centered algorithm systems. The findings provide initial and robust support for the proposed Algorithm Acceptance Model. © 2019 Elsevier Ltd",https://www.sciencedirect.com/science/article/pii/S0268401219314161?via%3Dihub,1,1,1,NA,"fairness, usefulness",NA,NA,D Shin and B Zhong and F A Biocca,10.1016/j.ijinfomgt.2019.102061,NA,NA,International Journal of Information Management,NA,NA,Cited By :93,NA,NA,NA,52,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077915121&doi=10.1016%2fj.ijinfomgt.2019.102061&partnerID=40&md5=f0f296f866cd673ed2c34dd3069f6fb8,2020,NA,NA
ID-531,article,Slow response times undermine trust in algorithmic (but not human) predictions,"Algorithms consistently perform well on various prediction tasks, but people often mistrust their advice. Here, we demonstrate one component that affects people's trust in algorithmic predictions: response time. In seven studies (total N = 1928 with 14,184 observations), we find that people judge slowly generated predictions from algorithms as less accurate and they are less willing to rely on them. This effect reverses for human predictions, where slowly generated predictions are judged to be more accurate. In explaining this asymmetry, we find that slower response times signal the exertion of effort for both humans and algorithms. However, the relationship between perceived effort and prediction quality differs for humans and algorithms. For humans, prediction tasks are seen as difficult and observing effort is therefore positively correlated with the perceived quality of predictions. For algorithms, however, prediction tasks are seen as easy and effort is therefore uncorrelated to the quality of algorithmic predictions. These results underscore the complex processes and dynamics underlying people's trust in algorithmic (and human) predictions and the cues that people use to evaluate their quality. © 2020 Elsevier Inc.",https://www.sciencedirect.com/science/article/pii/S074959781930192X?via%3Dihub#s0015,1,1,1,NA,NA,NA,2 (Prediction provider: Human vs. Algorithm; between-subjects) × 2 (Response time: Fast vs. Slow; within-subjects) mixed-design experiment; Predicted that slowly-generated predictions are perceived favorably for humans but unfavorably for algorithms. DVs: self-reported Likert scale perceived accuracy and willingness to use prediction,E Efendić and P.P.F.M. Van de Calseyde and A M Evans,10.1016/j.obhdp.2020.01.008,NA,NA,Organizational Behavior and Human Decision Processes,NA,NA,Cited By :39,103-114,NA,NA,157,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079058427&doi=10.1016%2fj.obhdp.2020.01.008&partnerID=40&md5=3fbf674c45688c5637093b3a403024f5,2020,NA,NA
ID-542,article,Task-Dependent Algorithm Aversion,"Research suggests that consumers are averse to relying on algorithms to perform tasks that are typically done by humans, despite the fact that algorithms often perform better. The authors explore when and why this is true in a wide variety of domains. They find that algorithms are trusted and relied on less for tasks that seem subjective (vs. objective) in nature. However, they show that perceived task objectivity is malleable and that increasing a task’s perceived objectivity increases trust in and use of algorithms for that task. Consumers mistakenly believe that algorithms lack the abilities required to perform subjective tasks. Increasing algorithms’ perceived affective human-likeness is therefore effective at increasing the use of algorithms for subjective tasks. These findings are supported by the results of four online lab studies with over 1,400 participants and two online field studies with over 56,000 participants. The results provide insights into when and why consumers are likely to use algorithms and how marketers can increase their use when they outperform humans. © American Marketing Association 2019.",https://content.ebscohost.com/cds/retrieve?content=AQICAHiylJ_bvOB56hI8UzTN6Ryruh7a0kiIBN_ANwtaWYjmxwFWGPqGoqLRfgzs7i3aXamaAAAA4zCB4AYJKoZIhvcNAQcGoIHSMIHPAgEAMIHJBgkqhkiG9w0BBwEwHgYJYIZIAWUDBAEuMBEEDEZIX9qavuUXfcoO6wIBEICBm1ZFhKsC25mRdQqB0u0RulXVU9kBo3-2z5A2bu4XA5NZSTWCw42ApZT2AuNryrlcaMzH3oLQFdRvMP77pHYQrfAf8LwjTPD-25a0YemgaqW2UInCGIyPqpjmWTJUI_sfnjTmqe9Z9G8ChUjPfqS9cd_IH3LVmTbKUPsXsM91l8eF-Up7QNbOhf93ZTHyIzBwuFCafE-RZKkGWecv,1,1,1,"dating, financial advice, medical diagnosis, judge joke quality...See Table 4",human likeness,subjective vs. objective,making a task seem more objective can reduce people's algorithm aversion. Shows the value of quantitative analysis (rather than intuition) where people recognize that AI has an edge,N Castelo and M W Bos and D R Lehmann,10.1177/0022243719851788,NA,5,Journal of Marketing Research,NA,NA,Cited By :477,809-825,NA,NA,56,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077381162&doi=10.1177%2f0022243719851788&partnerID=40&md5=50224d5b4761bb9be26644a0fef49ea9,2019,NA,NA
ID-543,article,Psychosocial factors affecting artificial intelligence adoption in health care in China: Cross-sectional study,"Background: Poor quality primary health care is a major issue in China, particularly in blindness prevention. Artificial intelligence (AI) could provide early screening and accurate auxiliary diagnosis to improve primary care services and reduce unnecessary referrals, but the application of AI in medical settings is still an emerging field. Objective: This study aimed to investigate the general public's acceptance of ophthalmic AI devices, with reference to those already used in China, and the interrelated influencing factors that shape people's intention to use these devices. Methods: We proposed a model of ophthalmic AI acceptance based on technology acceptance theories and variables from other health care-related studies. The model was verified via a 32-item questionnaire with 7-point Likert scales completed by 474 respondents (nationally random sampled). Structural equation modeling was used to evaluate item and construct reliability and validity via a confirmatory factor analysis, and the model's path effects, significance, goodness of fit, and mediation and moderation effects were analyzed. Results: Standardized factor loadings of items were between 0.583 and 0.876. Composite reliability of 9 constructs ranged from 0.673 to 0.841. The discriminant validity of all constructs met the Fornell and Larcker criteria. Model fit indicators such as standardized root mean square residual (0.057), comparative fit index (0.915), and root mean squared error of approximation (0.049) demonstrated good fit. Intention to use (R2=0.515) is significantly affected by subjective norms (beta=.408; P<.001), perceived usefulness (beta=.336; P=.03), and resistance bias (beta=-.237; P=.02). Subjective norms and perceived behavior control had an indirect impact on intention to use through perceived usefulness and perceived ease of use. Eye health consciousness had an indirect positive effect on intention to use through perceived usefulness. Trust had a significant moderation effect (beta=-.095; P=.049) on the effect path of perceived usefulness to intention to use. Conclusions: The item, construct, and model indicators indicate reliable interpretation power and help explain the levels of public acceptance of ophthalmic AI devices in China. The influence of subjective norms can be linked to Confucian culture, collectivism, authoritarianism, and conformity mentality in China. Overall, the use of AI in diagnostics and clinical laboratory analysis is underdeveloped, and the Chinese public are generally mistrustful of medical staff and the Chinese medical system. Stakeholders such as doctors and AI suppliers should therefore avoid making misleading or over-exaggerated claims in the promotion of AI health care products. © 2019 Tiantian Ye, Jiaolong Xue, Mingguang He, Jing Gu, Haotian Lin, Bin Xu, Yu Cheng.",https://www.jmir.org/2019/10/e14316/,1,1,1,medical,"perceived control, ease of use",NA,NA,T Ye and J Xue and M He and J Gu and H Lin and B Xu and Y Cheng,10.2196/14316,NA,10,Journal of Medical Internet Research,NA,NA,Cited By :62,NA,NA,NA,21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073603025&doi=10.2196%2f14316&partnerID=40&md5=5f542e5a27439daa91b1c5f131e463d5,2019,NA,NA
ID-544,article,Algorithm Overdependence: How the Use of Algorithmic Recommendation Systems Can Increase Risks to Consumer Well-Being,"Consumers increasingly encounter recommender systems when making consumption decisions of all kinds. While numerous efforts have aimed to improve the quality of algorithm-generated recommendations, evidence has indicated that people often remain averse to superior algorithmic sources of information in favor of their own personal intuitions (a type II problem). The current work highlights an additional (type I) problem associated with the use of recommender systems: algorithm overdependence. Five experiments illustrate that, stemming from a belief that algorithms hold greater domain expertise, consumers surrender to algorithm-generated recommendations even when the recommendations are inferior. Counter to prior findings, this research indicates that consumers frequently depend too much on algorithm-generated recommendations, posing potential harms to their own well-being and leading them to play a role in propagating systemic biases that can influence other users. Given the rapidly expanding application of recommender systems across consumer domains, the authors believe that an appreciation and understanding of these risks is crucial to the effective guidance and development of recommendation systems that support consumer interests. © American Marketing Association 2019.",https://journals.sagepub.com/doi/pdf/10.1177/0743915619858057?casa_token=_zUDocJb4iMAAAAA:yr0pJh84lvkodmkJPpHDs2GaDAXtZEV87NaJ_hodwcoUYxLWwsythl7k-p-1w-J8Fq3kdjLKEzS4,1,1,1,e-commerce,NA,NA,"dominated = inferior recommendation, nondominated = superior recommendation. Explains that people may overrely on algorithms to reduce cognitive load: ""Consumers are more likely to defer their decisions, experience decreased satisfaction and confidence, and face greater regret when processing choice options requires greater effort (Chernev, Bo¨ckenholt, and Goodman 2015). Thus, people who are unwilling to expend effort are more inclined to rely on shortcuts such as recommendations when making decisions (Banker et al. 2017).",S Banker and S Khetani,10.1177/0743915619858057,NA,4,Journal of Public Policy and Marketing,NA,NA,Cited By :41,500-515,NA,NA,38,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071127956&doi=10.1177%2f0743915619858057&partnerID=40&md5=2fb7a74761ef7cd15c720228e970c364,2019,NA,NA
ID-545,article,“It would be pretty immoral to choose a random algorithm”: Opening up algorithmic interpretability and transparency,"Purpose: The purpose of this paper is to report on empirical work conducted to open up algorithmic interpretability and transparency. In recent years, significant concerns have arisen regarding the increasing pervasiveness of algorithms and the impact of automated decision-making in our lives. Particularly problematic is the lack of transparency surrounding the development of these algorithmic systems and their use. It is often suggested that to make algorithms more fair, they should be made more transparent, but exactly how this can be achieved remains unclear. Design/methodology/approach: An empirical study was conducted to begin unpacking issues around algorithmic interpretability and transparency. The study involved discussion-based experiments centred around a limited resource allocation scenario which required participants to select their most and least preferred algorithms in a particular context. In addition to collecting quantitative data about preferences, qualitative data captured participants’ expressed reasoning behind their selections. Findings: Even when provided with the same information about the scenario, participants made different algorithm preference selections and rationalised their selections differently. The study results revealed diversity in participant responses but consistency in the emphasis they placed on normative concerns and the importance of context when accounting for their selections. The issues raised by participants as important to their selections resonate closely with values that have come to the fore in current debates over algorithm prevalence. Originality/value: This work developed a novel empirical approach that demonstrates the value in pursuing algorithmic interpretability and transparency while also highlighting the complexities surrounding their accomplishment. © 2019, Emerald Publishing Limited.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064059515&doi=10.1108%2fJICES-11-2018-0092&partnerID=40&md5=34ce2e453a8c57f7f80b38e6237052aa,1,"1, 2",1,NA,NA,NA,NA,H Webb and M Patel and M Rovatsos and A Davoust and S Ceppi and A Koene and L Dowthwaite and V Portillo and M Jirotka and M Cano,10.1108/JICES-11-2018-0092,NA,2,"Journal of Information, Communication and Ethics in Society",NA,NA,Cited By :10,210-228,NA,NA,17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064059515&doi=10.1108%2fJICES-11-2018-0092&partnerID=40&md5=34ce2e453a8c57f7f80b38e6237052aa,2019,NA,NA
ID-546,article,Trusting forecasts,"Accurate forecasting is necessary to remain competitive in today's business environment. Forecast support systems are designed to aid forecasters in achieving high accuracy. However, studies have shown that people are distrustful of automated forecasters. This has recently been dubbed “algorithm aversion.” In this study, we explore the relationship between trust and forecasts, and if trust can be boosted in order to achieve a higher acceptance rate of system forecasts and lessen the occurrence of damaging adjustments. In a survey with 134 executives, we ask them to rate the determinants of trust in forecasts, what trust in forecasting means to them, and how trust in forecasts can be increased. The findings point to four main factors that play a role in trusting forecasts: (a) the forecast bundle, (b) forecaster competence, (c) combination of forecasts, and (d) knowledge. Implications of these factors for designing effective forecast support and future-focused management processes are discussed. © 2019 John Wiley & Sons Ltd.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119946961&doi=10.1002%2fffo2.19&partnerID=40&md5=3f1959ef9c1adaaaecc4ee87666c7da9,1,1,1,NA,NA,NA,NA,D Önkal and M S Gönül and S De Baets,10.1002/ffo2.19,NA,3-4,Futures and Foresight Science,NA,NA,Cited By :10,NA,NA,NA,1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119946961&doi=10.1002%2fffo2.19&partnerID=40&md5=3f1959ef9c1adaaaecc4ee87666c7da9,2019,NA,NA
ID-547,article,"Role of fairness, accountability, and transparency in algorithmic affordance","As algorithm-based services increase, social topics such as fairness, transparency, and accountability (FAT) must be addressed. This study conceptualizes such issues and examines how they influence the use and adoption of algorithm services. In particular, we investigate how trust is related to such issues and how trust influences the user experience of algorithm services. A multi-mixed method was used by integrating interpretive methods and surveys. The overall results show the heuristic role of fairness, accountability, and transparency, regarding their fundamental links to trust. Despite the importance of algorithms, no single testable definition has been observed. We reconstructed the understandings of algorithm and its affordance with user perception, invariant properties, and contextuality. The study concludes by arguing that algorithmic affordance offers a distinctive perspective on the conceptualization of algorithmic process. Individuals’ perceptions of FAT and how they actually perceive them are important topics for further study. © 2019 Elsevier Ltd",https://www.sciencedirect.com/science/article/pii/S0747563219301591?via%3Dihub,1,1,1,NA,"fairness, transparency, accountability",NA,NA,D Shin and Y J Park,10.1016/j.chb.2019.04.019,NA,NA,Computers in Human Behavior,NA,NA,Cited By :228,277-284,NA,NA,98,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065764209&doi=10.1016%2fj.chb.2019.04.019&partnerID=40&md5=203147582223a20b6fee22fb77a8a0ba,2019,NA,NA
ID-557,article,People are averse to machines making moral decisions,"Do people want autonomous machines making moral decisions? Nine studies suggest that that the answer is ‘no’—in part because machines lack a complete mind. Studies 1–6 find that people are averse to machines making morally-relevant driving, legal, medical, and military decisions, and that this aversion is mediated by the perception that machines can neither fully think nor feel. Studies 5–6 find that this aversion exists even when moral decisions have positive outcomes. Studies 7–9 briefly investigate three potential routes to increasing the acceptability of machine moral decision-making: limiting the machine to an advisory role (Study 7), increasing machines’ perceived experience (Study 8), and increasing machines’ perceived expertise (Study 9). Although some of these routes show promise, the aversion to machine moral decision-making is difficult to eliminate. This aversion may prove challenging for the integration of autonomous technology in moral domains including medicine, the law, the military, and self-driving vehicles. © 2018 Elsevier B.V.",https://www.sciencedirect.com/science/article/pii/S0010027718302087?via%3Dihub,1,1,1,"driving, legal, medical, military","experience, expertise",NA,mind perception,Y E Bigman and K Gray,10.1016/j.cognition.2018.08.003,NA,NA,Cognition,NA,NA,Cited By :269,21-34,NA,NA,181,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051401081&doi=10.1016%2fj.cognition.2018.08.003&partnerID=40&md5=80637d76be25ed53fb3ecc6af67500e7,2018,NA,NA
ID-581,article,Understanding algorithm aversion: When is advice from automation discounted?,"Forecasting advice from human advisors is often utilized more than advice from automation. There is little understanding of why “algorithm aversion” occurs, or specific conditions that may exaggerate it. This paper first reviews literature from two fields—interpersonal advice and human–automation trust—that can inform our understanding of the underlying causes of the phenomenon. Then, an experiment is conducted to search for these underlying causes. We do not replicate the finding that human advice is generally utilized more than automated advice. However, after receiving bad advice, utilization of automated advice decreased significantly more than advice from humans. We also find that decision makers describe themselves as having much more in common with human than automated advisors despite there being no interpersonal relationship in our study. Results are discussed in relation to other findings from the forecasting and human–automation trust fields and provide a new perspective on what causes and exaggerates algorithm aversion. Copyright © 2017 John Wiley & Sons, Ltd.",https://onlinelibrary.wiley.com/doi/epdf/10.1002/for.2464?saml_referrer,1,1,1,forecasting,NA,NA,"After receiving bad advice from a human and an algorithm, people are more likely to discount subsequent algorithm advice",A Prahl and L Van Swol,10.1002/for.2464,NA,6,Journal of Forecasting,NA,NA,Cited By :143,691-702,NA,NA,36,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016415876&doi=10.1002%2ffor.2464&partnerID=40&md5=f425a1c8bb2ffa85f75bfc794bb0b373,2017,NA,NA
ID-821,article,MetaWriter: Exploring the Potential and Perils of AI Writing Support in Scientific Peer Review,"Recent advances in Large Language Models (LLMs) show the potential to significantly augment or even replace complex human writing activities. However, for complex tasks where people need to make decisions as well as write a justification, the trade offs between making work efficient and hindering decisions remain unclear. In this paper, we explore this question in the context of designing intelligent scaffolding for writing meta-reviews for an academic peer review process. We prototyped a system called “MetaWriter” trained on five years of open peer review data to support meta-reviewing. The system highlights common topics in the original peer reviews, extracts key points by each reviewer, and on request, provides a preliminary draft of a meta-review that can be further edited. To understand how novice and experienced meta-reviewers use MetaWriter, we conducted a within-subject study with 32 participants. Each participant wrote meta-reviews for two papers: one with and one without MetaWriter. We found that MetaWriter significantly expedited the authoring process and improved the coverage of meta-reviews, as rated by experts, compared to the baseline. While participants recognized the efficiency benefits, they raised concerns around trust, over-reliance, and agency. We also interviewed six paper authors to understand their opinions of using machine intelligence to support the peer review process and reported critical reflections. We discuss implications for future interactive AI writing tools to support complex synthesis work. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",https://dl.acm.org/doi/pdf/10.1145/3637371,1,1,1,NA,NA,NA,NA,L Sun and S Tao and J Hu and S P Dow,10.1145/3637371,NA,CSCW1,Proceedings of the ACM on Human-Computer Interaction,NA,NA,Cited By :1,NA,NA,NA,8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190973010&doi=10.1145%2f3637371&partnerID=40&md5=8d402b2bcc7120481835e15696f9432f,2024,NA,NA
ID-822,article,Towards human-centred standards for legal help AI,"As more groups consider how AI may be used in the legal sector, this paper envisions how companies and policymakers can prioritize the perspective of community members as they design AI and policies around it. It presents findings of structured interviews and design sessions with community members, in which they were asked about whether, how, and why they would use AI tools powered by large language models to respond to legal problems like receiving an eviction notice. The respondents reviewed options for simple versus complex interfaces for AI tools, and expressed how they would want to engage with an AI tool to resolve a legal problem. These empirical findings provide directions that can counterbalance legal domain experts' proposals about the public interest around AI, as expressed by attorneys, court officials, advocates and regulators. By hearing directly from community members about how they want to use AI for civil justice tasks, what risks concern them, and the value they would find in different kinds of AI tools, this research can ensure that people's points of view are understood and prioritized, rather than only domain experts' assertions about people's needs and preferences around legal help AI. This article is part of the theme issue 'A complexity science approach to law and governance'. © 2024 The Author(s).",https://royalsocietypublishing-org.stanford.idm.oclc.org/doi/full/10.1098/rsta.2023.0157,1,2,2,NA,NA,NA,NA,M Hagan,10.1098/rsta.2023.0157,NA,2270,"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",NA,NA,Cited By :1,NA,NA,NA,382,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186145753&doi=10.1098%2frsta.2023.0157&partnerID=40&md5=8af4955585404ad2bb30567d33808c2e,2024,NA,NA
ID-829,article,Factors affecting performance expectancy and intentions to use ChatGPT: Using SmartPLS to advance an information technology acceptance framework,"Few studies have explored the use of artificial intelligence-enabled (AI-enabled) large language models (LLMs). This research addresses this knowledge gap. It investigates perceptions and intentional behaviors to utilize AI dialogue systems like Chat Generative Pre-Trained Transformer (ChatGPT). A survey questionnaire comprising measures from key information technology adoption models, was used to capture quantitative data from a sample of 654 respondents. A partial least squares (PLS) approach assesses the constructs' reliabilities and validities. It also identifies the relative strength and significance of the causal paths in the proposed research model. The findings from SmartPLS4 report that there are highly significant effects in this empirical investigation particularly between source trustworthiness and performance expectancy from AI chatbots, as well as between perceived interactivity and intentions to use this algorithm, among others. In conclusion, this contribution puts forward a robust information technology acceptance framework that clearly evidences the factors that entice online users to habitually engage with text-generating AI chatbot technologies. It implies that although they may be considered as useful interactive systems for content creators, there is scope to continue improving the quality of their responses (in terms of their accuracy and timeliness) to reduce misinformation, social biases, hallucinations and adversarial prompts. © 2024 The Author(s)",https://www.sciencedirect.com/science/article/pii/S004016252400043X?pes=vor,1,1,1,NA,NA,NA,Table 1 has an overview of the various theories that attempt to explain people's willingness to adopt technologies.,M A Camilleri,10.1016/j.techfore.2024.123247,NA,NA,Technological Forecasting and Social Change,NA,NA,Cited By :8,NA,NA,NA,201,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183980582&doi=10.1016%2fj.techfore.2024.123247&partnerID=40&md5=c0eab33e3f02c3f59d1c41b6c598d48b,2024,NA,NA
ID-847,article,"The Impact of Performance Expectancy, Workload, Risk, and Satisfaction on Trust in ChatGPT: Cross-Sectional Survey Analysis","Background: ChatGPT (OpenAI) is a powerful tool for a wide range of tasks, from entertainment and creativity to health care queries. There are potential risks and benefits associated with this technology. In the discourse concerning the deployment of ChatGPT and similar large language models, it is sensible to recommend their use primarily for tasks a human user can execute accurately. As we transition into the subsequent phase of ChatGPT deployment, establishing realistic performance expectations and understanding users' perceptions of risk associated with its use are crucial in determining the successful integration of this artificial intelligence (AI) technology. Objective: The aim of the study is to explore how perceived workload, satisfaction, performance expectancy, and risk-benefit perception influence users' trust in ChatGPT. Methods: A semistructured, web-based survey was conducted with 607 adults in the United States who actively use ChatGPT. The survey questions were adapted from constructs used in various models and theories such as the technology acceptance model, the theory of planned behavior, the unified theory of acceptance and use of technology, and research on trust and security in digital environments. To test our hypotheses and structural model, we used the partial least squares structural equation modeling method, a widely used approach for multivariate analysis. Results: A total of 607 people responded to our survey. A significant portion of the participants held at least a high school diploma (n=204, 33.6%), and the majority had a bachelor's degree (n=262, 43.1%). The primary motivations for participants to use ChatGPT were for acquiring information (n=219, 36.1%), amusement (n=203, 33.4%), and addressing problems (n=135, 22.2%). Some participants used it for health-related inquiries (n=44, 7.2%), while a few others (n=6, 1%) used it for miscellaneous activities such as brainstorming, grammar verification, and blog content creation. Our model explained 64.6% of the variance in trust. Our analysis indicated a significant relationship between (1) workload and satisfaction, (2) trust and satisfaction, (3) performance expectations and trust, and (4) risk-benefit perception and trust. Conclusions: The findings underscore the importance of ensuring user-friendly design and functionality in AI-based applications to reduce workload and enhance user satisfaction, thereby increasing user trust. Future research should further explore the relationship between risk-benefit perception and trust in the context of AI chatbots. © 2024 JMIR Publications Inc.. All rights reserved.",https://humanfactors.jmir.org/2024/1/e55399,1,2,1,NA,NA,NA,NA,A Choudhury and H Shamszare,10.2196/55399,NA,NA,JMIR Human Factors,NA,NA,Export Date: 30 July 2024,NA,NA,NA,11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195775384&doi=10.2196%2f55399&partnerID=40&md5=0023f8ae0830dc6c653c74405e9add31,2024,NA,NA
ID-854,article,Physician Versus Large Language Model Chatbot Responses to Web-Based Questions From Autistic Patients in Chinese: Cross-Sectional Comparative Analysis,"Background: There is a dearth of feasibility assessments regarding using large language models (LLMs) for responding to inquiries from autistic patients within a Chinese-language context. Despite Chinese being one of the most widely spoken languages globally, the predominant research focus on applying these models in the medical field has been on English-speaking populations. Objective: This study aims to assess the effectiveness of LLM chatbots, specifically ChatGPT-4 (OpenAI) and ERNIE Bot (version 2.2.3; Baidu, Inc), one of the most advanced LLMs in China, in addressing inquiries from autistic individuals in a Chinese setting. Methods: For this study, we gathered data from DXY—a widely acknowledged, web-based, medical consultation platform in China with a user base of over 100 million individuals. A total of 100 patient consultation samples were rigorously selected from January 2018 to August 2023, amounting to 239 questions extracted from publicly available autism-related documents on the platform. To maintain objectivity, both the original questions and responses were anonymized and randomized. An evaluation team of 3 chief physicians assessed the responses across 4 dimensions: relevance, accuracy, usefulness, and empathy. The team completed 717 evaluations. The team initially identified the best response and then used a Likert scale with 5 response categories to gauge the responses, each representing a distinct level of quality. Finally, we compared the responses collected from different sources. Results: Among the 717 evaluations conducted, 46.86% (95% CI 43.21%-50.51%) of assessors displayed varying preferences for responses from physicians, with 34.87% (95% CI 31.38%-38.36%) of assessors favoring ChatGPT and 18.27% (95% CI 15.44%-21.10%) of assessors favoring ERNIE Bot. The average relevance scores for physicians, ChatGPT, and ERNIE Bot were 3.75 (95% CI 3.69-3.82), 3.69 (95% CI 3.63-3.74), and 3.41 (95% CI 3.35-3.46), respectively. Physicians (3.66, 95% CI 3.60-3.73) and ChatGPT (3.73, 95% CI 3.69-3.77) demonstrated higher accuracy ratings compared to ERNIE Bot (3.52, 95% CI 3.47-3.57). In terms of usefulness scores, physicians (3.54, 95% CI 3.47-3.62) received higher ratings than ChatGPT (3.40, 95% CI 3.34-3.47) and ERNIE Bot (3.05, 95% CI 2.99-3.12). Finally, concerning the empathy dimension, ChatGPT (3.64, 95% CI 3.57-3.71) outperformed physicians (3.13, 95% CI 3.04-3.21) and ERNIE Bot (3.11, 95% CI 3.04-3.18). Conclusions: In this cross-sectional study, physicians’ responses exhibited superiority in the present Chinese-language context. Nonetheless, LLMs can provide valuable medical guidance to autistic patients and may even surpass physicians in demonstrating empathy. However, it is crucial to acknowledge that further optimization and research are imperative prerequisites before the effective integration of LLMs in clinical settings across diverse linguistic environments can be realized. © 2024 JMIR Publications Inc.. All rights reserved.",https://www.jmir.org/2024/1/e54706,1,1,1,medical,NA,NA,NA,W He and W Zhang and Y Jin and Q Zhou and H Zhang and Q Xia,10.2196/54706,NA,NA,Journal of Medical Internet Research,NA,NA,Export Date: 30 July 2024,NA,NA,NA,26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191916107&doi=10.2196%2f54706&partnerID=40&md5=a34195cc7e92af2fe5108890b88114f4,2024,NA,NA
ID-856,article,Quality of Answers of Generative Large Language Models Versus Peer Users for Interpreting Laboratory Test Results for Lay Patients: Evaluation Study,"Background: Although patients have easy access to their electronic health records and laboratory test result data through patient portals, laboratory test results are often confusing and hard to understand. Many patients turn to web-based forums or question-and-answer (Q&A) sites to seek advice from their peers. The quality of answers from social Q&A sites on health-related questions varies significantly, and not all responses are accurate or reliable. Large language models (LLMs) such as ChatGPT have opened a promising avenue for patients to have their questions answered. Objective: We aimed to assess the feasibility of using LLMs to generate relevant, accurate, helpful, and unharmful responses to laboratory test–related questions asked by patients and identify potential issues that can be mitigated using augmentation approaches. Methods: We collected laboratory test result–related Q&A data from Yahoo! Answers and selected 53 Q&A pairs for this study. Using the LangChain framework and ChatGPT web portal, we generated responses to the 53 questions from 5 LLMs: GPT-4, GPT-3.5, LLaMA 2, MedAlpaca, and ORCA_mini. We assessed the similarity of their answers using standard Q&A similarity-based evaluation metrics, including Recall-Oriented Understudy for Gisting Evaluation, Bilingual Evaluation Understudy, Metric for Evaluation of Translation With Explicit Ordering, and Bidirectional Encoder Representations from Transformers Score. We used an LLM-based evaluator to judge whether a target model had higher quality in terms of relevance, correctness, helpfulness, and safety than the baseline model. We performed a manual evaluation with medical experts for all the responses to 7 selected questions on the same 4 aspects. Results: Regarding the similarity of the responses from 4 LLMs; the GPT-4 output was used as the reference answer, the responses from GPT-3.5 were the most similar, followed by those from LLaMA 2, ORCA_mini, and MedAlpaca. Human answers from Yahoo data were scored the lowest and, thus, as the least similar to GPT-4–generated answers. The results of the win rate and medical expert evaluation both showed that GPT-4’s responses achieved better scores than all the other LLM responses and human responses on all 4 aspects (relevance, correctness, helpfulness, and safety). LLM responses occasionally also suffered from lack of interpretation in one’s medical context, incorrect statements, and lack of references. Conclusions: By evaluating LLMs in generating responses to patients’ laboratory test result–related questions, we found that, compared to other 4 LLMs and human answers from a Q&A website, GPT-4’s responses were more accurate, helpful, relevant, and safer. There were cases in which GPT-4 responses were inaccurate and not individualized. We identified a number of ways to improve the quality of LLM responses, including prompt engineering, prompt augmentation, retrieval-augmented generation, and response evaluation. ©Zhe He, Balu Bhasuran, Qiao Jin, Shubo Tian, Karim Hanna, Cindy Shavor, Lisbeth Garcia Arguello, Patrick Murray, Zhiyong Lu.",https://www.jmir.org/2024/1/e56655,1,1,1,medical,NA,NA,NA,Z He and B Bhasuran and Q Jin and S Tian and K Hanna and C Shavor and L G Arguello and P Murray and Z Lu,10.2196/56655,NA,1,Journal of Medical Internet Research,NA,NA,Export Date: 30 July 2024,NA,NA,NA,26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190903860&doi=10.2196%2f56655&partnerID=40&md5=9c881f6e5332fef0799c9f9e46b2f9fb,2024,NA,NA
ID-860,article,User acceptance and adoption dynamics of ChatGPT in educational settings,"Recent developments in natural language understanding have sparked a great amount of interest in the large language models such as ChatGPT that contain billions of parameters and are trained for thousands of hours on all the textual data of the internet. ChatGPT has received immense attention because it has widespread applications, which it is able to do out-of-the-box, with no prior training or fine-tuning. These models show emergent skill and can perform virtually any textual task and provide glimmers, or “sparks”, of artificial general intelligence, in the form of a general problem solver as envisioned by Newell and Simon in the early days of artificial intelligence research. Researchers are now exploring the opportunities of ChatGPT in education. Yet, the factors influencing and driving users’ acceptance of ChatGPT remains largely unexplored. This study investigates users’ (n=138) acceptance of ChatGPT. We test a structural model developed using Unified Theory of Acceptance and Use of Technology model. The study reveals that performance expectancy is related to behavioral intention, which in turn is related to ChatGPT use. Findings are discussed within the context of mass adoption and the challenges and opportunities for teaching and learning. The findings provide empirical grounding to support understanding of technology acceptance decisions through the lens of students’ use of ChatGPT and further document the influence of situational factors on technology acceptance more broadly. This research contributes to body of knowledge and facilitates future research on digital innovation acceptance and use. © 2024 by the authors; licensee Modestum. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/).=",https://www.ejmste.com/download/user-acceptance-and-adoption-dynamics-of-chatgpt-in-educational-settings-14151.pdf,1,1,1,education,NA,NA,"Broad question: What are determinants of use of ChatGPT among college students? Draws upon Unified Theory of Acceptance and Use of Technology model. Considers ease of use, social influence, perceived perfomance expectancy, etc.",P Bazelais and D J Lemay and T Doleck,10.29333/ejmste/14151,NA,2,"Eurasia Journal of Mathematics, Science and Technology Education",NA,NA,Export Date: 30 July 2024,NA,NA,NA,20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189479590&doi=10.29333%2fejmste%2f14151&partnerID=40&md5=5d016a7df691a39cf0b39ce031fa1429,2024,NA,NA
ID-866,article,Exploring EFL university teachers’ beliefs in integrating ChatGPT and other large language models in language education: a study in China,"Nowadays, the prevalence of ChatGPT and other Large Language Models (LLMs) has posed significant challenges into the education field, particularly in English education. In response, this study aimed to investigate the beliefs of 95 EFL university teachers from Chinese universities regarding the integration of LLMs in language education, as well as the relationships between their beliefs and other factors. The study yielded several findings: (1) According to the quantitative and qualitative results, we revealed several concerns among Chinese EFL university teachers regarding LLMs integration, such as neglection of traditional learning resources, academic integrity, and excessive reliance. (2) Previous experiences with LLMs, frequency of LLMs use, and self-evaluation on stages of LLMs integration all played vital roles in shaping university teachers’ beliefs in integrating LLMs in language education. (3) No significant correlation was observed between university teachers’ beliefs in integrating LLMs in language education and the availability of IT personnel. (4) No significant correlation was observed between university teachers’ beliefs in integrating LLMs in language education their evaluation on IT infrastructure. This research has provided some insights into university teachers’ beliefs in ChatGPT and other LLMs to promote effective policies and strategies in the digital era. © 2024 National Institute of Education, Singapore.",https://www.researchgate.net/profile/Xiaochen-Wang-31/publication/377574349_Exploring_EFL_university_teachers'_beliefs_in_integrating_ChatGPT_and_other_large_language_models_in_language_education_a_study_in_China/links/65adc363ee1e1951fbd7ab5e/Exploring-EFL-university-teachers-beliefs-in-integrating-ChatGPT-and-other-large-language-models-in-language-education-a-study-in-China.pdf,1,"1, 2",1,education,NA,NA,"RQ1: How do Chinese EFL university teachers perceive integrating LLMs in language education?
RQ2: What is the relationship between university teachers’ beliefs in integrating LLMs in language
education and their previous experiences?
RQ3: What is the relationship between university teachers’ beliefs in integrating LLMs in language
education and the availability of IT personnel in their institutions?
RQ4: What is the relationship between university teachers’ beliefs in integrating LLMs in language
education and their evaluation on IT infrastructure?",Y Gao and Q Wang and X Wang,10.1080/02188791.2024.2305173,NA,1,Asia Pacific Journal of Education,NA,NA,Cited By :11,29-44,NA,NA,44,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182815193&doi=10.1080%2f02188791.2024.2305173&partnerID=40&md5=9584cdc31ac347c1abd230a1ca0191d6,2024,NA,NA
ID-879,article,AI-generated feedback on writing: insights into efficacy and ENL student preference,"The question of how generative AI tools, such as large language models and chatbots, can be leveraged ethically and effectively in education is ongoing. Given the critical role that writing plays in learning and assessment within educational institutions, it is of growing importance for educators to make thoughtful and informed decisions as to how and in what capacity generative AI tools should be leveraged to assist in the development of students’ writing skills. This paper reports on two longitudinal studies. Study 1 examined learning outcomes of 48 university English as a new language (ENL) learners in a six-week long repeated measures quasi experimental design where the experimental group received writing feedback generated from ChatGPT (GPT-4) and the control group received feedback from their human tutor. Study 2 analyzed the perceptions of a different group of 43 ENLs who received feedback from both ChatGPT and their tutor. Results of study 1 showed no difference in learning outcomes between the two groups. Study 2 results revealed a near even split in preference for AI-generated or human-generated feedback, with clear advantages to both forms of feedback apparent from the data. The main implication of these studies is that the use of AI-generated feedback can likely be incorporated into ENL essay evaluation without affecting learning outcomes, although we recommend a blended approach that utilizes the strengths of both forms of feedback. The main contribution of this paper is in addressing generative AI as an automatic essay evaluator while incorporating learner perspectives. © 2023, The Author(s).",https://link.springer.com/article/10.1186/s41239-023-00425-2?utm_source=getftr&utm_medium=getftr&utm_campaign=getftr_pilot,1,1,1,education,NA,NA,1. Does ChatGPT outperform the effectiveness of human tutors? 2. Do students prefer ChatGPT tutors?,J Escalante and A Pack and A Barrett,10.1186/s41239-023-00425-2,NA,1,International Journal of Educational Technology in Higher Education,NA,NA,Cited By :27,NA,NA,NA,20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175095013&doi=10.1186%2fs41239-023-00425-2&partnerID=40&md5=0bcf8ccafacec9619ab054c71d32b06f,2023,NA,NA
ID-907,article,"Human favoritism, not AI aversion: People’s perceptions (and bias) toward generative AI, human experts, and human–GAI collaboration in persuasive content generation","With the wide availability of large language models and generative AI, there are four primary paradigms for human–AI collaboration: human-only, AI-only (ChatGPT-4), augmented human (where a human makes the final decision with AI output as a reference), or augmented AI (where the AI makes the final decision with human output as a reference). In partnership with one of the world’s leading consulting firms, we enlisted professional content creators and ChatGPT-4 to create advertising content for products and persuasive content for campaigns following the aforementioned paradigms. First, we find that, contrary to the expectations of some of the existing algorithm aversion literature on conventional predictive AI, the content generated by generative AI and augmented AI is perceived as of higher quality than that produced by human experts and augmented human experts. Second, revealing the source of content production reduces—but does not reverse—the perceived quality gap between human-and AI-generated content. This bias in evaluation is predominantly driven by human favoritism rather than AI aversion: Knowing that the same content is created by a human expert increases its (reported) perceived quality, but knowing that AI is involved in the creation process does not affect its perceived quality. Further analysis suggests this bias is not due to a ‘quality prime’ as knowing the content they are about to evaluate comes from competent creators (e.g., industry professionals and state-of-the-art AI) without knowing exactly that the creator of each piece of content does not increase participants’ perceived quality. © The Author(s), 2023.",https://www.cambridge.org/core/journals/judgment-and-decision-making/article/human-favoritism-not-ai-aversion-peoples-perceptions-and-bias-toward-generative-ai-human-experts-and-humangai-collaboration-in-persuasive-content-generation/419C4BD9CE82673EAF1D8F6C350C4FA8,1,1,1,"advertising, marketing",NA,NA,"3 (information level: baseline, partially informed, and informed) x 4 (content generation paradigms: human-only, AI-only, augmented human, or augmented AI)",Y Zhang and R Gosline,10.1017/JDM.2023.37,NA,NA,Judgment and Decision Making,NA,NA,Cited By :5,NA,NA,NA,18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184159567&doi=10.1017%2fJDM.2023.37&partnerID=40&md5=34dd9c76d8a5052a0c85bcfc3b07bce5,2023,NA,NA
ID-1133,article,Blame the Machine? Insights From an Experiment on Algorithm Aversion and Blame Avoidance in Computer-Aided Human Resource Management,"Algorithms have become increasingly relevant in supporting human resource (HR) management, but their application may entail psychological biases and unintended side effects on employee behavior. This study examines the effect of the type of HR decision (i.e., promoting or dismissing staff) on the likelihood of delegating these HR decisions to an algorithm-based decision support system. Based on prior research on algorithm aversion and blame avoidance, we conducted a quantitative online experiment using a 2×2 randomly controlled design with a sample of N = 288 highly educated young professionals and graduate students in Germany. This study partly replicates and substantially extends the methods and theoretical insights from a 2015 study by Dietvorst and colleagues. While we find that respondents exhibit a tendency of delegating presumably unpleasant HR tasks (i.e., dismissals) to the algorithm—rather than delegating promotions—this effect is highly conditional upon the opportunity to pretest the algorithm, as well as individuals’ level of trust in machine-based and human forecast. Respondents’ aversion to algorithms dominates blame avoidance by delegation. This study is the first to provide empirical evidence that the type of HR decision affects algorithm aversion only to a limited extent. Instead, it reveals the counterintuitive effect of algorithm pretesting and the relevance of confidence in forecast models in the context of algorithm-aided HRM, providing theoretical and practical insights.",https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.779028/full,1,1,1,HR management,NA,NA,NA,Christian Maasland and Kristina S Weißmüller,10.3389/fpsyg.2022.779028,1664-1078,NA,Frontiers in psychology,"algorithm aversion,algorithm-based decision support systems,behavioral experimental research,blame avoidance,human resource management",NA,NA,779028,NA,NA,13,http://dx.doi.org/10.3389/fpsyg.2022.779028 https://www.ncbi.nlm.nih.gov/pubmed/35693517 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9177159,2022,Switzerland,NA
ID-1137,article,People Reject Algorithms in Uncertain Decision Domains Because They Have Diminishing Sensitivity to Forecasting Error,"Will people use self-driving cars, virtual doctors, and other algorithmic decision-makers if they outperform humans? The answer depends on the uncertainty inherent in the decision domain. We propose that people have diminishing
sensitivity to forecasting error and that this preference results in people favoring riskier (and often worse-performing) decision-making methods, such as human judgment, in inherently uncertain domains. In nine studies (N = 4,820), we found that (a) people have diminishing sensitivity to each marginal unit of error that a forecast produces, (b) people are less likely to use the best possible algorithm in decision domains that are more unpredictable, (c) people choose between decision-making methods on the basis of the perceived likelihood of those methods producing a near-perfect answer, and (d) people prefer methods that exhibit higher variance in performance (all else being equal). To the extent that investing, medical decision-making, and other domains are inherently uncertain, people may be unwilling to use even the best possible algorithm in those domains.",https://journals.sagepub.com/doi/pdf/10.1177/0956797620948841?casa_token=_I4VF7Tou-oAAAAA:12TbFIFvHfThB_qqJCO49Y6jxkGOEpmwhWhPUcKMLOo6xsCOOK7xxMxWzY3wMfCqBJ4nvtzxZfQd,1,1,1,NA,NA,uncertainty involved,Decision-makers who have diminishing sensitivity to error face a smaller penalty for each marginal unit of error that a forecast produces. May explain why they tend to rely more on (often faulty) human judgment as the perceived uncertainty of the domain increases. Ps were assigned to models with varying levels of performance/certainty at arriving at correct answer. DV: whether they chose to use their own answer or the algorithm's answer,Berkeley J Dietvorst and Soaham Bharti,10.1177/0956797620948841,1467-9280,10,Psychological science,"algorithm aversion,decision-making,forecasting,judgment,open data,open materials,preregistered,variance",NA,NA,1302-1314,NA,NA,31,http://dx.doi.org/10.1177/0956797620948841 https://www.ncbi.nlm.nih.gov/pubmed/32916083,2020,United States,NA
ID-1139,article,The impact of text topic and assumed human vs. AI authorship on competence,"Background: While Large Language Models (LLMs) are considered positively with respect to technological progress and abilities, people are rather opposed to machines making moral decisions. But the circumstances under which algorithm aversion or algorithm appreciation are more likely to occur with respect to LLMs have not yet been sufficiently investigated. Therefore, the aim of this study was to investigate how texts with moral or technological topics, allegedly written either by a human author or by ChatGPT, are perceived.

Methods: In a randomized controlled experiment, n = 164 participants read six texts, three of which had a moral and three a technological topic (predictor text topic). The alleged author of each text was randomly either labeled “ChatGPT” or “human author” (predictor authorship). We captured three dependent variables: assessment of author competence, assessment of content quality, and participants' intention to submit the text in a hypothetical university course (sharing intention). We hypothesized interaction effects, that is, we expected ChatGPT to score lower than alleged human authors for moral topics and higher than alleged human authors for technological topics and vice versa.

Results: We only found a small interaction effect for perceived author competence, p = 0.004, d = 0.40, but not for the other dependent variables. However, ChatGPT was consistently devalued compared to alleged human authors across all dependent variables: there were main effects of authorship for assessment of the author competence, p < 0.001, d = 0.95; for assessment of content quality, p < 0.001, d = 0.39; as well as for sharing intention, p < 0.001, d = 0.57. There was also a small main effect of text topic on the assessment of text quality, p = 0.002, d = 0.35.

Conclusion: These results are more in line with previous findings on algorithm aversion than with algorithm appreciation. We discuss the implications of these findings for the acceptance of the use of LLMs for text composition.",https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1412710/full,1,1,1,moral decisions,NA,NA,NA,Sebastian Proksch and Julia Schühle and Elisabeth Streeb and Finn Weymann and Teresa Luther and Joachim Kimmerle,10.3389/frai.2024.1412710,2624-8212,NA,Frontiers in artificial intelligence,"ChatGPT,algorithm aversion,competence,large language models,morality,quality assessment,technology",NA,NA,1412710,NA,NA,7,http://dx.doi.org/10.3389/frai.2024.1412710 https://www.ncbi.nlm.nih.gov/pubmed/38881953 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11176609,2024,Switzerland,NA
ID-1145,article,How Terminology Affects Users' Responses to System Failures,"Objective: The objective of our research is to advance the understanding of behavioral responses to a system's error. By examining trust as a dynamic variable and drawing from attribution theory, we explain the underlying mechanism and suggest how terminology can be used to mitigate the so-called algorithm aversion. In this way, we show that the use of different terms may shape consumers' perceptions and provide guidance on how these differences can be mitigated.

Background: Previous research has interchangeably used various terms to refer to a system and results regarding trust in systems have been ambiguous.

Methods: Across three studies, we examine the effect of different system terminology on consumer behavior following a system failure.

Results: Our results show that terminology crucially affects user behavior. Describing a system as ""AI"" (i.e., self-learning and perceived as more complex) instead of as ""algorithmic"" (i.e., a less complex rule-based system) leads to more favorable behavioral responses by users when a system error occurs.

Conclusion: We suggest that in cases when a system's characteristics do not allow for it to be called ""AI,"" users should be provided with an explanation of why the system's error occurred, and task complexity should be pointed out. We highlight the importance of terminology, as this can unintentionally impact the robustness and replicability of research findings.

Application: This research offers insights for industries utilizing AI and algorithmic systems, highlighting how strategic terminology use can shape user trust and response to errors, thereby enhancing system acceptance.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11141081/,1,1,1,NA,perceived stability and complexity,NA,NA,Cindy Candrian and Anne Scherer,10.1177/00187208231202572,1547-8181,8,Human factors,"expert systems,human-automation interaction,system design,technology acceptance,trust in automation",NA,NA,2082-2103,NA,NA,66,http://dx.doi.org/10.1177/00187208231202572 https://www.ncbi.nlm.nih.gov/pubmed/37734726 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11141081,2023,United States,NA
ID-1153,article,Effects of interacting with a large language model compared with a human,"Introduction Versatile large language models (LLMs) have the potential to augment diagnostic decision-making by assisting diagnosticians, thanks to their ability to engage in open-ended, natural conversations and their comprehensive knowledge access. Yet the novelty of LLMs in diagnostic decision-making introduces uncertainties regarding their impact. Clinicians unfamiliar with the use of LLMs in their professional context may rely on general attitudes towards LLMs more broadly, potentially hindering thoughtful use and critical evaluation of their input, leading to either over-reliance and lack of critical thinking or an unwillingness to use LLMs as diagnostic aids. To address these concerns, this study examines the influence on the diagnostic process and outcomes of interacting with an LLM compared with a human coach, and of prior training vs no training for interacting with either of these ‘coaches’. Our findings aim to illuminate the potential benefits and risks of employing artificial intelligence (AI) in diagnostic decision-making.
Methods and analysis We are conducting a prospective, randomised experiment with N=158 fourth-year medical students from Charité Medical School, Berlin, Germany. Participants are asked to diagnose patient vignettes after being assigned to either a human coach or ChatGPT and after either training or no training (both between-subject factors). We are specifically collecting data on the effects of using either of these ‘coaches’ and of additional training on information search, number of hypotheses entertained, diagnostic accuracy and confidence. Statistical methods will include linear mixed effects models. Exploratory analyses of the interaction patterns and attitudes towards AI will also generate more generalisable knowledge about the role of AI in medicine.",file:///Users/sarahwu/Downloads/Effects_of_interacting_with_a_large_language_model.pdf,1,1,1,medical,NA,NA,NA,Juliane E Kämmer and Wolf E Hautz and Gert Krummrey and Thomas C Sauter and Dorothea Penders and Tanja Birrenbach and Nadine Bienefeld,10.1136/bmjopen-2024-087469,2044-6055,7,BMJ open,"Artificial Intelligence,Clinical Decision-Making,Clinical Reasoning,MEDICAL EDUCATION & TRAINING",NA,NA,e087469,NA,NA,14,http://dx.doi.org/10.1136/bmjopen-2024-087469 https://www.ncbi.nlm.nih.gov/pubmed/39025818 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11261684,2024,England,NA
ID-1154,article,ChatGPT vs Medical Professional: Analyzing Responses to Laboratory Medicine Questions on Social Media,"Background
The integration of ChatGPT, a large language model (LLM) developed by OpenAI, into healthcare has sparked significant interest due to its potential to enhance patient care and medical education. With the increasing trend of patients accessing laboratory results online, there is a pressing need to evaluate the effectiveness of ChatGPT in providing accurate laboratory medicine information. Our study evaluates ChatGPT's effectiveness in addressing patient questions in this area, comparing its performance with that of medical professionals on social media.

Methods
This study sourced patient questions and medical professional responses from Reddit and Quora, comparing them with responses generated by ChatGPT versions 3.5 and 4.0. Experienced laboratory medicine professionals evaluated the responses for quality and preference. Evaluation results were further analyzed using R software.

Results
The study analyzed 49 questions, with evaluators reviewing responses from both medical professionals and ChatGPT. ChatGPT's responses were preferred by 75.9% of evaluators and generally received higher ratings for quality. They were noted for their comprehensive and accurate information, whereas responses from medical professionals were valued for their conciseness. The interrater agreement was fair, indicating some subjectivity but a consistent preference for ChatGPT's detailed responses.

Conclusions
ChatGPT demonstrates potential as an effective tool for addressing queries in laboratory medicine, often surpassing medical professionals in response quality. These results support the need for further research to confirm ChatGPT's utility and explore its integration into healthcare settings.",https://academic.oup.com/clinchem/advance-article/doi/10.1093/clinchem/hvae093/7714978?searchresult=1#475536750,1,1,1,medical,NA,NA,NA,Mark R Girton and Dina N Greene and Geralyn Messerlian and David F Keren and Min Yu,10.1093/clinchem/hvae093,1530-8561,NA,Clinical chemistry,NA,NA,NA,NA,NA,NA,NA,http://dx.doi.org/10.1093/clinchem/hvae093 https://www.ncbi.nlm.nih.gov/pubmed/39013110,2024,England,NA
ID-1156,article,Public comfort with the use of ChatGPT and expectations for healthcare,"Objectives
To examine whether comfort with the use of ChatGPT in society differs from comfort with other uses of AI in society and to identify whether this comfort and other patient characteristics such as trust, privacy concerns, respect, and tech-savviness are associated with expected benefit of the use of ChatGPT for improving health.

Materials and Methods
We analyzed an original survey of U.S. adults using the NORC AmeriSpeak Panel (n = 1787). We conducted paired t-tests to assess differences in comfort with AI applications. We conducted weighted univariable regression and 2 weighted logistic regression models to identify predictors of expected benefit with and without accounting for trust in the health system.

Results
Comfort with the use of ChatGPT in society is relatively low and different from other, common uses of AI. Comfort was highly associated with expecting benefit. Other statistically significant factors in multivariable analysis (not including system trust) included feeling respected and low privacy concerns. Females, younger adults, and those with higher levels of education were less likely to expect benefits in models with and without system trust, which was positively associated with expecting benefits (P = 1.6 × 10−11). Tech-savviness was not associated with the outcome.

Discussion
Understanding the impact of large language models (LLMs) from the patient perspective is critical to ensuring that expectations align with performance as a form of calibrated trust that acknowledges the dynamic nature of trust.

Conclusion
Including measures of system trust in evaluating LLMs could capture a range of issues critical for ensuring patient acceptance of this technological innovation.",https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae164/7705626,1,1,1,healthcare,NA,NA,NA,Jodyn Platt and Paige Nong and Renée Smiddy and Reema Hamasha and Gloria Carmona Clavijo and Joshua Richardson and Sharon L R Kardia,10.1093/jamia/ocae164,1527-974X,NA,Journal of the American Medical Informatics Association : JAMIA,"artificial intelligence,large language model,patient trust,public opinion",NA,NA,NA,NA,NA,NA,http://dx.doi.org/10.1093/jamia/ocae164 https://www.ncbi.nlm.nih.gov/pubmed/38960730,2024,England,NA
ID-1159,article,"Averse to what: Consumer aversion to algorithmic labels, but not their outputs?","Inspired by significant technical advancements, a rapidly growing stream of research explores human lay beliefs and reactions surrounding AI tools, which employ algorithms to mimic elements of human intelligence. This literature predominantly documents negative reactions to these tools or the underlying algorithms, often referred to as algorithm aversion or, alternatively, a preference for humans. This article proposes a third interpretation: people may be averse to their labels, but appreciative of their output. This perspective offers three core insights for how we study people's reactions to algorithms. Research would benefit from (1) carefully considering the labeling of AI tools, (2) broadening the scope of study to include interactions with these tools, and (3) accounting for their technical configuration.

",https://www.sciencedirect.com/science/article/pii/S2352250X24000526,1,1,1,NA,NA,NA,"not an empirical study, but a brief synthesis of the literature to show disparity between people's self-reported aversion to algorithms and their behavior (appreciation of their outputs). For example, when people don't know that emotionally supportive messages were written by AI, they actually report feeling better than when reading messages by humans.",Shwetha Mariadassou and Anne-Kathrin Klesse and Johannes Boegershausen,10.1016/j.copsyc.2024.101839,2352-2518,NA,Current opinion in psychology,"Algorithm,Algorithm aversion,Artificial intelligence,Explainable AI,Human-AI interactions,Labeling,Machine learning,Online platforms,Web scraping",NA,NA,101839,NA,NA,58,http://dx.doi.org/10.1016/j.copsyc.2024.101839 https://www.ncbi.nlm.nih.gov/pubmed/38996629,2024,Netherlands,NA
ID-1160,article,"ChatGPT vs. neurologists: a cross-sectional study investigating preference, satisfaction ratings and perceived empathy in responses among people living with multiple sclerosis","Background: ChatGPT is an open-source natural language processing software that replies to users' queries. We conducted a cross-sectional study to assess people living with Multiple Sclerosis' (PwMS) preferences, satisfaction, and empathy toward two alternate responses to four frequently-asked questions, one authored by a group of neurologists, the other by ChatGPT.

Methods: An online form was sent through digital communication platforms. PwMS were blind to the author of each response and were asked to express their preference for each alternate response to the four questions. The overall satisfaction was assessed using a Likert scale (1-5); the Consultation and Relational Empathy scale was employed to assess perceived empathy.

Results: We included 1133 PwMS (age, 45.26 ± 11.50 years; females, 68.49%). ChatGPT's responses showed significantly higher empathy scores (Coeff = 1.38; 95% CI = 0.65, 2.11; p > z < 0.01), when compared with neurologists' responses. No association was found between ChatGPT' responses and mean satisfaction (Coeff = 0.03; 95% CI = - 0.01, 0.07; p = 0.157). College graduate, when compared with high school education responder, had significantly lower likelihood to prefer ChatGPT response (IRR = 0.87; 95% CI = 0.79, 0.95; p < 0.01).

Conclusions: ChatGPT-authored responses provided higher empathy than neurologists. Although AI holds potential, physicians should prepare to interact with increasingly digitized patients and guide them on responsible AI use. Future development should consider tailoring AIs' responses to individual characteristics. Within the progressive digitalization of the population, ChatGPT could emerge as a helpful support in healthcare management rather than an alternative.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11233331/,1,1,1,medical,NA,NA,"compared MS patients' preferences, satisfaction, and empathy toward two alternate responses to four frequently-asked questions, one authored by a group of neurologists, the other by ChatGPT

ChatGPT answers were rated as more empathetic than neurologists' answers",Elisabetta Maida and Marcello Moccia and Raffaele Palladino and Giovanna Borriello and Giuseppina Affinito and Marinella Clerico and Anna Maria Repice and Alessia Di Sapio and Rosa Iodice and Antonio Luca Spiezia and Maddalena Sparaco and Giuseppina Miele and Floriana Bile and Cristiano Scandurra and Diana Ferraro and Maria Laura Stromillo and Renato Docimo and Antonio De Martino and Luca Mancinelli and Gianmarco Abbadessa and Krzysztof Smolik and Lorenzo Lorusso and Maurizio Leone and Elisa Leveraro and Francesca Lauro and Francesca Trojsi and Lidia Mislin Streito and Francesca Gabriele and Fabiana Marinelli and Antonio Ianniello and Federica De Santis and Matteo Foschi and Nicola De Stefano and Vincenzo Brescia Morra and Alvino Bisecco and Giancarlo Coghe and Eleonora Cocco and Michele Romoli and Francesco Corea and Letizia Leocani and Jessica Frau and Simona Sacco and Matilde Inglese and Antonio Carotenuto and Roberta Lanzillo and Alessandro Padovani and Maria Triassi and Simona Bonavita and Luigi Lavorgna and Web Digital Technologies Social Media Study Group of the Italian Society of Neurology (SIN),10.1007/s00415-024-12328-x,1432-1459,7,Journal of neurology,"Artificial intelligence,Large language model,Machine learning,Multiple sclerosis",NA,NA,4057-4066,NA,NA,271,http://dx.doi.org/10.1007/s00415-024-12328-x https://www.ncbi.nlm.nih.gov/pubmed/38568227 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11233331,2024,Germany,NA
ID-1161,article,Assessing AI receptivity through a persuasion knowledge lens,"Understanding human-artificial intelligence (AI) interactions is a growing academic interest. This article conceptualizes AI as a persuasion agent and reviews the recent literature on AI through the lens of persuasion knowledge. It presents research on AI acceptance and aversion in terms of the properties of the AI itself (e.g., anthropomorphism, functionality, and usability), the properties of individuals interacting with AI (e.g., individual differences in judgments of AI, perceived uniqueness, and task performance), and the context of the interaction (e.g., type of decision, domain, and usage occasion). In assessing AI interaction research through this lens, we systematically categorize these findings and identify promising future research directions.",https://www.sciencedirect.com/science/article/pii/S2352250X24000472,1,3,2,NA,NA,NA,"factors that determine algorithm appreciation/aversion: domain, usage, framing, identity-relevance, hedonic vs. utilitarian context",Jared Watson and Francesca Valsesia and Shoshana Segal,10.1016/j.copsyc.2024.101834,2352-2518,NA,Current opinion in psychology,NA,NA,NA,101834,NA,NA,58,http://dx.doi.org/10.1016/j.copsyc.2024.101834 https://www.ncbi.nlm.nih.gov/pubmed/38970936,2024,Netherlands,NA
ID-1168,article,Existential anxiety about artificial intelligence (AI)- is it the end of humanity era or a new chapter in the human revolution: questionnaire-based observational study,"Background: Existential anxiety can profoundly affect an individual, influencing their perceptions, behaviours, sense of well-being, academic performance, and decisions. Integrating artificial intelligence into society has elicited complex public reactions, marked by appreciation and concern, with its acceptance varying across demographics and influenced by factors such as age, gender, and prior AI experiences. This study aimed to investigate the existential anxiety about artificial intelligence (AI) in public in Saudi Arabia.

Methods: The present questionnaire-based observational, analytical cross-sectional study with a structured, self-administered survey was conducted via Google Forms, using a scale to assess the existential anxiety levels induced by the recent development of AI. The study encompassed a diverse population with a sample size of 300 participants.

Results: This study’s findings revealed a high prevalence of existential anxieties related to the rapid advancements in AI. Key concerns included the fear of death (96% of participants), fate’s unpredictability (86.3%), a sense of emptiness (79%), anxiety about meaninglessness (92.7%), guilt over potential AI-related catastrophes (87.7%), and fear of condemnation due to ethical dilemmas in AI (93%), highlighting widespread apprehensions about humanity’s future in an AI-dominated era.

Conclusion: The public has concerns including unpredictability, a sense of emptiness, anxiety, guilt over potential AI-related catastrophes, and fear of condemnation due to ethical dilemmas in AI, highlighting widespread apprehensions about humanity’s future in an AI-dominated era. The results indicate that there is a need for a multidisciplinary strategy to address the existential anxieties in the AI era. The strategic approach must blend technological advancements with psychological, philosophical, and ethical insights, underscoring the significance of human values in an increasingly technology-driven world.

",https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2024.1368122/full,1,1,1,NA,NA,NA,"not the most relevant, but still potentially useful. survey data on people's existential anxiety about the prevalence of AI",Joud Mohammed Alkhalifah and Abdulrahman Mohammed Bedaiwi and Narmeen Shaikh and Waleed Seddiq and Sultan Ayoub Meo,10.3389/fpsyt.2024.1368122,1664-0640,NA,Frontiers in psychiatry,"anxiety,artificial intelligence,existential anxiety,fear,human revolution",NA,NA,1368122,NA,NA,15,http://dx.doi.org/10.3389/fpsyt.2024.1368122 https://www.ncbi.nlm.nih.gov/pubmed/38654726 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11036542,2024,Switzerland,NA
ID-1187,article,Knowledge sharing in manufacturing using LLM-powered tools: user study and,"Recent advances in natural language processing enable more intelligent ways to support knowledge sharing in factories. In manufacturing, operating production lines has become increasingly knowledge-intensive, putting strain on a factory's capacity to train and support new operators. This paper introduces a Large Language Model (LLM)-based system designed to retrieve information from the extensive knowledge contained in factory documentation and knowledge shared by expert operators. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. We conducted a user study at a factory to assess its potential impact and adoption, eliciting several perceived benefits, namely, enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several commercial and open-sourced LLMs for this system. The current state-of-the-art model, GPT-4, consistently outperformed its counterparts, with open-source models trailing closely, presenting an attractive option given their data privacy and customization benefits. In summary, this work offers preliminary insights and a system design for factories considering using LLM tools for knowledge management.",https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1293084/full,1,2,2,NA,NA,NA,concerns about potential safety risks and the efficacy of information retrieval compared to consulting expert personnel.,Samuel Kernan Freire and Chaofan Wang and Mina Foosherian and Stefan Wellsandt and Santiago Ruiz-Arenas and Evangelos Niforatos,10.3389/frai.2024.1293084,2624-8212,NA,Frontiers in artificial intelligence,"Large Language Models,benchmarking,factory,industrial settings,industry 5.0,information retrieval,knowledge sharing,natural language interface",NA,NA,1293084,NA,NA,7,http://dx.doi.org/10.3389/frai.2024.1293084 https://www.ncbi.nlm.nih.gov/pubmed/38601111 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11004332,2024,Switzerland,NA
ID-1191,article,"Modeling the influence of attitudes, trust, and beliefs on endoscopists’ acceptance of artificial intelligence applications in medical practice","Introduction: The potential for deployment of Artificial Intelligence (AI) technologies in various fields of medicine is vast, yet acceptance of AI amongst clinicians has been patchy. This research therefore examines the role of antecedents, namely trust, attitude, and beliefs in driving AI acceptance in clinical practice.

Methods: We utilized online surveys to gather data from clinicians in the field of gastroenterology.

Results: A total of 164 participants responded to the survey. Participants had a mean age of 44.49 (SD = 9.65). Most participants were male (n = 116, 70.30%) and specialized in gastroenterology (n = 153, 92.73%). Based on the results collected, we proposed and tested a model of AI acceptance in medical practice. Our findings showed that while the proposed drivers had a positive impact on AI tools’ acceptance, not all effects were direct. Trust and belief were found to fully mediate the effects of attitude on AI acceptance by clinicians.

Discussion: The role of trust and beliefs as primary mediators of the acceptance of AI in medical practice suggest that these should be areas of focus in AI education, engagement and training. This has implications for how AI systems can gain greater clinician acceptance to engender greater trust and adoption amongst public health systems and professional networks which in turn would impact how populations interface with AI. Implications for policy and practice, as well as future research in this nascent field, are discussed.",https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2023.1301563/full,1,1,1,medical,NA,NA,NA,Peter J Schulz and May O Lwin and Kalya M Kee and Wilson W B Goh and Thomas Y T Lam and Joseph J Y Sung,10.3389/fpubh.2023.1301563,2296-2565,NA,Frontiers in public health,"acceptance,artificial intelligence,attitudes,gastroenterology,trust",NA,NA,1301563,NA,NA,11,http://dx.doi.org/10.3389/fpubh.2023.1301563 https://www.ncbi.nlm.nih.gov/pubmed/38089040 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10715310,2023,Switzerland,NA
ID-1193,article,Competitive organizational climate and artificial intelligence (AI) acceptance: the moderating role of leaders’ power construal,"Introduction: The incorporation of Artificial Intelligence (AI) in organizations is pivotal to deal with work-related tasks and challenges effectively, yet little is known about the organizational factors that influence AI acceptance (i.e., employee favorable AI attitudes and AI use). To address this limitation in the literature and provide insight into the organizational antecedents influencing AI acceptance, this research investigated the relationship between competitive organizational climate and AI acceptance among employees. Moreover, given the critical role of a leader in employee attitude and behavior, we examined the moderating role of leaders’ power construal as responsibility or as opportunity in this relationship.

Methods: Study 1 was a three-wave field study among employees (N = 237, Mage = 38.28) working in various organizations in the UK. The study measured employees’ perception of a competitive organizational climate at Time 1, leaders’ power construal (as perceived by employees) at Time 2, and employee attitudes towards AI and their actual use of AI in the workplace at Times 2 and 3. Study 2 was a 2 (climate: highly competitive vs. low competitive) by 2 (power construal: responsibility vs. opportunity) experiment among employee participants (N = 150, Mage = 37.50).

Results: Study 1 demonstrated a positive relationship between competitive climate and employee AI use over time. Furthermore, both studies revealed an interaction between competitive climate and leader’s power construal in the prediction of employee AI acceptance: In Study 1, competitive climate was negatively related to AI acceptance over time when leaders construed power as opportunity. In Study 2 competitive climate was positively related to AI acceptance when leaders construed power as responsibility rather than as opportunity.

Discussion: These results underscore the organizational factors that are required in order for employees to shape favorable attitudes towards AI and actually use AI at work. Importantly, this research expands the limited body of literature on AI integration in organizations.",https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1359164/full,1,1,1,"workplace, industry",NA,NA,NA,Kyriaki Fousiani and Georgios Michelakis and Pieter A Minnigh and Kiki M M De Jonge,10.3389/fpsyg.2024.1359164,1664-1078,NA,Frontiers in psychology,"artificial intelligence (AI) acceptance in organizations,competitive organizational climate,employee attitudes,leadership,power as opportunity,power as responsibility,power construal",NA,NA,1359164,NA,NA,15,http://dx.doi.org/10.3389/fpsyg.2024.1359164 https://www.ncbi.nlm.nih.gov/pubmed/38596327 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11003519,2024,Switzerland,NA
ID-1201,article,"Integrating Artificial Intelligence in Pediatric Healthcare: Parental Perceptions and Ethical Implications
","Background: Our study aimed to explore the way artificial intelligence (AI) utilization is perceived in pediatric medicine, examining its acceptance among patients (in this case represented by their adult parents), and identify the challenges it presents in order to understand the factors influencing its adoption in clinical settings.

Methods: A structured questionnaire was applied to caregivers (parents or grandparents) of children who presented in tertiary pediatric clinics.

Results: The most significant differentiations were identified in relation to the level of education (e.g., aversion to AI involvement was 22.2% among those with postgraduate degrees, 43.9% among those with university degrees, and 54.5% among those who only completed high school). The greatest fear among respondents regarding the medical use of AI was related to the possibility of errors occurring (70.1%).

Conclusions: The general attitude toward the use of AI can be considered positive, provided that it remains human-supervised, and that the technology used is explained in detail by the physician. However, there were large differences among groups (mainly defined by education level) in the way AI is perceived and accepted.",https://www.mdpi.com/2227-9067/11/2/240,1,1,1,medical,NA,NA,NA,Elena Camelia Berghea and Marcela Daniela Ionescu and Radu Marian Gheorghiu and Iulia Florentina Tincu and Claudia Oana Cobilinschi and Mihai Craiu and Mihaela Bălgrădean and Florian Berghea,10.3390/children11020240,2227-9067,2,"Children (Basel, Switzerland)","artificial intelligence,attitude,ethics,medicine,pediatric",NA,NA,NA,NA,NA,11,http://dx.doi.org/10.3390/children11020240 https://www.ncbi.nlm.nih.gov/pubmed/38397353 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10887612,2024,Switzerland,NA
ID-1203,article,"Triangle of Trust in Cancer Care? The Physician, the Patient, and Artificial Intelligence Chatbot
","Trust, as a philosophic paradigm, is predominantly interpersonal, between human beings, and is differentiated from reliance. Can a person trust an inhumane amoral agent, such as a large language model artificial intelligence (AI) chatbot, to manifest the goodwill and willingness normally required in order for it to be deemed trustworthy? This article explores the relationship between the cancer patient, their physician, and AI chatbot in a proposed tripartite, consultative, personalized approach to shared-care in precision molecular oncology. It examines the nature of trust between human agents and machines. It also contemplates AI-enhanced technical precision in state-of-the-art cancer management, complemented by trustworthy, holistic clinical care by a physician, for each individual patient. ""To what extent can the user ""trust"" GPT-4?""",http://dx.doi.org/10.1089/cbr.2023.0112 https://www.ncbi.nlm.nih.gov/pubmed/37707991,1,1,1,NA,NA,NA,NA,J Harvey Turner,10.1089/cbr.2023.0112,1557-8852,9,Cancer biotherapy & radiopharmaceuticals,"phronesis,precision versus personalized oncology,shared-care,trust in artificial intelligence",NA,NA,581-584,NA,NA,38,http://dx.doi.org/10.1089/cbr.2023.0112 https://www.ncbi.nlm.nih.gov/pubmed/37707991,2023,United States,NA
ID-1206,article,The impact of human-AI collaboration types on consumer evaluation and usage intention: a perspective of responsibility attribution,"Despite the widespread availability of artificial intelligence (AI) products and services, consumer evaluations and adoption intentions have not met expectations. Existing research mainly focuses on AI’s instrumental attributes from the consumer perspective, along with negative impacts of AI failures on evaluations and willingness to use. However, research is lacking on AI as a collaborative agent, investigating the impact of human-AI collaboration on AI acceptance under different outcome expectations. This study examines the interactive effects of human-AI collaboration types (AI-dominant vs. AI-assisted) and outcome expectations (positive vs. negative) on AI product evaluations and usage willingness, along with the underlying mechanisms, from a human-AI relationship perspective. It also investigates the moderating role of algorithm transparency in these effects. Using three online experiments with analysis of variance and bootstrap methods, the study validates these interactive mechanisms, revealing the mediating role of attribution and moderating role of algorithm transparency. Experiment 1 confirms the interactive effects of human-AI collaboration types and outcome expectations on consumer evaluations and usage willingness. Under positive outcome expectations, consumers evaluate and express willingness to use AI-dominant intelligent vehicles with autonomous driving capabilities higher than those with emergency evasion capabilities (AI-assisted). However, under negative outcome expectations, consumers rate autonomous driving capabilities lower compared to emergency evasion capabilities. Experiment 2 examines the mediating role of attribution through ChatGPT’s dominant or assisting role under different outcome expectations. Experiment 3 uses a clinical decision-making system to study algorithm transparency’s moderating role, showing higher transparency improves evaluations and willingness to use AI products and services under negative outcome expectations. Theoretically, this study advances consumer behavior research by exploring the human-AI relationship within artificial intelligence, enhancing understanding of consumer acceptance variations. Practically, it offers insights for better integrating AI products and services into the market.

",https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1277861/full,1,1,1,NA,NA,NA,"AI-dominant vs. AI assisted, negative vs. positive outcome expectations",Beibei Yue and Hu Li,10.3389/fpsyg.2023.1277861,1664-1078,NA,Frontiers in psychology,"algorithm transparency,artificial intelligence,evaluation,human-AI collaboration,outcome expectation,responsibility attribution,usage intention",NA,NA,1277861,NA,NA,14,http://dx.doi.org/10.3389/fpsyg.2023.1277861 https://www.ncbi.nlm.nih.gov/pubmed/38022995 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10643528,2023,Switzerland,NA
ID-1232,article,"An integrative review on the acceptance of artificial intelligence among healthcare professionals in hospitals
","Artificial intelligence (AI) in the domain of healthcare is increasing in prominence. Acceptance is an indispensable prerequisite for the widespread implementation of AI. The aim of this integrative review is to explore barriers and facilitators influencing healthcare professionals’ acceptance of AI in the hospital setting. Forty-two articles met the inclusion criteria for this review. Pertinent elements to the study such as the type of AI, factors influencing acceptance, and the participants’ profession were extracted from the included studies, and the studies were appraised for their quality. The data extraction and results were presented according to the Unified Theory of Acceptance and Use of Technology (UTAUT) model. The included studies revealed a variety of facilitating and hindering factors for AI acceptance in the hospital setting. Clinical decision support systems (CDSS) were the AI form included in most studies (n = 21). Heterogeneous results with regard to the perceptions of the effects of AI on error occurrence, alert sensitivity and timely resources were reported. In contrast, fear of a loss of (professional) autonomy and difficulties in integrating AI into clinical workflows were unanimously reported to be hindering factors. On the other hand, training for the use of AI facilitated acceptance. Heterogeneous results may be explained by differences in the application and functioning of the different AI systems as well as inter-professional and interdisciplinary disparities. To conclude, in order to facilitate acceptance of AI among healthcare professionals it is advisable to integrate end-users in the early stages of AI development as well as to offer needs-adjusted training for the use of AI in healthcare and providing adequate infrastructure.

",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10257646,1,3,2,healthcare,NA,NA,UTAUT,Sophie Isabelle Lambert and Murielle Madi and Saša Sopka and Andrea Lenes and Hendrik Stange and Claus-Peter Buszello and Astrid Stephan,10.1038/s41746-023-00852-5,2398-6352,1,npj digital medicine,NA,NA,NA,111,NA,NA,6,http://dx.doi.org/10.1038/s41746-023-00852-5 https://www.ncbi.nlm.nih.gov/pubmed/37301946 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10257646,2023,NA,NA
ID-1246,article,Ethical and legal considerations influencing human involvement in the implementation of artificial intelligence in a clinical pathway: A multi-stakeholder perspective,"Introduction: Ethical and legal factors will have an important bearing on when and whether automation is appropriate in healthcare. There is a developing literature on the ethics of artificial intelligence (AI) in health, including specific legal or regulatory questions such as whether there is a right to an explanation of AI decision-making. However, there has been limited consideration of the specific ethical and legal factors that influence when, and in what form, human involvement may be required in the implementation of AI in a clinical pathway, and the views of the wide range of stakeholders involved. To address this question, we chose the exemplar of the pathway for the early detection of Barrett's Oesophagus (BE) and oesophageal adenocarcinoma, where Gehrung and colleagues have developed a “semi-automated”, deep-learning system to analyse samples from the CytospongeTM TFF3 test (a minimally invasive alternative to endoscopy), where AI promises to mitigate increasing demands for pathologists' time and input.

Methods: We gathered a multidisciplinary group of stakeholders, including developers, patients, healthcare professionals and regulators, to obtain their perspectives on the ethical and legal issues that may arise using this exemplar.

Results: The findings are grouped under six general themes: risk and potential harms; impacts on human experts; equity and bias; transparency and oversight; patient information and choice; accountability, moral responsibility and liability for error. Within these themes, a range of subtle and context-specific elements emerged, highlighting the importance of pre-implementation, interdisciplinary discussions and appreciation of pathway specific considerations.

Discussion: To evaluate these findings, we draw on the well-established principles of biomedical ethics identified by Beauchamp and Childress as a lens through which to view these results and their implications for personalised medicine. Our findings are not only relevant to this context but have implications for AI in digital pathology and healthcare more broadly.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10043985,1,2,2,healthcare,NA,NA,NA,Elizabeth Redrup Hill and Colin Mitchell and Tanya Brigden and Alison Hall,10.3389/fdgth.2023.1139210,2673-253X,NA,Frontiers in digital health,"artificial intelligence,digital pathology,healthcare,human involvement,interdisciplinary,law,medical ethics",NA,NA,1139210,NA,NA,5,http://dx.doi.org/10.3389/fdgth.2023.1139210 https://www.ncbi.nlm.nih.gov/pubmed/36999168 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10043985,2023,NA,NA
ID-1251,article,Assessing the value of ChatGPT for clinical decision support optimization,"Objective: To determine if ChatGPT can generate useful suggestions for improving clinical decision support (CDS) logic and to assess noninferiority compared to human-generated suggestions.

Methods: We supplied summaries of CDS logic to ChatGPT, an artificial intelligence (AI) tool for question answering that uses a large language model, and asked it to generate suggestions. We asked human clinician reviewers to review the AI-generated suggestions as well as human-generated suggestions for improving the same CDS alerts, and rate the suggestions for their usefulness, acceptance, relevance, understanding, workflow, bias, inversion, and redundancy.

Results: Five clinicians analyzed 36 AI-generated suggestions and 29 human-generated suggestions for 7 alerts. Of the 20 suggestions that scored highest in the survey, 9 were generated by ChatGPT. The suggestions generated by AI were found to offer unique perspectives and were evaluated as highly understandable and relevant, with moderate usefulness, low acceptance, bias, inversion, redundancy.

Conclusion: AI-generated suggestions could be an important complementary part of optimizing CDS alerts, can identify potential improvements to alert logic and support their implementation, and may even be able to assist experts in formulating their own suggestions for CDS improvement. ChatGPT shows great potential for using large language models and reinforcement learning from human feedback to improve CDS alert logic and potentially other medical areas involving complex, clinical logic, a key step in the development of an advanced learning health system.

",http://dx.doi.org/10.1101/2023.02.21.23286254 https://www.ncbi.nlm.nih.gov/pubmed/36865144 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9980251,1,1,1,"medical, healthcare",NA,NA,NA,Siru Liu and Aileen P Wright and Barron L Patterson and Jonathan P Wanderer and Robert W Turer and Scott D Nelson and Allison B McCoy and Dean F Sittig and Adam Wright,10.1101/2023.02.21.23286254,NA,NA,medRxiv: the preprint server for health sciences,NA,NA,NA,NA,NA,NA,NA,http://dx.doi.org/10.1101/2023.02.21.23286254 https://www.ncbi.nlm.nih.gov/pubmed/36865144 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9980251,2023,NA,NA
ID-1254,article,"Artificial intelligence (AI) acceptance in primary care during the coronavirus pandemic: What is the role of patients' gender, age and health awareness? A two-phase pilot study
","Background: Artificial intelligence (AI) is steadily entering and transforming the health care and Primary Care (PC) domains. AI-based applications assist physicians in disease detection, medical advice, triage, clinical decision-making, diagnostics and digital public health. Recent literature has explored physicians' perspectives on the potential impact of digital public health on key tasks in PC. However, limited attention has been given to patients' perspectives of AI acceptance in PC, specifically during the coronavirus pandemic. Addressing this research gap, we administered a pilot study to investigate criteria for patients' readiness to use AI-based PC applications by analyzing key factors affecting the adoption of digital public health technology.

Methods: The pilot study utilized a two-phase mixed methods approach. First, we conducted a qualitative study with 18 semi-structured interviews. Second, based on the Technology Readiness and Acceptance Model (TRAM), we conducted an online survey (n = 447).

Results: The results indicate that respondents who scored high on innovativeness had a higher level of readiness to use AI-based technology in PC during the coronavirus pandemic. Surprisingly, patients' health awareness and sociodemographic factors, such as age, gender and education, were not significant predictors of AI-based technology acceptance in PC.

Conclusions: This paper makes two major contributions. First, we highlight key social and behavioral determinants of acceptance of AI-enabled health care and PC applications. Second, we propose that to increase the usability of digital public health tools and accelerate patients' AI adoption, in complex digital public health care ecosystems, we call for implementing adaptive, population-specific promotions of AI technologies and applications.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9868720,1,"1, 2",1,"medical, healthcare",NA,NA,NA,Hila Chalutz Ben-Gal,10.3389/fpubh.2022.931225,2296-2565,NA,Frontiers in public health,"artificial intelligence,coronavirus pandemic,digital public health,health awareness,pilot study,primary care",NA,NA,931225,NA,NA,10,http://dx.doi.org/10.3389/fpubh.2022.931225 https://www.ncbi.nlm.nih.gov/pubmed/36699881 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9868720,2022,NA,NA
ID-1255,article,"Algorithms in the court: does it matter which part of the judicial decision-making is automated?
","Artificial intelligence plays an increasingly important role in legal disputes, influencing not only the reality outside the court but also the judicial decision-making process itself. While it is clear why judges may generally benefit from technology as a tool for reducing effort costs or increasing accuracy, the presence of technology in the judicial process may also affect the public perception of the courts. In particular, if individuals are averse to adjudication that involves a high degree of automation, particularly given fairness concerns, then judicial technology may yield lower benefits than expected. However, the degree of aversion may well depend on how technology is used, i.e., on the timing and strength of judicial reliance on algorithms. Using an exploratory survey, we investigate whether the stage in which judges turn to algorithms for assistance matters for individual beliefs about the fairness of case outcomes. Specifically, we elicit beliefs about the use of algorithms in four different stages of adjudication: (i) information acquisition, (ii) information analysis, (iii) decision selection, and (iv) decision implementation. Our analysis indicates that individuals generally perceive the use of algorithms as fairer in the information acquisition stage than in other stages. However, individuals with a legal profession also perceive automation in the decision implementation stage as less fair compared to other individuals. Our findings, hence, suggest that individuals do care about how and when algorithms are used in the courts.

",http://dx.doi.org/10.1007/s10506-022-09343-6 https://www.ncbi.nlm.nih.gov/pubmed/36643574 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9826621,1,1,1,"judicial, legal",fairness,NA,considers the timing and strength of reliance on judicial algorithms,Dovilė Barysė and Roee Sarel,10.1007/s10506-022-09343-6,0924-8463,1,Artificial intelligence and law,"Human-automation interaction,Judicial decision-making,Legal technologies,Levels of automation,Perceived fairness",NA,NA,1-30,NA,Springer Science and Business Media LLC,32,http://dx.doi.org/10.1007/s10506-022-09343-6 https://www.ncbi.nlm.nih.gov/pubmed/36643574 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9826621,2023,NA,NA
ID-1257,article,Art in an age of artificial intelligence,"Artificial intelligence (AI) will affect almost every aspect of our lives and replace many of our jobs. On one view, machines are well suited to take over automated tasks and humans would remain important to creative endeavors. In this essay, I examine this view critically and consider the possibility that AI will play a significant role in a quintessential creative activity, the appreciation and production of visual art. This possibility is likely even though attributes typically important to viewers–the agency of the artist, the uniqueness of the art and its purpose might not be relevant to AI art. Additionally, despite the fact that art at its most powerful communicates abstract ideas and nuanced emotions, I argue that AI need not understand ideas or experience emotions to produce meaningful and evocative art. AI is and will increasingly be a powerful tool for artists. The continuing development of aesthetically sensitive machines will challenge our notions of beauty, creativity, and the nature of art.

",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9749485,1,3,2,art,NA,NA,"not exactly a systematic review, but an essay synthesizing the research on AI in art. compares to AI in other domains in terms of ambiguity, identity-relevance. at least helpful to find references on AI in art",Anjan Chatterjee,10.3389/fpsyg.2022.1024449,1664-1078,NA,Frontiers in psychology,"aesthetics,artist,cognition,machine learning,neuroaesthetics",NA,NA,1024449,NA,Frontiers Media SA,13,http://dx.doi.org/10.3389/fpsyg.2022.1024449 https://www.ncbi.nlm.nih.gov/pubmed/36533018 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9749485,2022,NA,NA
ID-1267,article,"Acceptance of clinical artificial intelligence among physicians and medical students: A systematic review with cross-sectional survey
","Background
Artificial intelligence (AI) needs to be accepted and understood by physicians and medical students, but few have systematically assessed their attitudes. We investigated clinical AI acceptance among physicians and medical students around the world to provide implementation guidance.

Materials and methods
We conducted a two-stage study, involving a foundational systematic review of physician and medical student acceptance of clinical AI. This enabled us to design a suitable web-based questionnaire which was then distributed among practitioners and trainees around the world.

Results
Sixty studies were included in this systematic review, and 758 respondents from 39 countries completed the online questionnaire. Five (62.50%) of eight studies reported 65% or higher awareness regarding the application of clinical AI. Although, only 10–30% had actually used AI and 26 (74.28%) of 35 studies suggested there was a lack of AI knowledge. Our questionnaire uncovered 38% awareness rate and 20% utility rate of clinical AI, although 53% lacked basic knowledge of clinical AI. Forty-five studies mentioned attitudes toward clinical AI, and over 60% from 38 (84.44%) studies were positive about AI, although they were also concerned about the potential for unpredictable, incorrect results. Seventy-seven percent were optimistic about the prospect of clinical AI. The support rate for the statement that AI could replace physicians ranged from 6 to 78% across 40 studies which mentioned this topic. Five studies recommended that efforts should be made to increase collaboration. Our questionnaire showed 68% disagreed that AI would become a surrogate physician, but believed it should assist in clinical decision-making. Participants with different identities, experience and from different countries hold similar but subtly different attitudes.

Conclusion
Most physicians and medical students appear aware of the increasing application of clinical AI, but lack practical experience and related knowledge. Overall, participants have positive but reserved attitudes about AI. In spite of the mixed opinions around clinical AI becoming a surrogate physician, there was a consensus that collaborations between the two should be strengthened. Further education should be conducted to alleviate anxieties associated with change and adopting new technologies.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9472134,1,3,2,NA,NA,NA,NA,Mingyang Chen and Bo Zhang and Ziting Cai and Samuel Seery and Maria J Gonzalez and Nasra M Ali and Ran Ren and Youlin Qiao and Peng Xue and Yu Jiang,10.3389/fmed.2022.990604,2296-858X,NA,Frontiers in medicine,"acceptance,artificial intelligence (AI),attitude,medical students,physicians",NA,NA,990604,NA,Frontiers Media SA,9,http://dx.doi.org/10.3389/fmed.2022.990604 https://www.ncbi.nlm.nih.gov/pubmed/36117979 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9472134,2022,NA,NA
ID-1272,article,Should I Trust the Artificial Intelligence to Recruit? Recruiters’ Perceptions and Behavior When Faced With Algorithm-Based Recommendation Systems During Resume Screening,"Resume screening assisted by decision support systems that incorporate artificial intelligence is currently undergoing a strong development in many organizations, raising technical, managerial, legal, and ethical issues. The purpose of the present paper is to better understand the reactions of recruiters when they are offered algorithm-based recommendations during resume screening. Two polarized attitudes have been identified in the literature on users’ reactions to algorithm-based recommendations: algorithm aversion, which reflects a general distrust and preference for human recommendations; and automation bias, which corresponds to an overconfidence in the decisions or recommendations made by algorithmic decision support systems (ADSS). Drawing on results obtained in the field of automated decision support areas, we make the general hypothesis that recruiters trust human experts more than ADSS, because they distrust algorithms for subjective decisions such as recruitment. An experiment on resume screening was conducted on a sample of professionals (N = 694) involved in the screening of job applications. They were asked to study a job offer, then evaluate two fictitious resumes in a 2 × 2 factorial design with manipulation of the type of recommendation (no recommendation/algorithmic recommendation/human expert recommendation) and of the consistency of the recommendations (consistent vs. inconsistent recommendation). Our results support the general hypothesis of preference for human recommendations: recruiters exhibit a higher level of trust toward human expert recommendations compared with algorithmic recommendations. However, we also found that recommendation’s consistence has a differential and unexpected impact on decisions: in the presence of an inconsistent algorithmic recommendation, recruiters favored the unsuitable over the suitable resume. Our results also show that specific personality traits (extraversion, neuroticism, and self-confidence) are associated with a differential use of algorithmic recommendations. Implications for research and HR policies are finally discussed.

",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9298741,1,1,1,candidate screening,NA,NA,NA,Alain Lacroux and Christelle Martin-Lacroux,10.3389/fpsyg.2022.895997,1664-1078,NA,Frontiers in psychology,"algorithm aversion,algorithmic decision support systems,artificial intelligence,resume screening,trust",NA,NA,895997,NA,NA,13,http://dx.doi.org/10.3389/fpsyg.2022.895997 https://www.ncbi.nlm.nih.gov/pubmed/35874355 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9298741,2022,NA,NA
ID-1274,article,Artificial Intelligence Can’t Be Charmed: The Effects of Impartiality on Laypeople’s Algorithmic Preferences,"Over the coming years, AI could increasingly replace humans for making complex decisions because of the promise it holds for standardizing and debiasing decision-making procedures. Despite intense debates regarding algorithmic fairness, little research has examined how laypeople react when resource-allocation decisions are turned over to AI. We address this question by examining the role of perceived impartiality as a factor that can influence the acceptance of AI as a replacement for human decision-makers. We posit that laypeople attribute greater impartiality to AI than human decision-makers. Our investigation shows that people value impartiality in decision procedures that concern the allocation of scarce resources and that people perceive AI as more capable of impartiality than humans. Yet, paradoxically, laypeople prefer human decision-makers in allocation decisions. This preference reverses when potential human biases are made salient. The findings highlight the importance of impartiality in AI and thus hold implications for the design of policy measures.

",http://dx.doi.org/10.3389/fpsyg.2022.898027 https://www.ncbi.nlm.nih.gov/pubmed/35846643 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9277554,1,1,1,NA,NA,NA,NA,Marius C Claudy and Karl Aquino and Maja Graso,10.3389/fpsyg.2022.898027,1664-1078,NA,Frontiers in psychology,"algorithm aversion,artificial intelligence,decision-making,impartiality,procedural justice",NA,NA,898027,NA,Frontiers Media SA,13,http://dx.doi.org/10.3389/fpsyg.2022.898027 https://www.ncbi.nlm.nih.gov/pubmed/35846643 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9277554,2022,NA,NA
ID-1277,article,"Adoption of AI-Enabled Tools in Social Development Organizations in India: An Extension of UTAUT Model
","Social development organizations increasingly employ artificial intelligence (AI)-enabled tools to help team members collaborate effectively and efficiently. These tools are used in various team management tasks and activities. Based on the unified theory of acceptance and use of technology (UTAUT), this study explores various factors influencing employees’ use of AI-enabled tools. The study extends the model in two ways: a) by evaluating the impact of these tools on the employees’ collaboration and b) by exploring the moderating role of AI aversion. Data were collected through an online survey of employees working with AI-enabled tools. The analysis of the research model was conducted using partial least squares (PLS), with a two-step model – measurement and structural models of assessment. The results revealed that the antecedent variables, such as effort expectancy, performance expectancy, social influence, and facilitating conditions, are positively associated with using AI-enabled tools, which have a positive relationship with collaboration. It also concluded a significant effect of AI aversion in the relationship between performance expectancy and use of technology. These findings imply that organizations should focus on building an environment to adopt AI-enabled tools while also addressing employees’ concerns about AI.

",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9251489,1,1,1,NA,NA,NA,AI aversion as a moderator,Ruchika Jain and Naval Garg and Shikha N Khera,10.3389/fpsyg.2022.893691,1664-1078,NA,Frontiers in psychology,"AI aversion,UTAUT,artificial intelligence,collaboration,social organizations",NA,NA,893691,NA,NA,13,http://dx.doi.org/10.3389/fpsyg.2022.893691 https://www.ncbi.nlm.nih.gov/pubmed/35795409 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9251489,2022,NA,NA
ID-1284,article,How transparency modulates trust in artificial intelligence,"Recent advances in artificial intelligence (AI) and machine learning have brought the study of human-AI (HAI) teams into sharper focus. An important set of questions for those designing HAI interfaces concerns trust—specifically, human trust in the AI systems with which they form teams. We review the literature on how perceiving an AI making mistakes violates trust and how such violations might be repaired. In doing so, we discuss the role played by various forms of algorithmic transparency in the process of trust repair, including explanations of algorithms, uncertainty estimates, and performance metrics.

",https://www.sciencedirect.com/science/article/pii/S2666389922000289,1,3,2,NA,"errors, uncertainty",NA,NA,John Zerilli and Umang Bhatt and Adrian Weller,10.1016/j.patter.2022.100455,2666-3899,4,"Patterns (New York, N.Y.)","artificial intelligence,explainable AI,human factors,human-AI teams,human-computer interaction,machine learning,transparency,trust",NA,NA,100455,NA,Elsevier BV,3,http://dx.doi.org/10.1016/j.patter.2022.100455 https://www.ncbi.nlm.nih.gov/pubmed/35465233 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9023880,2022,NA,NA
ID-1289,article,Interactive Design Psychology and Artificial Intelligence-Based Innovative Exploration of Anglo-American Traumatic Narrative Literature,"The advent of the intelligence age has injected new elements into the development of literature. The synergic modification of Anglo-American (AAL) traumatic narrative (TN) literature by artificial intelligence (AI) technology and interactive design (ID) psychology will produce new possibilities in literary creation. First, by studying natural language processing (NLP) technology, this study proposes a modification language model (LM) based on the double-layered recurrent neural network (RNN) algorithm and constructs an intelligent language modification system based on the improved LM model. The results show that the performance of the proposed model is excellent; only about 30% of the respondents like AAL literature; the lack of common cultural background, appreciation difficulties, and language barriers have become the main reasons for the decline of reading willingness of AAL literature. Finally, AI technology and ID psychology are used to modify a famous TN work respectively and synergically, and the modified work is appreciated by respondents to collect their comments. The results corroborate that 62% of the respondents like original articles, but their likability scores have decreased for individually modified work by AI or ID psychology. In comparison, under the synergic modification efforts of AI and ID psychology, the popularity of the modified work has increased slightly, with 65% of the respondents showing a likability to read. Therefore, it is concluded that literary modification by single ID psychology or AI technology will reduce the reading threshold by trading off the literary value of the original work. The core of literary creation depends on human intelligence, and AI might still not be able to generate high-standard literary works independently because human minds and thoughts cannot be controlled and predicted by machines. The research results provide new ideas and improvement directions for the field of AI-assisted writing.

",https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2021.755039/full,1,1,1,writing,NA,NA,NA,Xia Hou and Noritah Omar and Jue Wang,10.3389/fpsyg.2021.755039,1664-1078,NA,Frontiers in psychology,"Anglo-American literature,artificial intelligence,interactive design,psychology,trauma narrative",NA,NA,755039,NA,Frontiers Media SA,12,http://dx.doi.org/10.3389/fpsyg.2021.755039 https://www.ncbi.nlm.nih.gov/pubmed/35222140 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8866447,2021,NA,NA
ID-1296,article,"Assessment of the Willingness of Radiologists and Radiographers to Accept the Integration of Artificial Intelligence Into Radiology Practice
","Rationale and objectives: This study aimed to investigate radiologists' and radiographers' knowledge, perception, readiness, and challenges regarding Artificial Intelligence (AI) integration into radiology practice.

Materials and methods: An electronically distributed cross-sectional study was conducted among radiologists and radiographers in the United Arab Emirates. The questionnaire captured the participants' demographics, qualifications, professional experience, and postgraduate training. Their knowledge, perception, organisational readiness, and challenges regarding AI integration into radiology were examined.

Results: There was a significant lack of knowledge and appreciation of the integration of AI into radiology practice. Organisations are stepping toward building AI implementation strategies. The availability of appropriate training courses is the main challenge for both radiographers and radiologists.

Conclusion: The excitement of AI implementation into radiology practise was accompanied by a lack of knowledge and effort required to improve the user's appreciation of AI. The knowledge gap requires collaboration between educational institutes and professional bodies to develop structured training programs for radiologists and radiographers.",https://www.ncbi.nlm.nih.gov/pubmed/33129659,1,1,1,"medical, healthcare",NA,NA,"survey collecting radiologists' knowledge and perceptions of AI integration in radiology. Another article showing that despite AI's proven capabilities in the medical domain, the experts resist the technology. threat to identity",Mohamed M Abuzaid and Wiam Elshami and Huseyin Tekin and Bashar Issa,10.1016/j.acra.2020.09.014,1076-6332,1,Academic radiology,"Artificial Intelligence,Knowledge,Questionnaires,Radiography,Radiology,Surveys",NA,NA,87-94,NA,Elsevier BV,29,http://dx.doi.org/10.1016/j.acra.2020.09.014 https://www.ncbi.nlm.nih.gov/pubmed/33129659,2022,NA,NA
ID-1299,article,Conservatism predicts aversion to consequential Artificial Intelligence,"Artificial intelligence (AI) has the potential to revolutionize society by automating tasks as diverse as driving cars, diagnosing diseases, and providing legal advice. The degree to which AI can improve outcomes in these and other domains depends on how comfortable people are trusting AI for these tasks, which in turn depends on lay perceptions of AI. The present research examines how these critical lay perceptions may vary as a function of conservatism. Using five survey experiments, we find that political conservatism is associated with low comfort with and trust in AI-i.e., with AI aversion. This relationship between conservatism and AI aversion is explained by the link between conservatism and risk perception; more conservative individuals perceive AI as being riskier and are therefore more averse to its adoption. Finally, we test whether a moral reframing intervention can reduce AI aversion among conservatives.

",https://www.ncbi.nlm.nih.gov/pubmed/34928989,1,1,1,NA,NA,NA,political conservatism correlated with low trust in AI,Noah Castelo and Adrian F Ward,10.1371/journal.pone.0261467,1932-6203,12,PloS one,NA,NA,NA,e0261467,NA,Public Library of Science (PLoS),16,http://dx.doi.org/10.1371/journal.pone.0261467 https://www.ncbi.nlm.nih.gov/pubmed/34928989 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8687590,2021,NA,NA
ID-1322,article,The effects of personality and locus of control on trust in humans versus,"Introduction
We are increasingly exposed to applications that embed some sort of artificial intelligence (AI) algorithm, and there is a general belief that people trust any AI-based product or service without question. This study investigated the effect of personality characteristics (Big Five Inventory (BFI) traits and locus of control (LOC)) on trust behaviour, and the extent to which people trust the advice from an AI-based algorithm, more than humans, in a decision-making card game.

Method
One hundred and seventy-one adult volunteers decided whether the final covered card, in a five-card sequence over ten trials, had a higher/lower number than the second-to-last card. They either received no suggestion (control), recommendations from what they were told were previous participants (humans), or an AI-based algorithm (AI). Trust behaviour was measured as response time and concordance (number of participants' responses that were the same as the suggestion), and trust beliefs were measured as self-reported trust ratings.

Results
It was found that LOC influences trust concordance and trust ratings, which are correlated. In particular, LOC negatively predicted beyond the BFI dimensions trust concordance. As LOC levels increased, people were less likely to follow suggestions from both humans or AI. Neuroticism negatively predicted trust ratings. Openness predicted reaction time, but only for suggestions from previous participants. However, people chose the AI suggestions more than those from humans, and self-reported that they believed such recommendations more.

Conclusions
The results indicate that LOC accounts for a significant variance for trust concordance and trust ratings, predicting beyond BFI traits, and affects the way people select whom they trust whether humans or AI. These findings also support the AI-based algorithm appreciation.","https://www.sciencedirect.com/science/article/pii/S240584402031416X#:~:text=It%20was%20found%20that%20LOC,Neuroticism%20negatively%20predicted%20trust%20ratings.",1,1,1,NA,NA,NA,"locus of control as a user characteristic predicting algorithm aversion. DVs: response time, agreement with recommendation, and self-reported trust.",Navya Nishith Sharan and Daniela Maria Romano,10.1016/j.heliyon.2020.e04572,2405-8440,8,Heliyon,"Artificial intelligence,Big five personality traits,Individual traits,Locus of control,Psychology,Trust",NA,NA,e04572,NA,NA,6,http://dx.doi.org/10.1016/j.heliyon.2020.e04572 https://www.ncbi.nlm.nih.gov/pubmed/32923706 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7475230,2020,NA,NA
ID-1338,article,Algorithm Aversion in Delegated Investing,"The tendency of humans to shy away from using algorithms--even when algorithms observably outperform their human counterpart--has been referred to as algorithm aversion. We conduct an experiment with young adults to test for algorithm aversion in financial decision making. Participants acting as investors can tie their incentives to either a human fund manager or an investment algorithm. We find no sign of algorithm aversion: participants care about returns, but do not have strong preferences which financial intermediary obtains these returns. Contrary to what has been suggested, participants are neither quicker to lose confidence in the algorithm after seeing it err. However, we find that participants' inability to separate skill and luck when evaluating intermediaries slows down their migration to the algorithm.",https://link-springer-com.stanford.idm.oclc.org/article/10.1007/s11573-022-01121-9,1,1,1,finance,NA,"1. survey responses about performance of algorithms vs. humans, whether algorithms can replace humans in the financial domain, etc. 2. Investor participants decide whether they want to invest in a human fund manager or an algorithm. Note their definition of algorithm: ""Dietvorst et al. (2015, p. 114) define the term algorithm to “encompass any evidence-based forecasting formula or rule. Thus, the term includes statistical methods, decision rules, and all other mechanical procedures that can be used for forecasting.” For an investment context, we derive the following criteria for the algorithm: (1) once constructed, it must act independently of a human, (2) it must be strictly rule-based, and (3) its recommended actions must be executed automatically.""",NA,Maximilian Germann and Christoph Merkle,10.1007/s11573-022-01121-9,442372,9,Journal of Business Economics,Optimization Techniques; Programming Models; Dynamic Analysis C61,NA,Accession Number: 2072738; Publication Type: Journal Article; Update Code: 20231130,1691-1727,NA,NA,93,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2072738&site=ehost-live&authtype=ip,sso&custid=s4392798",2023,NA,NA
ID-1344,article,Robo-advice and the Future of Delegated Investment,"Robo-advisors can replace financial advisors and asset managers at low costs. However, human managers and advisors will survive. This is predominantly because although robo-advisors primarily appeal to a clientele of already financially sophisticated investors, they lack some of the qualities people look for in a ""money doctor"", and their business models have not yet stood the test of time. While a general algorithm aversion is absent in the financial domain, even tech-savvy millennials do not particularly favor robo-advisors. As new survey data shows, investors view algorithms as an aid to human managers rather than competitors. A hybrid model with humans and robos working together, as already implemented by some financial institutions, might be the future of delegated investment.",https://elsevier-ssrn-document-store-prod.s3.amazonaws.com/20/05/28/ssrn_id3612986_code743829.pdf?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQC5tl6w49m%2BTF8etOq8YBtDb4A3V%2BnKjAsTJ3EnnPaFfgIhAJnF3RjvtOgDiuFzAA8GZ4t%2B2KxxPBN6X7wepIvNVZ%2BCKsYFCJL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBBoMMzA4NDc1MzAxMjU3IgzzEhb1mNbHOS6dpo0qmgW5XQpAA3ZdOl7W%2Bl8U%2F1KZUZuqpJpLmAwSpclnfrDO0SDvz3EFycBwy6MGCxySn8lhtMc0uvN1Rg%2BSww1d9m5%2BSaBZ5pKboEIxlqe8EsTH%2B6qNBkep6uT1qiqUSy8jAnzzn8n3knWZpImbEqyf8uYXiuYbTBpvX6j85dLSgbRC2bXYqGVDoYaqYtCHiWGu22HI35AYfGH5fqa7PG2tpRmcHsCQq1RrW60R752z6PJwHSVgLPtxGHSDUwwZeTxc%2Fw5n31kuhcPGHbn6CWgeBgEYWAcOwtYIUJiSuXFMP1Wki4GxY9W0vQNgYJXdft007tY6egBPDOfilmDFYd0zynfKFzn1jkGL2UJ9kYEM7yAOfoHvlLrkMHnHCHj4IP0uvP3OmIm66%2BkWfdfJJdfrzOsV%2BztS0f6nkYIOaDc7E%2BrzMF87NR6dnXm541i%2Frt3AOqgwdigQ5%2BbX3BpCyzbPLYEL%2FqjXDnSB7rN9JIWYoAWOxmo3NTH18D0gRxhuOdcTWSjsG8i5qa7boFMJ96GWs0askGryorqbaDFTcHAPQp303xYw7vKDUfvBWE96XddUv2DE0Y2PAIRtoLuGhIzvgBV79SYyXZP1AFFtTuJsoUjZ761%2BwmfTZH5z50UnEo0Pun5SV24FykIPdZnW6ZUVrsvs701C7XjAOBxcaVD2abKzHZRwRVdpZBtFTfuYXFTz%2Bbqc5MdqcJc369FIsM0dlGFCDpvVQxHcoO0QefkoZJx%2FR%2BtzbDE7FdHG5GD2PakezRRQRpfFWcakkvbpQPjc%2F0sN7xujqSFQfCbckQMUnsAbro3p4x8KfRpd%2BuZ29wuocc7vw0CINTDP9AS1yP5C0QrZAHzMQRX7YavYD8Xr1Gfmx1W42MX3jSW0kEow7obptQY6sAGKFmq5aMY2NKOYMw1zXy3dTf68SFgNM4%2FBS0l9M7BwOHJ5IGU9%2Ftr9BkDNqc6Am1z3s2VF8I8iZi6cH00vRQA0EWgSZDwegCwbSm1%2FbSVfIP56sXoxp8o0bHqy%2FG8eHb3d6ojnWJHyl04Xn7S7%2FA7qE9Io3uMmIpCdSuxXhcqnHJAbzpIP%2FJmj%2BL6RrRDjb%2ByBX0iTfG7fns%2Bg0oln8bjL9WCS59nQi8qRhapZrF8GHA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240812T181317Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAUPUUPRWE4MAKN3UH%2F20240812%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=3785712cc1eda3c48a1f1eb7e74e43fb511258ee5cd1505426c2ec43f5eeeac0,1,3,2,financial,NA,NA,"Some investors are hesitant to adopt robo-advice because of “enduring human strengths”: areas in which robo-advisors are unlikely
to catch up soon. These include the ability to steady clients in through difficult markets, to
persuade to action, to provide validation, and to synthesize custom client solutions. Authors propose a hybrid model as an acceptable way forward.",Christoph Merkle,NA,1755361X,51,Journal of Financial Transformation,"Household Saving, Borrowing, Debt, and Wealth G51,Investment Banking; Venture Capital; Brokerage; Ratings and Ratings Agencies G24,Personal, Professional, and Business Services L84",4,Accession Number: 1864846; Keywords: Financial Institution; Publication Type: Journal Article; Update Code: 20201203,20-27,NA,NA,NA,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1864846&site=ehost-live&authtype=ip,sso&custid=s4392798 http://www.capco.com/insights/capco-institute",2020,NA,NA
ID-1350,article,Adopting Robot Lawyer? The Extending Artificial Intelligence Robot Lawyer Technology Acceptance Model for Legal Industry by an Exploratory Study,"The development of artificial intelligence has created new opportunities and challenges in industries. The competition between robots and humans has elicited extensive attention among legal researchers. In this exploratory study, we addressed issues regarding the introduction of robots to the practice of legal service through a semistructured interviews with lawyers, judges, artificial intelligence experts, and potential clients. An extended robot lawyer technology acceptance model with five facets and 11 elements is proposed in this study. This model highlights two dimensions: 'legal use' and 'perception of trust.' In summary, this study provides new specific implications and exhibits three characteristics, namely, derivative, macroscopic, and instructive, in the legal services with artificial intelligence. In addition, artificial intelligence robot lawyers are being developed with some of the abilities necessary to substitute for human beings. Nevertheless, working with human lawyers is imperative to produce benefits from this type of reciprocity.",https://www-cambridge-org.stanford.idm.oclc.org/core/services/aop-cambridge-core/content/view/DA4AFD28E477B20CC0807ACEB6A6A98F/S1833367218000810a.pdf/adopting_robot_lawyer_the_extending_artificial_intelligence_robot_lawyer_technology_acceptance_model_for_legal_industry_by_an_exploratory_study.pdf,1,2,2,legal,"sense of trust, perceived ease of use",NA,"1. identify irreplaceable human qualities; 2. propose a technology acceptance model (TAM) specific to robots in law. According to TAM, the actual behavior of users to accept new technology depends on their intention to use it; perceived usefulness and perceived ease of use will affect user intention. Interviewees in this study identified some irreplaceable human qualities: intuition, empathy when communicating with humans, creativity, psychological warfare to demoralize adversaries, negotiaion ability",Ni Xu and Kung-Jeng Wang,NA,18333672,5,Journal of Management and Organization,"Personal, Professional, and Business Services L84,Technological Change: Choices and Consequences; Diffusion Processes O33",9,Accession Number: 2003223; Keywords: Lawyer;  Legal Service;  Services;  Technology; Publication Type: Journal Article; Update Code: 20221124,867-885,NA,NA,27,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2003223&site=ehost-live&authtype=ip,sso&custid=s4392798",2021,NA,NA
ID-1351,article,AI Adoption in the Hiring Process--Important Criteria and Extent of AI Adoption,"Purpose: In the context of new workplace environment, this study aims to study and generate insights about artificial intelligence (AI) adoption in hiring process of firms. It is very relevant when AI is dramatically reshaping hiring function in the changing scenario. Design/methodology/approach: The objectives are achieved with the help of three studies involving Delphi method to explore the criteria for AI adoption decision. Followed by two multi criteria decision-making techniques, i.e., analytic hierarchy process to identify weights of the criteria and fuzzy technique for order preference by similarity to ideal solution to assess the extent of AI adoption in hiring. Findings: The findings reveal that information security and return on investment are considered two very important criteria by human resources managers while contemplating the adoption of AI in hiring process. It was found that AI adoption will be suitable at the sourcing and initial screening stages of hiring. And the suitability of the hiring stage where AI can be applied has been found to have changed from before and after the onset of COVID-19 pandemic situation. The findings and its discussion assist and enhance better decisions about AI adoption in hiring processes of firms amid changing scenario--external and internal to a firm.",https://www.proquest.com/docview/2786667679?accountid=14026&sourcetype=Scholarly%20Journals,1,1,1,hiring,NA,NA,"identifies the factors managers consider when hiring people, uses Multi-Criteria Decision Making model to determine at what point it is appropriate to use AI in the hiring process",Prachi Bhatt,NA,14636689,1,Foresight,"Labor Demand J23,Neural Networks and Related Topics C45,Personnel Economics: Firm Employment Decisions; Promotions M51,Personnel Management; Executives; Executive Compensation M12",NA,Accession Number: 2063105; Keywords: Firm;  Firms;  Hiring; Publication Type: Journal Article; Update Code: 20230921,144-163,NA,NA,25,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2063105&site=ehost-live&authtype=ip,sso&custid=s4392798",2023,NA,NA
ID-1356,article,Are Robots Taking Over? Technological Advancements and Investor Risk Tolerance,"The banking industry is being overhauled by robots. Artificial intelligence (AI) is the most recent technological breakthrough made in the banking industry. There is a wide range of variables that influence investor risk tolerance which has previously been examined, however, the influence of technological advancements on investor risk tolerance in a South African context remains unsolved. This paper aims to investigate the influence of technological advancements on investor risk tolerance. The results of this study found technological factors contributed significantly towards explaining high risk tolerance behaviour to a rather moderate degree. Evidence suggests investors employing Robo-advisors for assistance when making investment decisions, tend to become more risk-tolerant. The results of this study procured that certain demographic variables included in this study have a significant influence on the individual investor risk tolerance levels of South African investors. It is recommended that the research study be utilised by individual investors, financial planners, investment companies and current or future researchers originating from both frontiers to be acquainted with how technological advancements influence investor risk tolerance. Therefore, ensuring technological advancements are used accordingly, to the benefit of the investor privately or in practice.",https://dj.univ-danubius.ro/index.php/AUDOE/article/view/986/1414,1,1,1,financial,NA,NA,"survey data, demographic factors as predictors of AI acceptance in banking",Jaenre Pietersen and Sune Ferreira-Schenk and Zandri Dickason-Koekemoer,NA,20650175,4,Acta Universitatis Danubius. Economica,"Banks; Depository Institutions; Micro Finance Institutions; Mortgages G21,Behavioral Finance: Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making in Financial Markets G41,Economic Development: Financial Markets; Saving and Capital Investment; Corporate Finance and Governance O16,IT Management M15,Personal, Professional, and Business Services L84",NA,Accession Number: 2053275; Keywords: Banking;  Financial Planner; Geographic Descriptors: South Africa; Geographic Region: Africa; Publication Type: Journal Article; Update Code: 20230727,7-27,NA,NA,17,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2053275&site=ehost-live&authtype=ip,sso&custid=s4392798",2021,NA,NA
ID-1357,article,Are You Adopting Artificial Intelligence Products? Social-Demographic Factors to Explain Customer Acceptance,"Are consumers accepting AI-based products? What are the socio-demographics influencing the adoption of these products? This study tests the potential users' social-demographic characteristics that influence the relationship between innovation and AI-based products. The latter are robots (e.g., chatbots) and AI (e.g., recommendation systems, amongst others). A mixed methods approach is adopted using both qualitative and quantitative analysis with non-metrical multidimensional scaling (NMDS) to map opinions about digitally-intensive products, such as robots and AI, and the attitude towards innovation. The research uses data on the general population from the Spanish innovation barometer survey (N = 3,005). Findings show that individuals who have a negative attitude towards innovation have a negative opinion about robots and AI. As regards social-demographic dimensions, age and economic conditions moderate this effect, causing a more positive opinion towards digitally-intensive products amongst young people and individuals with a higher socio-economic level. These effects are increased by the moderating role of sex.",https://web.p.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=1&sid=fe58a17b-2535-4734-bd23-bbab3c1e7cbf%40redis,1,"1, 2",1,NA,NA,NA,attitudes toward innovation predict opinions about AI and robots,Mariano Mendez-Suarez and Abel Monfort and Jose-Luis Hervas-Oliver,10.1016/j.iedeen.2023.100223,24448834,3,European Research on Management and Business Economics,"Consumer Economics: Empirical Analysis D12,Economics of Gender; Non-labor Discrimination J16,Innovation and Invention: Processes and Incentives O31,Technological Change: Choices and Consequences; Diffusion Processes O33",9,"Accession Number: 2075059 Alternate Accession Number: EP174091364; Keywords: Consumer;  Demographics;  Individual;  Innovation; Publication Type: Journal Article; Update Code: 20231214; Copyright: Copyright of European Research on Management & Business Economics is the property of Asociacion Europea de Direccion y Economia de la Empresa (AEDEM) and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.",NA,NA,NA,29,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2075059&site=ehost-live&authtype=ip,sso&custid=s4392798",2023,NA,NA
ID-1359,article,Artificial Intelligence in Electronic Commerce: Basic Chatbots and the Consumer Journey,"This study aims to empirically cover the impact of the use of artificial intelligence through chatbots on online retail in terms of content implemented in the communication process. The presented research brings a contribution to the specialized literature by analyzing the perceived utility and demonstrating the facility, key concepts of the Technology Acceptance Model. In this sense, ten online stores in Romania were studied, selected according to the number of users, the research being carried out through a non-reactive method--content analysis. The method of data collection was that of the ""mysterious client"" in order not to generate a change in the behavior of the entities studied. The interpretation of the data obtained through the content grid allowed a horizontal and vertical approach that led to a series of results that confirmed the low level of performance of market leaders, as well as the high potential of this type of technology applied in the field. Regarding the impact of the use of chatbots, it has been shown that poor quality of the content displayed to users affects the consumer's journey, the point of satisfaction not being reached in these conditions.",https://www.econstor.eu/bitstream/10419/281561/1/1793800693.pdf,1,1,e-commerce,NA,NA,NA,NA,Eliza Nichifor and Adrian Trifan and Elena Mihaela Nechifor,NA,15829146,56,Amfiteatru Economic,"Consumer Economics: Empirical Analysis D12,IT Management M15,Marketing M31,Retail and Wholesale Trade; e-Commerce L81",2,"Accession Number: 1917207 Alternate Accession Number: EP148509335; Keywords: Consumer;  Retail; Publication Type: Journal Article; Update Code: 20210826; Copyright: Copyright of Amfiteatru Economic is the property of Academia de Studii Economice and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.",87-101,NA,NA,23,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1917207&site=ehost-live&authtype=ip,sso&custid=s4392798 http://www.amfiteatrueconomic.ro/Arhiva_EN.aspx",2021,NA,NA
ID-1362,article,Artificial Intelligence in the Legal Sector: Pressures and Challenges of Transformation,"Recent technological developments in automation and artificial intelligence (AI) promise to disrupt the very foundations of how legal work is practised and delivered. Yet how they challenge current business models, where they encounter resistance and how the benefits of AI can be realised remain unexplored. Drawing on interviews with professionals in the UK legal services sector, the article highlights how technological and market pressures combine to challenge the business models of legal services firms. However, the findings reveal important cultural and structural challenges that hamper transformation. The article extends the debate on technological disruption in legal services through a focus on business model innovation as a tool that can support firms in the sector to reimagine legal services provision.",https://academic.oup.com/cjres/article/13/1/135/5716343,1,2,2,financial,NA,NA,"clarification of AI vs. simple automation: ""Specifically, there is a question as to whether new legal technologies are underpinned by AI capabilities or merely employ automation to perform more basic functions. While not being asked specifically about this, three of the respondents raised this issue. Interestingly, they differentiated between automation, which underpins the majority of new legal technologies and which in their view should not be labelled as AI, and ‘true AI’ involving largely ML, NLP, and vast amounts of data to perform more advanced ‘cognitive’ functions such as interpretation.""",Chay Brooks and Cristian Gherhes and Tim Vorley,10.1093/cjres/rsz026,17521378,1,"Cambridge Journal of Regions, Economy and Society","Basic Areas of Law: General (Constitutional Law) K10,IT Management M15,Innovation and Invention: Processes and Incentives O31,Personal, Professional, and Business Services L84,Professional Labor Markets; Occupational Licensing J44",3,Accession Number: 1834809; Keywords: Automation;  Firm;  Firms;  Innovation;  Legal Service;  Professionals;  Services; Geographic Descriptors: U.K.; Geographic Region: Europe; Publication Type: Journal Article; Update Code: 20200611,135-152,NA,NA,13,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1834809&site=ehost-live&authtype=ip,sso&custid=s4392798 https://academic.oup.com/cjres/issue",2020,NA,NA
ID-1367,article,Chatbot Usage Intention Analysis: Veterinary Consultation,"This study uses artificial intelligence and big data technologies to develop a chatbot prototype for veterinary consultations. To understand pet owners' behavioral intentions to use the chatbot for veterinary consultations, we modify the technology acceptance model to develop a usage intention model for the veterinary consultation chatbot. We survey members of a pet network community by using Google Forms to collect data and partial least squares structural equation modeling for analysis. The results indicate that the perceived accuracy, perceived completeness, and perceived ease of use increased pet owners' user satisfaction of veterinary consultation chatbot; the perceived convenience and pet owners' user satisfaction increased pet owners' behavioral intention to use chatbot for veterinary consultations. This study can be used as a basis for evaluation of using intelligent technologies in pet healthcare consultation and pet disease management.","https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1907817&site=ehost-live&authtype=ip,sso&custid=s4392798",1,1,1,NA,"perceived accuracy, completeness, ease of use",NA,chatbot,Duen-Huang Huang and Hao-En Chueh,10.1016/j.jik.2020.09.002,2444569X,3,Journal of Innovation and Knowledge,"IT Management M15,Large Data Sets: Modeling and Analysis C55,Personal, Professional, and Business Services L84",7,Accession Number: 1907817; Keywords: Management;  Modeling; Publication Type: Journal Article; Update Code: 20210624,135-144,NA,NA,6,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1907817&site=ehost-live&authtype=ip,sso&custid=s4392798",2021,NA,NA
ID-1368,article,Consumer Acceptance of the Use of Artificial Intelligence in Online Shopping: Evidence from Hungary,"The rapid development of technology has drastically changed the way consumers do their shopping. The volume of global online commerce has significantly been increasing partly due to the recent COVID-19 crisis that has accelerated the expansion of e-commerce. A growing number of webshops integrate Artificial Intelligence (AI), state-of-the-art technology into their stores to improve customer experience, satisfaction and loyalty. However, little research has been done to verify the process of how consumers adopt and use AI-powered webshops. Using the technology acceptance model (TAM) as a theoretical background, this study addresses the question of trust and consumer acceptance of Artificial Intelligence in online retail. An online survey in Hungary was conducted to build a database of 439 respondents for this study. To analyse data, structural equation modelling (SEM) was used. After the respecification of the initial theoretical model, a nested model, which was also based on TAM, was developed and tested. The widely used TAM was found to be a suitable theoretical model for investigating consumer acceptance of the use of Artificial Intelligence in online shopping. Trust was found to be one of the key factors influencing consumer attitudes towards Artificial Intelligence. Perceived usefulness as the other key factor in attitudes and behavioural intention was found to be more important than the perceived ease of use. These findings offer valuable implications for webshop owners to increase customer acceptance.","https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1917211&site=ehost-live&authtype=ip,sso&custid=s4392798 http://www.amfiteatrueconomic.ro/Arhiva_EN.aspx",1,1,1,e-commerce,NA,NA,NA,Szabolcs Nagy and Noemi Hajdu,NA,15829146,56,Amfiteatru Economic,"Consumer Economics: Empirical Analysis D12,IT Management M15,Marketing M31,Retail and Wholesale Trade; e-Commerce L81",2,"Accession Number: 1917211 Alternate Accession Number: EP148509339; Keywords: Consumer;  E-Commerce;  Retail; Geographic Descriptors: Hungary; Geographic Region: Europe; Publication Type: Journal Article; Update Code: 20210826; Copyright: Copyright of Amfiteatru Economic is the property of Academia de Studii Economice and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.",155-173,NA,NA,23,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1917211&site=ehost-live&authtype=ip,sso&custid=s4392798 http://www.amfiteatrueconomic.ro/Arhiva_EN.aspx",2021,NA,NA
ID-1374,article,Decision-Making in the Era of AI Support--How Decision Environment and Individual Decision Preferences Affect Advice-Taking in Forecasts,"Decision makers often receive advice from different forecast sources (e.g., human or machine), which they can integrate into their own assessments. Previous studies led to different results regarding the use of forecasts depending on their source. With the ongoing digitalization, it can be assumed, first, that forecasts will be used more frequently in general and, second, that forecasts will increasingly be created digitally or in an automated manner. To investigate these factors in more detail, a vignette experiment was conducted with over 600 participants. In particular, we focused on the decision preference of the forecast users to take a deeper look at the fit of a forecast source, decision preference, and advice-taking. The results show that both a longer time horizon as well as greater trust in advice increase the acceptance of advice. With regard to the forecast source, we find that in a long-term scenario the advice by artificial intelligence is preferred over a human advice. Concerning the relationship of trust and decision preference on the acceptance of advice, our results show that the impact of the forecast source on trust in advice is moderated by trust in digital technologies and that the moderation by trust in digital technologies is itself moderated by decision preference, which highlights the relevance of a fit between the forecast source and an individual's decision preference.",https://psycnet-apa-org.stanford.idm.oclc.org/fulltext/2023-51713-001.pdf,1,1,1,NA,NA,NA,"time horizon as a factor. How long- or short-term is the decision? seems to make a broad distinction between human and nonhuman (which includes algorithm, AI)",Bernadette Mayer and Florian Fuchs and Volker Lingnau,NA,1937321X,1,"Journal of Neuroscience, Psychology, and Economics","Consumer Economics: Empirical Analysis D12,Neural Networks and Related Topics C45,Neuroeconomics D87",3,Accession Number: 2051753; Keywords: Individual;  Preference; Publication Type: Journal Article; Update Code: 20230713,1-11,NA,NA,16,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2051753&site=ehost-live&authtype=ip,sso&custid=s4392798",2023,NA,NA
ID-1380,article,Ethical Perceptions of AI in Hiring and Organizational Trust: The Role of Performance Expectancy and Social Influence,"The use of artificial intelligence (AI) in hiring entails vast ethical challenges. As such, using an ethical lens to study this phenomenon is to better understand whether and how AI matters in hiring. In this paper, we examine whether ethical perceptions of using AI in the hiring process influence individuals' trust in the organizations that use it. Building on the organizational trust model and the unified theory of acceptance and use of technology, we explore whether ethical perceptions are shaped by individual differences in performance expectancy and social influence and how they, in turn, impact organizational trust. We collected primary data from over 300 individuals who were either active job seekers or who had recent hiring experience to capture perceptions across the full range of hiring methods. Our findings indicate that performance expectancy, but not social influence, impacts the ethical perceptions of AI in hiring, which in turn influence organizational trust. Additional analyses indicate that these findings vary depending on the type of hiring methods AI is used for, as well as on whether participants are job seekers or individuals with hiring experience. Our study offers theoretical and practical implications for ethics in HRM and informs policy implementation about when and how to use AI in hiring methods, especially as it pertains to acting ethically and trustworthily.",https://web.p.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=1&sid=96111349-06c8-4403-8fe9-88e7341bc863%40redis,1,1,1,hiring,NA,NA,NA,Maria Figueroa-Armijos and Brent B Clark and Serge P da Motta Veiga,10.1007/s10551-022-05166-2,1674544,1,Journal of Business Ethics,"Corporate Culture; Diversity; Social Responsibility M14,Labor Turnover; Vacancies; Layoffs J63,Personnel Economics: Labor Management M54,Personnel Management; Executives; Executive Compensation M12",8,Accession Number: 2056310; Keywords: Ethical;  Ethics;  Hiring; Publication Type: Journal Article; Update Code: 20230810,179-197,NA,NA,186,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2056310&site=ehost-live&authtype=ip,sso&custid=s4392798",2023,NA,NA
ID-1383,article,Expectation Management in AI Implementation Projects: A Case Study,"Purpose: The aim of this case study is to exemplify the application of a change story to facilitate the user centered introduction of an AI-based assistance system. Thereby, user expectations considered critical for technology acceptance and continuance intention are actively taken into account.Design/methodology/approach: Semi-structured interviews are conducted with future users of the AI-based assistance system. Data are analysed by means of inductive and deductive qualitative content analysis. The resulting categories are considered as communicational core messages and included in the developed change story. Findings: Paradox user expectations were revealed and answered in the change story by informational and motivational means. Thus, accurate expectation management is enabled and, additionally, the users are prepared for the upcoming change process, i.e., the implementation of the AI-based assistance system.",https://d1wqtxts1xzle7.cloudfront.net/112079773/Expectation_Mgt_AI_EMJB_2022-libre.pdf?1709553997=&response-content-disposition=inline%3B+filename%3DExpectation_management_in_AI_implementat.pdf&Expires=1723493013&Signature=ekXHRYhiFJ0hXq1Pe3U4fPP65YI6OGp7HdmSWjdIPvSLouAo61dH2xN~QKueFGBEODI~dxuolI4YWaN3bFaT4p3rUB1FbMCH7vr0YeTk8PUHSMDE2A4g2kjLcxMuR4Y8LshTOdMYfp-kRUr2ukfAFC7rYG3meRNM~WqWhoFOwnKRmUIjCu1LNfGffT0JwknIH7ed9b6XQnt21qZNa-mIYyH~D9btZyo7YuyoNU7cHhIo6K7B5BjER9Yo8zQ45-TCVAM2w78QXVVZhvXOVDIjghqEqYu9jvJwUXledtTR4rIZULvQeJrM-ev8YgrV46QcgXp2kqwJF-odPIeLkKIE7Q__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA,1,2,2,NA,NA,NA,NA,Katharina Buschmeyer and Sarah Hatfield and Ina Heine and Svenja Jahn and Antonia Lea Markus,NA,14502194,3,EuroMed Journal of Business,"Information and Internet Services; Computer Software L86,Neural Networks and Related Topics C45",NA,Accession Number: 2085396; Publication Type: Journal Article; Update Code: 20240215,441-451,NA,NA,18,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2085396&site=ehost-live&authtype=ip,sso&custid=s4392798",2023,NA,NA
ID-1386,article,Factors Affecting the Adoption of Artificial Intelligence in Healthcare,"This paper investigates how clinicians perceive the usefulness and the ease of use of Artificial Intelligence (AI) in healthcare. The paper aims to understand whether AI solutions are perceived to have a positive impact on patient care and the clinician' work, and which factors affects the adoption of AI in healthcare. The paper draws upon key concepts of TAM (Technology Acceptance Model), adopting an exploratory approach. Semi-structured interviews with 22 clinicians from the NHS (the National Health System, in the United Kingdom) reveal that they perceive the usefulness of AI for healthcare (better efficiency, healthcare quality, and diagnostic accuracy). However, respondents point out factors which affect the way they perceive the ease of use of AI, such as the difficulty to integrate the technology within healthcare systems (low compatibility) and to understand the technology (high complexity), concerns with ethical issues, and the need to have intensive training on digital skills.",https://www.theibfr2.com/RePEc/ibf/gjbres/gjbr-v15n1-2021/GJBR-V15N1-2021-4.pdf,1,2,2,healthcare,NA,NA,"compatibility with existing systems, understanding the technology",Magda Hercheui and Gianluca Mech,NA,19310277,1,Global Journal of Business Research,"Analysis of Health Care Markets I11,Health: Government Policy; Regulation; Public Health I18,IT Management M15,National Government Expenditures and Health H51,Neural Networks and Related Topics C45",NA,Accession Number: 2080718; Keywords: Health;  Health Care;  Healthcare; Geographic Descriptors: U.K.; Geographic Region: Europe; Publication Type: Journal Article; Update Code: 20240118,77-88,NA,NA,15,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2080718&site=ehost-live&authtype=ip,sso&custid=s4392798 http://www.theibfr.com/gjbrsample.htm",2021,NA,NA
ID-1390,article,How Trust Can Drive Forward the User Acceptance to the Technology? In-Vehicle Technology for Autonomous Vehicle,"The autonomous vehicle innovation has a potential effect on traffic and it's a widespread issue in the academic municipal. Based on its feature, autonomous vehicle is expected to improve traffic flow, reduce accidents, reduce social exclusion and improve the utility of time on travel. It is increasingly accepted that the next step to the evolution of human transportation is the replacement of human as the driver by the artificial-intelligence-capable machine. Regardless, the challenges remain especially in convincing the consumers to switch to autonomous cars despite the apparent benefits. In the past, papers highlighted individual trust toward machine as the key point to encourage acceptance of autonomous cars. This paper seeks to highlight the ethical implications of the autonomous technology and how it is related to the user acceptance of the technology. This paper reviewed recent studies on the ethical implications of the autonomous vehicle and the user acceptance of the technology. A systematic review of papers from the literature was done using Scopus, ISI and Web of Science. The results of the review have shown that the level of trust, which may vary on the sociodemographic profile of the users, has been studied as one of the factors for user acceptance. Many researchers also highlight ethical implication as an important aspect of autonomous technology that needs to be tended to. The researcher proposes that ethical implication can be an important element of user trust toward autonomous technology. Hence, there is a need to embed ethical implication as a construct for user trust leading to user acceptance model in future studies. In the line of this research review researcher was addressed the following comprehensive groups: examination of autonomous vehicle topographies and flexibility prototypes, consequences for street traffic and connectivity to substructure (specifically in the urban areas with the medium and low density), communal attitudes and apprehensions, transportable behaviour and plea, possible business models, and strategy implications. Lastly, the main objective of this research article is to recognise critical problems for the growth of an effort cluster investigation to comprehend attitudes of possible manipulators of AVs and to highpoint autonomous vehicle growth issues for Malaysian lawful agencies.","https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1743791&site=ehost-live&authtype=ip,sso&custid=s4392798",1,3,1,self-driving vehicles,NA,NA,NA,Nadia Adnan and Shahrina Md Nordin and Mohamad Ariff bin Bahruddin and Murad Ali,10.1016/j.tra.2018.10.019,9658564,NA,Transportation Research: Part A: Policy and Practice,"Automobiles; Other Transportation Equipment; Related Parts and Equipment L62,Economic Development: Urban, Rural, Regional, and Transportation Analysis; Housing; Infrastructure O18,Innovation and Invention: Processes and Incentives O31,Transportation Economics: Government Pricing and Policy R48,Transportation: Demand, Supply, and Congestion; Travel Time; Safety and Accidents; Transportation Noise R41",12,Accession Number: 1743791; Keywords: Accident;  Car;  Innovation;  Science;  Technology;  Traffic;  Transportation;  Urban;  Vehicles; Geographic Descriptors: Malaysia; Geographic Region: Asia; Publication Type: Journal Article; Update Code: 20181213,819-836,NA,NA,118,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1743791&site=ehost-live&authtype=ip,sso&custid=s4392798",2018,NA,NA
ID-1391,article,Human Capital Investment and Perceived Automation Risks: Evidence from 16 Countries,"Robotics and artificial intelligence are transforming jobs and career paths, and equipping workers with the ability to gain new skills has become a strategic policy imperative. Previous studies showed that investment in human capital depends on locus of control, risk preferences and impatience. Yet in a changing world of work, the perceived risk of being replaced by a machine or an algorithm might also affect workers' decisions to gain new professional skills. Using novel survey data from representative samples of working individuals in 16 countries, this paper shows that fear of automation is positively associated with workers' intentions to invest in training activities outside their workplace. This effect is robust to controlling for other known behavioural traits. We also show that fear of automation reinforces the effect that internal locus of control exerts on retraining intentions.",https://www.sciencedirect.com/science/article/pii/S0167268121005370,1,1,1,NA,NA,NA,"attitudes toward AI in the workplace. based on large survey from 2019 assessing people's work situation, intention to retrain, perceptions of tech, locus of control. fear of automation positively associated with intention to retrain (gain new professional skills in free time)",Stefania Innocenti and Marta Golin,10.1016/j.jebo.2021.12.027,1672681,NA,Journal of Economic Behavior and Organization,"Human Capital; Skills; Occupational Choice; Labor Productivity J24,Optimization Techniques; Programming Models; Dynamic Analysis C61,Technological Change: Choices and Consequences; Diffusion Processes O33",3,Accession Number: 1955287; Keywords: Automation;  Human Capital;  Retraining;  Robotics;  Skill;  Training; Geographic Descriptors: Selected Countries; Publication Type: Journal Article; Update Code: 20220407,27-41,NA,NA,195,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1955287&site=ehost-live&authtype=ip,sso&custid=s4392798",2022,NA,NA
ID-1399,article,Is Society Ready for AI Ethical Decision Making? Lessons from a Study on Autonomous Cars,"We conduct two separate experiments to study the social acceptance of AI ethical decision making. In the first experiment, we test whether there is an ""unfounded"" fear of technology. We contrast two methods to measure this fear: an indirect method that measures preferences implicitly and a direct method that measures preferences explicitly. Direct questions show that humans have an aversion to AI; however, indirect questions show that humans are not averse to the implementation of new technologies. We provide a theory to identify the cause of this discrepancy: in addition to their own preferences, subjects largely weight social preferences in direct questions. In the second experiment, we study how humans react to different ways of introducing this new technology to society and find that part of the fear of AI may be related to trust in one's government. Our results show that although individuals do not have a bias against AI, its explicit discussion may generate antagonism.",https://www.sciencedirect.com/science/article/pii/S2214804322000556?casa_token=vqVoQHlhHMwAAAAA:2EBX4HA8N7NAUhVvU6TDNWoIfqKeBu8y8zjo3OxPpTxJM-UYOw5kdKdqZGTrMEvmigTmiV3nLA,1,1,1,moral,NA,NA,What explains the discrepancy between self-reported negative attitudes toward AI and openness to new technologies? People largely weight social preferences in direct questions. Explicitly talking about AI may create antagonism,Johann Caro-Burnett and Shinji Kaneko,10.1016/j.socec.2022.101881,22148043,NA,Journal of Behavioral and Experimental Economics,"Consumer Economics: Empirical Analysis D12,Design of Experiments: Laboratory, Individual C91,Innovation and Invention: Processes and Incentives O31,Micro-Based Behavioral Economics: Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making D91",6,Accession Number: 1986404; Keywords: Experiment;  Experiments;  Individual;  Preference;  Technologies;  Technology; Publication Type: Journal Article; Update Code: 20220825,NA,NA,NA,98,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1986404&site=ehost-live&authtype=ip,sso&custid=s4392798",2022,NA,NA
ID-1400,article,Machine Talk: How Verbal Embodiment in Conversational AI Shapes Consumer-Brand Relationships,"This research shows that AI-based conversational interfaces can have a profound impact on consumer-brand relationships. We develop a conceptual model of verbal embodiment in technology-mediated communication that integrates three key properties of human-to-human dialogue--(1) turn-taking (i.e., alternating contributions by the two parties), (2) turn initiation (i.e., the act of initiating the next turn in a sequence), and (3) grounding between turns (i.e., acknowledging the other party's contribution by restating or rephrasing it). These fundamental conversational properties systematically shape consumers' perception of an AI-based conversational interface, their perception of the brand that the interface represents, and their behavior in connection with that brand. Converging evidence from four studies shows that these dialogue properties enhance the perceived humanness of the interface, which in turn promotes more intimate consumer-brand relationships and more favorable behavioral brand outcomes (greater recommendation acceptance, willingness to pay a price premium, brand advocacy, and brand loyalty). Moreover, we show that these effects are reduced in contexts requiring less mutual understanding between the consumer and the brand. This research highlights how fundamental principles of human-to-human communication can be harnessed to design more intimate consumer-brand interactions in an increasingly AI-driven marketplace.",https://academic.oup.com/jcr/article/50/4/742/7067749,1,1,1,e-commerce,NA,NA,"enhancing a chatbot's perceived humanness by considering communication principles: turn-taking, initiation, acknowledgement",Anouk S Bergner and Christian Hildebrand and Gerald Haubl,NA,935301,4,Journal of Consumer Research,"Advertising M37,Consumer Economics: Empirical Analysis D12,Marketing M31,Micro-Based Behavioral Economics: Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making D91,Neural Networks and Related Topics C45",12,Accession Number: 2094311; Keywords: Brand;  Consumer; Publication Type: Journal Article; Update Code: 20240411,742-764,NA,NA,50,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2094311&site=ehost-live&authtype=ip,sso&custid=s4392798 https://academic.oup.com/jcr/issue",2023,NA,NA
ID-1401,article,Managing Consumers' Adoption of Artificial Intelligence-Based Financial Robo-advisory Services: A Moderated Mediation Model,"Introduction/Main Objectives: This study investigates the determinants of willingness to use financial robo-advisory services. The study aims to identify the intertwined roles of perceived value, perceived risk, and perceived financial knowledge in consumers' acceptance of financial robo-advisory services. Background Problem: Fintech and AI-based applications have opened up new prospects for financial management, but studies into the adoption and implementation of robo-advisors are limited and scant. Novelty: The study offers novel insights by exploring the direct and indirect effects of perceived value and risk on consumer decisions around adopting robo-advisory services. The study also identifies other major drivers of robo-advisory service adoption and formulates a comprehensive model. Research Methods: A quantitative method using a deductive approach was applied, with PLS-SEM performed on a sample of 285 respondents from Bangladesh. The sample was gathered using a purposive sampling method. Findings/Results: Findings revealed that while relative advantage and perceived innovativeness positively affected perceived value and adoption intention, complexity negatively impacted perceived value and adoption intention. The findings also highlighted that attitude had a negative effect on perceived risk and intention to adopt robo-advisory services. The mediating impact of perceived value and risk in predicting the relationship between relative advantage, attitude and behavioral intention to adopt robo-advisory services was also identified. Moreover, the study revealed that perceived financial knowledge moderated the relationship between perceived value and behavioral intention. Conclusion: This study contributes to the existing body of literature by showing the intertwined roles of perceived value, perceived risk, and perceived financial knowledge in consumer acceptance of robo-advisory services. The study provides meaningful insights for financial institutions, and policymakers seeking to make robo-advisory services more reliable and acceptable to consumers through innovative service design and positioning.","https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2091136&site=ehost-live&authtype=ip,sso&custid=s4392798 https://journal.ugm.ac.id/v3/jieb/issue/archive",1,1,1,NA,NA,NA,NA,Dewan Mehrab Ashrafi,NA,20858272,3,Journal of Indonesian Economy and Business,"Banks; Depository Institutions; Micro Finance Institutions; Mortgages G21,Economic Development: Financial Markets; Saving and Capital Investment; Corporate Finance and Governance O16,Equities; Fixed Income Securities G12,IT Management M15,Industrialization; Manufacturing and Service Industries; Choice of Technology O14,Technological Change: Choices and Consequences; Diffusion Processes O33",9,Accession Number: 2091136; Keywords: Financial Institution;  Management;  Services; Geographic Descriptors: Bangladesh; Geographic Region: Asia; Publication Type: Journal Article; Update Code: 20240321,270-301,NA,NA,38,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2091136&site=ehost-live&authtype=ip,sso&custid=s4392798 https://journal.ugm.ac.id/v3/jieb/issue/archive",2023,NA,NA
ID-1402,article,Mapping the Conceptual Structure of Innovation in Artificial Intelligence Research: A Bibliometric Analysis and Systematic Literature Review,"This study uses bibliometric analysis and a systematic literature review to map the conceptual structure of artificial intelligence innovations (AI-I) in the social sciences between 2000 and 2023. It explicitly focuses on non-economic aspects conducive to AI-I, namely social, technological, cultural, sustainable, personal, moral, and ethical. Our analysis reveals that 1225 articles and proceeding papers have been published, and terms such as ""technology,"" ""big data,"" ""management,"" ""performance,"" ""future,"" and ""impact"" are the most frequently used when discussing innovation and AI. According to our time-zone analysis, the last two years have shown a significant emphasis on concepts such as ""transformation,"" ""corporate social responsibility,"" and ""resource-based view."" In terms of citations, the countries that receive the highest number of references in the AI-I field are the United Kingdom, the United States, Germany, Australia, and China. The most prolific authors in terms of publications are David Teece, Erik Brynjolfsson, and Anjan Chatterjee. Given that most studies highlight the economic side of AI-I, we selected the most prolific 163 articles from all social science research areas. These studies legitimize the main non-economic aspects that highlight both certainties and uncertainties conducive to such innovations. Although the technological component is the most popular in our analysis of the non-economic aspects of the AI-I subfield, we find an important emphasis on ethical/moral dimensions conducive to slow innovation principles. We also observe a growing interest in the cultural dimension, specifically exploring potential factors that can lead to better human acceptance of these innovations.",https://www.sciencedirect.com/science/article/pii/S2444569X24000052,1,3,2,NA,NA,NA,"addresses aspects that are often neglected outside the business economics field, such as personal, cultural, moral, and ethical aspects. not on algorithm aversion per se, but tracks how discussion of AI-innovation has evolved since 2000",Dragos M Obreja and Razvan Rughinis and Daniel Rosner,10.1016/j.jik.2024.100465,2444569X,1,Journal of Innovation and Knowledge,"Corporate Culture; Diversity; Social Responsibility M14,Industrialization; Manufacturing and Service Industries; Choice of Technology O14,Innovation and Invention: Processes and Incentives O31,Socialist and Transitional Economies: Factor and Product Markets; Industry Studies; Population P23,Technological Change: Choices and Consequences; Diffusion Processes O33",1,Accession Number: 2105019; Keywords: Ethical;  Innovation;  Management;  Morals;  Science;  Social Responsibility;  Technology; Geographic Descriptors: Australia; China; Germany; U.K.; U.S.; Geographic Region: Asia; Europe; Northern America; Oceania; Publication Type: Journal Article; Update Code: 20240613,NA,NA,NA,9,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2105019&site=ehost-live&authtype=ip,sso&custid=s4392798",2024,NA,NA
ID-1411,article,Prioritisation of Factors for Artificial Intelligence-Based Technology Adoption by Banking Customers in India: Evidence Using the DEMATEL Approach,"Artificial Intelligence (AI) is a concept of recent origin and is accepted for banking activities such as customer service, detection of fraudulent activities, and suspicious transactions. For the successful implementation of AI in the Indian context, a deep understanding is required in terms of its need and importance compared to the traditional banking system. To date, this outlook of AI has been less focused by industry practitioners and experts for the smooth flow of operational procedures in banks for developing countries, for example, India. This study aims to unearth factors and establish a relationship among the identified factors through the decision-making trial and evaluation laboratory (DEMATEL) approach to categorize the factors and frame the cause-and-effect relationships. Fifteen factors are identified through a literature review of existing studies, and ten experts were solicited to express their outlook on this subject. The result indicated that 'Transparency of information,' 'Perceived security of AI-based technology,' 'Social influence on customer,' 'Government regulation of AI in banks,' 'Awareness level of AI,' 'Efficiency of AI system,' 'Technical requirement,' and 'Cost of AI-based technology' were causative factors that support customer acceptance and penetration of AI in banks. The study presents a unique approach to customer acceptability towards AI in banks in developing countries using the DEMATEL technique. This study also discusses the possible area for the adoption of AI in Indian banks. The findings will support policymakers and practitioners in executing AI-based technologies in the banking sector in emerging nations.","https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2088465&site=ehost-live&authtype=ip,sso&custid=s4392798",1,3,2,NA,NA,NA,NA,Swaraj S Bharti and Kanika Prasad and Shwati Sudha and Vineeta Kumari,NA,22535799,2,Applied Finance Letters,"Banks; Depository Institutions; Micro Finance Institutions; Mortgages G21,Economic Development: Financial Markets; Saving and Capital Investment; Corporate Finance and Governance O16,Management of Technological Innovation and R&D O32,Neural Networks and Related Topics C45,Search; Learning; Information and Knowledge; Communication; Belief; Unawareness D83",NA,Accession Number: 2088465; Keywords: Bank;  Banking;  Developing Countries;  Information;  Technical;  Technologies;  Technology; Geographic Descriptors: India; Geographic Region: Asia; Publication Type: Journal Article; Update Code: 20240307,2-22,NA,NA,12,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=2088465&site=ehost-live&authtype=ip,sso&custid=s4392798",2023,NA,NA
ID-1417,article,Resistance to Medical Artificial Intelligence,"Artificial intelligence (AI) is revolutionizing healthcare, but little is known about consumer receptivity to AI in medicine. Consumers are reluctant to utilize healthcare provided by AI in real and hypothetical choices, separate and joint evaluations. Consumers are less likely to utilize healthcare (study 1), exhibit lower reservation prices for healthcare (study 2), are less sensitive to differences in provider performance (studies 3A-3C), and derive negative utility if a provider is automated rather than human (study 4). Uniqueness neglect, a concern that AI providers are less able than human providers to account for consumers' unique characteristics and circumstances, drives consumer resistance to medical AI. Indeed, resistance to medical AI is stronger for consumers who perceive themselves to be more unique (study 5). Uniqueness neglect mediates resistance to medical AI (study 6), and is eliminated when AI provides care (a) that is framed as personalized (study 7), (b) to consumers other than the self (study 8), or (c) that only supports, rather than replaces, a decision made by a human healthcare provider (study 9). These findings make contributions to the psychology of automation and medical decision making, and suggest interventions to increase consumer acceptance of AI in medicine.",https://academic.oup.com/jcr/article/46/4/629/5485292?login=true,1,1,1,"medical, healthcare",NA,NA,uniqueness neglect,Chiara Longoni and Andrea Bonezzi and Carey K Morewedge,10.1093/jcr/ucz013,935301,4,Journal of Consumer Research,"Analysis of Health Care Markets I11,Neural Networks and Related Topics C45",12,Accession Number: 1808853; Keywords: Health Care;  Healthcare;  Medicine;  Psychology; Publication Type: Journal Article; Update Code: 20200109,629-650,NA,NA,46,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1808853&site=ehost-live&authtype=ip,sso&custid=s4392798 https://academic.oup.com/jcr/issue",2019,NA,NA
ID-1429,article,The Impact of Artificial Intelligence on Consumers' Identity and Human Skills,"The development of artificial intelligence is one of the main paradigms of the contemporary society, which will radically change the existence of individuals and our society and it will have important effects on the economy. The use of artificial intelligence in the daily work of individuals and in the relationship between companies and consumers has a great number of advantages such as the increased efficiency, a high degree of fascination in interaction, but in the same time there are several fears related to its development in the future. Due to its great data storage capacity about the behavior of individuals and the processing speed of this data, there is a risk that the forms of artificial intelligence will become smarter than humans and thus intervene in the decisions made by them. Through the constant use of artificial intelligence, there is a high risk of manipulation of consumers as well as a high degree of dependence on intelligent technologies. This close relationship between the user and artificial intelligence can reduce an individual's cognitive abilities and can affect their thinking, personality and relationships with its social circle. This paper presents a mediation model between the efficiency and fascination with artificial intelligence and the consumers' perception of preserving their self-identity and human skills, having as mediator the influence and model of the social circle. The research results show that a higher degree of efficiency and fascination, as well as a positive influence from the social circle decrease the consumers' perception of reduction of human skills in relation to artificial intelligence. Moreover, the social circle mediates the relationship between efficiency and fascination produced by artificial intelligence and the perception of preserving human abilities.",https://www.econstor.eu/bitstream/10419/281558/1/1793738041.pdf,1,1,1,NA,efficiency,NA,NA,Corina Pelau and Irina Ene and Mihai-Ionut Pop,NA,15829146,56,Amfiteatru Economic,"Consumer Economics: Empirical Analysis D12,IT Management M15,Marketing M31,Retail and Wholesale Trade; e-Commerce L81",2,"Accession Number: 1917204 Alternate Accession Number: EP148509332; Keywords: Consumer;  Individual; Publication Type: Journal Article; Update Code: 20210826; Copyright: Copyright of Amfiteatru Economic is the property of Academia de Studii Economice and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.",33-45,NA,NA,23,"https://search.ebscohost.com/login.aspx?direct=true&db=eoh&AN=1917204&site=ehost-live&authtype=ip,sso&custid=s4392798 http://www.amfiteatrueconomic.ro/Arhiva_EN.aspx",2021,NA,NA
ID-1443,article,People Perceive Algorithmic Assessments as Less Fair and Trustworthy Than Identical Human Assessments,"Algorithmic risk assessments are being deployed in an increasingly broad spectrum of domains including banking, medicine, and law enforcement. However, there is widespread concern about their fairness and trustworthiness, and people are also known to display algorithm aversion, preferring human assessments even when they are quantitatively worse. Thus, how does the framing of who made an assessment affect how people perceive its fairness? We investigate whether individual algorithmic assessments are perceived to be more or less accurate, fair, and interpretable than identical human assessments, and explore how these perceptions change when assessments are obviously biased against a subgroup. To this end, we conducted an online experiment that manipulated how biased risk assessments are in a loan repayment task, and reported the assessments as being made either by a statistical model or a human analyst. We find that predictions made by the model are consistently perceived as less fair and less interpretable than those made by the analyst despite being identical. Furthermore, biased predictive errors were more likely to widen this perception gap, with the algorithm being judged even more harshly for making a biased mistake. Our results illustrate that who makes risk assessments can influence perceptions of how acceptable those assessments are – even if they are identically accurate and identically biased against subgroups. Additional work is needed to determine whether and how decision aids should be presented to stakeholders so that the inherent fairness and interpretability of their recommendations, rather than their framing, determines how they are perceived",https://www.cs.toronto.edu/~ashton/pubs/fairversion-cscw2023.pdf,1,1,1,"loan application, risk assessment",NA,NA,"provides a nice overview of key characteristics of the existing empirical research: who makes predictions vs. what is being predicted; performance vs. actions vs. perceptions; concrete situations vs. generic use with/without information about performance. 2 (agent: human or AI) x 2 (biased against a subgroup or not) design. participants to evaluate whether the predictor was accurate, fair, interpretable, and trustworthy in real life. Participants were also asked at the end of the experiment for their perceptions of the risk assessor after seeing all the loan applicants. Main effect of agent (AI advisors judged more harshly than humans) and interaction between agent and bias, where biased predictive errors widened the perception gap: algorithm judged more harshly for making the same mistake",Lillio Mok and Sasha Nanda and Ashton Anderson,10.1145/3610100,NA,CSCW2,Proc. ACM Hum. -Comput. Interact.,"algorithm aversion, bias, fairness, risk assessment",NA,NA,NA,NA,Association for Computing Machinery,7,https://doi-org.stanford.idm.oclc.org/10.1145/3610100 http://dx.doi.org/10.1145/3610100,2023,"New York, NY, USA",NA
ID-1444,article,Who is the Expert? Reconciling Algorithm Aversion and Algorithm,"The increased use of algorithms to support decision making raises questions about whether people prefer algorithmic or human input when making decisions. Two streams of research on algorithm aversion and algorithm appreciation have yielded contradicting results. Our work attempts to reconcile these contradictory findings by focusing on the framings of humans and algorithms as a mechanism. In three decision making experiments, we created an algorithm appreciation result (Experiment 1) as well as an algorithm aversion result (Experiment 2) by manipulating only the description of the human agent and the algorithmic agent, and we demonstrated how different choices of framings can lead to inconsistent outcomes in previous studies (Experiment 3). We also showed that these results were mediated by the agent's perceived competence, i.e., expert power. The results provide insights into the divergence of the algorithm aversion and algorithm appreciation literature. We hope to shift the attention from these two contradicting phenomena to how we can better design the framing of algorithms. We also call the attention of the community to the theory of power sources, as it is a systemic framework that can open up new possibilities for designing algorithmic decision support systems.
",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3479864,1,1,1,NA,NA,"RQ1. How does the framing of the agents (the human and the algorithm) affect algorithm aversion and algorithm appreciation effects? That is, can we change an algorithm aversion situation into an algorithm appreciation one (or vice versa) by simply changing the framing of the agents? RQ2. Does task type play a role in determining between algorithm aversion and algorithm
appreciation? Also, what is the relationship between task type and framing? Is there any
interaction? 2 (task type: creative vs. analytical) x 2 (agent: human vs. algorithm). DV example: Among 100 people, how many do you think will find this painting creative?",NA,Yoyo Tsung-Yu Hou and Malte F Jung,10.1145/3479864,NA,CSCW2,Proc. ACM Hum. -Comput. Interact.,"algorithm appreciation, algorithm aversion, augmented decision making, decision aids, decision support systems, expert power, human-algorithm interaction",NA,NA,NA,NA,Association for Computing Machinery,5,https://doi-org.stanford.idm.oclc.org/10.1145/3479864 http://dx.doi.org/10.1145/3479864,2021,"New York, NY, USA",NA
ID-1464,inproceedings,Is AI-generated content better? A study based on Ant Forest Game content recommendation,"As we entered the 21st century, with the development of computers and mobile internet, research in Artificial Intelligence has made significant advancements. Artificial Intelligence (AI) is a field of science and engineering focused on enabling computers to perform tasks that typically require human intelligence. It encompasses various subfields such as machine learning, expert systems, natural language processing, and computer vision. In 2022, OpenAI released a new chatbot model named ChatGPT, which can understand human language and generate text like a human. Its robust data capabilities have attracted the attention of experts in various fields. However, the public's perception and attitude towards AI technology and ChatGPT are also crucial. Therefore, in this study, we used an online experiment and employed different groups of ChatGPT-generated content recommendation and real-person-generated content recommendation as experimental manipulation conditions. From the user perception perspective, we aimed to explore the differences between user perceptions of content recommendation provided by Artificial Intelligence (ChatGPT) and those provided by real-person. We also investigated whether the perception of AI-generated (ChatGPT) content recommendation had an impact on users' subsequent intention to support Ant Forest Game. The results show significant differences in perceived content quality between AI-generated (ChatGPT) Ant Forest Game content recommendation and real-person-generated content recommendation. Additionally, in terms of subsequent support intention for Ant Forest Game, there were also significant differences between AI-generated (ChatGPT) Ant Forest Game content recommendation and real-person generated Ant Forest Game content recommendation. Ant Forest Game content recommendation was found to significantly enhance Ant Forest Game support intention. Participants exhibited higher perceived content quality and greater support intention for AI-generated (ChatGPT) Ant Forest Game content recommendation. This study explores the psychological mechanisms involved in human-computer interaction, contributing to research in the field of Artificial Intelligence. Compared to a real human person's recommendation, people tend to prefer ChatGPT's recommendation, showing that people exhibit similar social behaviors and emotional responses when interacting with generative AI (ChatGPT) as they do with real humans. These findings are significant for the development and improvement of Artificial Intelligence.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3675417.3675543,1,1,1,NA,NA,NA,ChatGPT vs. human content recommendation,Yajuan Chen and Shengxiang She and Yan Sun,10.1145/3675417.3675543,NA,NA,NA,NA,NA,NA,749-755,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3675417.3675543,2024,"New York, NY, USA",NA
ID-1466,inproceedings,How Much Decision Power Should (A)I Have?: Investigating Patients’ Preferences Towards AI Autonomy in Healthcare Decision Making,"Despite the growing potential of artificial intelligence (AI) in improving clinical decision making, patients' perspectives on the use of AI for their care decision making are underexplored. In this paper, we investigate patients’ preferences towards the autonomy of AI in assisting healthcare decision making. We conducted interviews and an online survey using an interactive narrative and speculative AI prototypes to elicit participants’ preferred choices of using AI in a pregnancy care context. The analysis of the interviews and in-story responses reveals that patients’ preferences for AI autonomy vary per person and context, and may change over time. This finding suggests the need for involving patients in defining and reassessing the appropriate level of AI assistance for healthcare decision making. Departing from these varied preferences for AI autonomy, we discuss implications for incorporating patient-centeredness in designing AI-powered healthcare decision making.",https://doi-org.stanford.idm.oclc.org/10.1145/3613904.3642883,1,"1, 2",2,healthcare,NA,NA,"varied level of AI autonomy: instrumental tool, co-decision maker, authority. Elicited patients’ preferred choices of using AI in an interactive narrative (fiction-based, speculative design approach). Ps make a series of decisions based on information from an AI or human healthcare professional. Some Ps were interviewed after to explain their decisions. Mentioned that when interacting with a virtual assistant you have to be careful about what words you use so as not to be led down the wrong path. But human can infer what you mean, and you don't have to use exactly the right words",Dajung Kim and Niko Vegt and Valentijn Visch and Marina Bos-De Vos,10.1145/3613904.3642883,NA,NA,NA,NA,NA,NA,NA,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3613904.3642883,2024,"New York, NY, USA",NA
ID-1470,inproceedings,The Role of AI in Peer Support for Young People: A Study of Preferences for Human- and AI-Generated Responses,"Generative Artificial Intelligence (AI) is integrated into everyday technology, including news, education, and social media. AI has further pervaded private conversations as conversational partners, auto-completion, and response suggestions. As social media becomes young people’s main method of peer support exchange, we need to understand when and how AI can facilitate and assist in such exchanges in a beneficial, safe, and socially appropriate way. We asked 622 young people to complete an online survey and evaluate blinded human- and AI-generated responses to help-seeking messages. We found that participants preferred the AI-generated response to situations about relationships, self-expression, and physical health. However, when addressing a sensitive topic, like suicidal thoughts, young people preferred the human response. We also discuss the role of training in online peer support exchange and its implications for supporting young people’s well-being. Disclaimer: This paper includes sensitive topics, including suicide ideation. Reader discretion is advised.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3613904.3642574,1,1,1,NA,NA,NA,NA,Jordyn Young and Laala M Jawara and Diep N Nguyen and Brian Daly and Jina Huh-Yoo and Afsaneh Razi,10.1145/3613904.3642574,NA,NA,NA,"AI-Mediated Communication (AI-MC),Artificial Intelligence (AI),Chatbot,Human-AI Interaction (HAII),LLM,Mental Health,Peer Support,Social Support,Youth",NA,NA,NA,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3613904.3642574,2024,"New York, NY, USA",NA
ID-1472,inproceedings,Nonverbal Human Signals Can Help Autonomous Agents Infer Human Preferences for Their Behavior,"An overarching goal of Artificial Intelligence (AI) is creating autonomous, social agents that help people. Two important challenges, though, are that different people prefer different assistance from agents and that preferences can change over time. Thus, helping behaviors should be tailored to how an individual feels during the interaction. We hypothesize that human nonverbal behavior can give clues about users' preferences for an agent's helping behaviors, augmenting an agent's ability to computationally predict such preferences with machine learning models. To investigate our hypothesis, we collected data from 194 participants via an online survey in which participants were recorded while playing a multiplayer game. We evaluated whether the inclusion of nonverbal human signals, as well as additional context (e.g., via game or personality information), led to improved prediction of user preferences between agent behaviors compared to explicitly provided survey responses. Our results suggest that nonverbal communication – a common type of human implicit feedback – can aid in understanding how people want computational agents to interact with them.",https://par.nsf.gov/servlets/purl/10474196,1,1,1,NA,NA,NA,how can AI leverage human nonverbal behaviors to improve reception by users?,Kate Candon and Jesse Chen and Yoony Kim and Zoe Hsu and Nathan Tsoi and Marynel Vázquez,NA,NA,NA,NA,"human-agent interaction,nonverbal behavior,preference learning",NA,NA,307-316,NA,International Foundation for Autonomous Agents and Multiagent Systems,NA,NA,2023,"Richland, SC",NA
ID-1474,inproceedings,How Does Predictive Information Affect Human Ethical Preferences?,"Artificial intelligence (AI) has been increasingly involved in decision making in high-stakes domains, including loan applications, employment screening, and assistive clinical decision making. Meanwhile, involving AI in these high-stake decisions has created ethical concerns on how to balance different trade-offs to respect human values. One approach for aligning AIs with human values is to elicit human ethical preferences and incorporate this information in the design of computer systems. In this work, we explore how human ethical preferences are impacted by the information shown to humans during elicitation. In particular, we aim to provide a contrast between verifiable information (e.g., patient demographics or blood test results) and predictive information (e.g., the probability of organ transplant success). Using kidney transplant allocation as a case study, we conduct a randomized experiment to elicit human ethical preferences on scarce resource allocation to understand how human ethical preferences are impacted by the verifiable and predictive information. We find that the presence of predictive information significantly changes how humans take into account other verifiable information in their ethical preferences. We also find that the source of the predictive information (e.g., whether the predictions are made by AI or human doctors) plays a key role in how humans incorporate the predictive information into their own ethical judgements.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3514094.3534165,1,1,1,"medical, healthcare",NA,NA,RQ2 is relevant because it manipulates prediction source (AI or human),Saumik Narayanan and Guanghui Yu and Wei Tang and Chien-Ju Ho and Ming Yin,10.1145/3514094.3534165,NA,NA,NA,"ai ethics,ethical preference,preference elicitation",NA,NA,508-517,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3514094.3534165,2022,"New York, NY, USA",NA
ID-1477,inproceedings,”It’s like a puppet master”: User Perceptions of Personal Autonomy when Interacting with Intelligent Technologies,"Applications which use some form of artificial intelligence (AI) have become embedded in our everyday interactions. Very often, AI-based apps are personalized and modelled on users’ needs and preferences. However, such applications of AI tread a delicate balance between enhancing user experience and jeopardizing personal autonomy. Personal autonomy and sense of agency are crucial for human well-being and development. In this paper, we probe this fine balance aiming to capture users’ lived experiences and perceptions of interacting with AI-based apps. We present insights from a phenomenological study (N=15) regarding users’ perception of personal autonomy when interacting with AI in everyday contexts. We found that these experiences are transitory and largely influenced by contextual factors. Users experience a loss of autonomy when their privacy or identity is threatened or when their expectations are broken. To mitigate such loss of autonomy, mechanisms for providing intelligibility and control of AI are desired.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3450613.3456820,1,2,2,NA,NA,NA,NA,Supraja Sankaran and Panos Markopoulos,10.1145/3450613.3456820,NA,NA,NA,"autonomy,human-AI interaction,phenomenology,user experience",NA,NA,108-118,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3450613.3456820,2021,"New York, NY, USA",NA
ID-1502,inproceedings,A Robot Walks into a Bar: Can Language Models Serve as Creativity SupportTools for Comedy? An Evaluation of LLMs’ Humour Alignment with Comedians,"We interviewed twenty professional comedians who perform live shows in front of audiences and who use artificial intelligence in their artistic process as part of 3-hour workshops on “AI x Comedy” conducted at the Edinburgh Festival Fringe in August 2023 and online. The workshop consisted of a comedy writing session with large language models (LLMs), a human-computer interaction questionnaire to assess the Creativity Support Index of AI as a writing tool, and a focus group interrogating the comedians’ motivations for and processes of using AI, as well as their ethical concerns about bias, censorship and copyright. Participants noted that existing moderation strategies used in safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by erasing minority groups and their perspectives, and qualified this as a form of censorship. At the same time, most participants felt the LLMs did not succeed as a creativity support tool, by producing bland and biased comedy tropes, akin to “cruise ship comedy material from the 1950s, but a bit less racist”. Our work extends scholarship about the subtle difference between, one the one hand, harmful speech, and on the other hand, “offensive” language as a practice of resistance, satire and “punching up”. We also interrogate the global value alignment behind such language models, and discuss the importance of community-based value alignment and data ownership to build AI tools that better suit artists’ needs. Warning: this study may contain offensive language and discusses self-harm.",https://doi-org.stanford.idm.oclc.org/10.1145/3630106.3658993,1,2,2,"humor, comedy, writing",NA,NA,hard for AI to be applied in the domain of comedy writing because it is trained not to be offensive (but it takes a little bi tof offensiveness to be funny). Also can't draw upon personal experience because it has no personal experience,Piotr Mirowski and Juliette Love and Kory Mathewson and Shakir Mohamed,10.1145/3630106.3658993,NA,NA,NA,"Censorship,Comedy,Creativity,Large Language Models,Offensive speech,Value Alignment",NA,NA,1622-1636,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3630106.3658993,2024,"New York, NY, USA",NA
ID-1504,inproceedings,Did that AI just Charge me a Fine? Citizens’ Perceptions of AI-based Discretion in Public Administration,"Automation and Artificial Intelligence (AI) are continuously advancing decision-making in public administrations. Research focuses on investigating benefits and challenges of AI agents that make decisions without human-intervention, namely AI-systems that independently exercise administrative discretion. However, little is known about citizens’ perspective of such autonomous systems. To address this gap, we investigated citizens’ fairness and satisfaction perceptions of an AI-based self-service kiosk that performs administrative discretion. Through a controlled Wizard of Oz experiment, we assessed how citizens perceived the kiosk’s decision to either impose or waive a fine. Precisely, we measured how engaging citizens in the decision-making process through a dialogue affect their perceptions. We contribute novel citizen-oriented views on how to apply AI-based discretion in public administration. Our results revealed that while citizens do not generally oppose delegating discretionary power to fully autonomous systems, engaging them in the decision-making process through a dialogue will positively alter their perceptions. Moreover, we provide insights on how other factors (i.e., decision justification and citizens’ self-gain) influence citizens’ perception of AI-based administrative discretion.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3582515.3609518,1,2,2,public administration,NA,NA,NA,Saja Aljuneidi and Wilko Heuten and Markus Tepe and Susanne Boll,10.1145/3582515.3609518,NA,NA,NA,"AI-based decision-making,Administrative Discretion (AD),Artificial Intelligence (AI),Intelligent Self Service Kiosk (ISSK),Satisfaction,Wizard of Oz,fairness",NA,NA,57-67,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3582515.3609518,2023,"New York, NY, USA",NA
ID-1583,inproceedings,Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burdens,"Recently, Artificial Intelligence (AI) has been used to enable efficient decision-making in managerial and organizational contexts, ranging from employment to dismissal. However, to avoid employees’ antipathy toward AI, it is important to understand what aspects of AI employees like and/or dislike. In this paper, we aim to identify how employees perceive current human resource (HR) teams and future algorithmic management. Specifically, we explored what factors negatively influence employees’ perceptions of AI making work performance evaluations. Through in-depth interviews with 21 workers, we found that 1) employees feel six types of burdens (i.e., emotional, mental, bias, manipulation, privacy, and social) toward AI's introduction to human resource management (HRM), and that 2) these burdens could be mitigated by incorporating transparency, interpretability, and human intervention to algorithmic decision-making. Based on our findings, we present design efforts to alleviate employees’ burdens. To leverage AI for HRM in fair and trustworthy ways, we call for the HCI community to design human-AI collaboration systems with various HR stakeholders.",https://doi-org.stanford.idm.oclc.org/10.1145/3411764.3445304,1,2,2,HR,NA,NA,NA,Hyanghee Park and Daehwan Ahn and Kartik Hosanagar and Joonhwan Lee,10.1145/3411764.3445304,NA,NA,NA,"Adoption,Algorithm aversion,Algorithmic fairness,Algorithmic management,Artificial intelligence (AI),Explainable AI (XAI),Fair and responsible AI,Human resource management (HRM),Human-AI collaboration,Interpretability,Transparency,Trust,User burden",NA,NA,NA,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3411764.3445304,2021,"New York, NY, USA",NA
ID-1594,inproceedings,"I create, therefore I agree: Exploring the effect of AI anthropomorphism on human decision-making","Artificial intelligence (AI) has been widely adopted daily, and much academic effort has been directed to investigate the influencing factors of human acceptance and collaboration with it. Anthropomorphism can effectively foster positive evaluations of inanimate objects, and AI is no exception. The current study investigates how naming and personalizing an AI prototype’s appearance affect the human decision in a recipe ingredient recommendation system and finds that people are more likely to accept AI’s recommendation if they could name and customize its appearance than if they did not have the options.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3584931.3606990,1,1,1,recommendation,anthropomorphism,NA,NA,Nanyi Bi and Janet Yi-Ching Huang,10.1145/3584931.3606990,NA,NA,NA,"anthropomorphism,artificial intelligence,decision-making,naming",NA,NA,241-244,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3584931.3606990,2023,"New York, NY, USA",NA
ID-1637,inproceedings,The Nature of Trust in Communication Robots: Through Comparison with Trusts in Other People and AI Systems,"In this study, the nature of human trust in communication robots was experimentally investigated comparing with trusts in other people and artificial intelligence (AI) systems. The results of the experiment showed that trust in robots is basically similar to that in AI systems in a calculation task where a single solution can be obtained and is partly similar to that in other people in an emotion recognition task where multiple interpretations can be acceptable. This study will contribute to designing a smooth interaction between people and communication robots.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9889521&casa_token=2gpSibkvyOYAAAAA:kIjuE3Gbg-KXWlIhoQu4bSkM7RBQBz_o6Xs2WCjQ34iAr8m-wMHyvIIAnz6sfXeAkqC2BhSE&tag=1,1,1,1,NA,NA,NA,comparison of trust in humans vs. robots vs. AI for calculation and emotion recognition tasks.,Akihiro Maehigashi,NA,NA,NA,NA,"ai system,communication robot,interpersonal trust,trust",NA,NA,900-903,NA,IEEE Press,NA,NA,2022,NA,NA
ID-1649,article,When Biased Humans Meet Debiased AI: A Case Study in College Major Recommendation,"Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g., along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when humans and fair AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using college major recommendation as a case study, we build a fair AI recommender by employing gender debiasing machine learning techniques. Our offline evaluation showed that the debiased recommender makes fairer career recommendations without sacrificing its accuracy in prediction. Nevertheless, an online user study of more than 200 college students revealed that participants on average prefer the original biased system over the debiased system. Specifically, we found that perceived gender disparity is a determining factor for the acceptance of a recommendation. In other words, we cannot fully address the gender bias issue in AI recommendations without addressing the gender bias in humans. We conducted a follow-up survey to gain additional insights into the effectiveness of various design options that can help participants to overcome their own biases. Our results suggest that making fair AI explainable is crucial for increasing its adoption in the real world.",https://dl-acm-org.stanford.idm.oclc.org/doi/10.1145/3611313#sec-8,1,1,1,college major recommendations,NA,NA,NA,Clarice Wang and Kathryn Wang and Andrew Y Bian and Rashidul Islam and Kamrun Naher Keya and James Foulds and Shimei Pan,10.1145/3611313,2160-6455,3,ACM Trans. Interact. Intell. Syst.,"AI,career recommendation,fairness,gender bias,machine learning",9,NA,NA,NA,Association for Computing Machinery,13,https://doi-org.stanford.idm.oclc.org/10.1145/3611313,2023,"New York, NY, USA",NA
ID-1675,inproceedings,News from Generative Artificial Intelligence Is Believed Less,"Artificial Intelligence (AI) can generate text virtually indistinguishable from text written by humans. A key question, then, is whether people believe news headlines generated by AI as much as news headlines generated by humans. AI is viewed as lacking human motives and emotions, suggesting that people might view news written by AI as more accurate. By contrast, two pre-registered experiments on representative U.S. samples (N = 4,034) showed that people rated news headlines written by AI as less accurate than those written by humans. People were more likely to incorrectly rate news headlines written by AI (vs. a human) as inaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they were indeed false. Our findings are important given the increasing adoption of AI in news generation, and the associated ethical and governance pressures to disclose it use and address standards of transparency and accountability.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3531146.3533077,1,1,1,news,NA,NA,NA,Chiara Longoni and Andrey Fradkin and Luca Cian and Gordon Pennycook,10.1145/3531146.3533077,NA,NA,NA,"algorithmic transparency,fairness,generative artificial intelligence,news,news generation",NA,NA,97-106,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3531146.3533077,2022,"New York, NY, USA",NA
ID-1703,inproceedings,Do Humans Prefer Debiased AI Algorithms? A Case Study in Career Recommendation,"Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g. along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when human- fair-AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using college major recommendation as a case study, we build a fair AI recommender by employing gender debiasing machine learning techniques. Our offline evaluation showed that the debiased recommender makes fairer and more accurate college major recommendations. Nevertheless, an online user study of more than 200 college students revealed that participants on average prefer the original biased system over the debiased system. Specifically, we found that the perceived gender disparity associated with a college major is a determining factor for the acceptance of a recommendation. In other words, our results demonstrate we cannot fully address the gender bias issue in AI recommendations without addressing the gender bias in humans. They also highlight the urgent need to extend the current scope of fair AI research from narrowly focusing on debiasing AI algorithms to including new persuasion and bias explanation technologies in order to achieve intended societal impacts.",https://doi-org.stanford.idm.oclc.org/10.1145/3490099.3511108,1,1,1,NA,NA,NA,NA,Clarice Wang and Kathryn Wang and Andrew Bian and Rashidul Islam and Kamrun Naher Keya and James Foulds and Shimei Pan,10.1145/3490099.3511108,NA,NA,NA,NA,NA,NA,134-147,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3490099.3511108,2022,"New York, NY, USA",NA
ID-1773,article,Conceptual Metaphors Impact Perceptions of Human-AI Collaboration,"With the emergence of conversational artificial intelligence (AI) agents, it is important to understand the mechanisms that influence users' experiences of these agents. In this paper, we study one of the most common tools in the designer's toolkit: conceptual metaphors. Metaphors can present an agent as akin to a wry teenager, a toddler, or an experienced butler. How might a choice of metaphor influence our experience of the AI agent? Sampling a set of metaphors along the dimensions of warmth and competence—defined by psychological theories as the primary axes of variation for human social perception—we perform a study (N=260) where we manipulate the metaphor, but not the behavior, of a Wizard-of-Oz conversational agent. Following the experience, participants are surveyed about their intention to use the agent, their desire to cooperate with the agent, and the agent's usability. Contrary to the current tendency of designers to use high competence metaphors to describe AI products, we find that metaphors that signal low competence lead to better evaluations of the agent than metaphors that signal high competence. This effect persists despite both high and low competence agents featuring identical, human-level performance and the wizards being blind to condition. A second study confirms that intention to adopt decreases rapidly as competence projected by the metaphor increases. In a third study, we assess effects of metaphor choices on potential users' desire to try out the system and find that users are drawn to systems that project higher competence and warmth. These results suggest that projecting competence may help attract new users, but those users may discard the agent unless it can quickly correct with a lower competence metaphor. We close with a retrospective analysis that finds similar patterns between metaphors and user attitudes towards past conversational agents such as Xiaoice, Replika, Woebot, Mitsuku, and Tay.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3415234,1,1,1,NA,warmth/competence,NA,"self-reported Likert questions on usability, intention to use, plus LIWC measures of conversational behavior",Pranav Khadpe and Ranjay Krishna and Li Fei-Fei and Jeffrey T Hancock and Michael S Bernstein,10.1145/3415234,NA,CSCW2,Proc. ACM Hum.-Comput. Interact.,"adoption of ai systems,conceptual metaphors,expectation shaping,perception of human-ai collaboration",10,NA,NA,NA,Association for Computing Machinery,4,https://doi-org.stanford.idm.oclc.org/10.1145/3415234,2020,"New York, NY, USA",NA
ID-1809,inproceedings,Capable but Amoral? Comparing AI and Human Expert Collaboration in Ethical Decision Making,"While artificial intelligence (AI) is increasingly applied for decision-making processes, ethical decisions pose challenges for AI applications. Given that humans cannot always agree on the right thing to do, how would ethical decision-making by AI systems be perceived and how would responsibility be ascribed in human-AI collaboration? In this study, we investigate how the expert type (human vs. AI) and level of expert autonomy (adviser vs. decider) influence trust, perceived responsibility, and reliance. We find that participants consider humans to be more morally trustworthy but less capable than their AI equivalent. This shows in participants’ reliance on AI: AI recommendations and decisions are accepted more often than the human expert’s. However, AI team experts are perceived to be less responsible than humans, while programmers and sellers of AI systems are deemed partially responsible instead.",https://doi-org.stanford.idm.oclc.org/10.1145/3491102.3517732,1,1,1,NA,NA,NA,NA,Suzanne Tolmeijer and Markus Christen and Serhiy Kandul and Markus Kneer and Abraham Bernstein,10.1145/3491102.3517732,NA,NA,NA,"Ethical AI,Human-AI Collaboration,Responsibility,Trust",NA,NA,NA,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3491102.3517732,2022,"New York, NY, USA",NA
ID-1818,inproceedings,Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models,"The field of eXplainable artificial intelligence (XAI) has produced a plethora of methods (e.g., saliency-maps) to gain insight into artificial intelligence (AI) models, and has exploded with the rise of deep learning (DL). However, human-participant studies question the efficacy of these methods, particularly when the AI output is wrong. In this study, we collected and analyzed 156 human-generated text and saliency-based explanations collected in a question-answering task (N = 40) and compared them empirically to state-of-the-art XAI explanations (integrated gradients, conservative LRP, and ChatGPT) in a human-participant study (N = 136). Our findings show that participants found human saliency maps to be more helpful in explaining AI answers than machine saliency maps, but performance negatively correlated with trust in the AI model and explanations. This finding hints at the dilemma of AI errors in explanation, where helpful explanations can lead to lower task performance when they support wrong AI predictions.",https://doi-org.stanford.idm.oclc.org/10.1145/3613904.3642934,1,1,1,explainable AI,NA,NA,"RQ3 is most directly relevant. How are human and machine text and saliency-based explanations empirically evaluated in terms of a wide range of factors such as trust, satisfaction, performance, and confidence, when evaluated in a human-participant task including both correct and incorrect answers? DVs: performance (whether they judged AI answer as correct or not), reaction time, subjective measures (fairness, trust, etc). All explanations were presented as AI-generated, but the actual source of the explanations differed. Actual human-generated saliency explanations are still preferred over machine alternatives for their helpfulness and efficiency.",Marvin Pafla and Kate Larson and Mark Hancock,10.1145/3613904.3642934,NA,NA,NA,"Stanford Question Answering Dataset (SQuAD 1.1v),dilemma of AI errors,explainability,explainable artificial intelligence (XAI),explanation confirmation bias,explanation evaluation,human explanation,human-participant study,large language models (LLMs),local explanations,machine explanation,post-hoc explanations,question-answering task,saliency maps,text-explanations",NA,NA,NA,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3613904.3642934,2024,"New York, NY, USA",NA
ID-1842,article,Powered by AI: Examining How AI Descriptions Influence Perceptions of Fertility Tracking Applications,"Recently, there has been a proliferation of personal health applications describing to use Artificial Intelligence (AI) to assist health consumers in making health decisions based on their data and algorithmic outputs. However, it is still unclear how such descriptions influence individuals' perceptions of such apps and their recommendations. We therefore investigate how current AI descriptions influence individuals' attitudes towards algorithmic recommendations in fertility self-tracking through a simulated study using three versions of a fertility app. We found that participants preferred AI descriptions with explanation, which they perceived as more accurate and trustworthy. Nevertheless, they were unwilling to rely on these apps for high-stakes goals because of the potential consequences of a failure. We then discuss the importance of health goals for AI acceptance, how literacy and assumptions influence perceptions of AI descriptions and explanations, and the limitations of transparency in the context of algorithmic decision-making for personal health.",https://doi-org.stanford.idm.oclc.org/10.1145/3631414,1,"1, 2",1,"personal health, fertility",NA,NA,"3 conditions of app descriptions: Base (no mention of AI or how the app makes predictions), AI (mentions AI but doesn't explain how it makes predictions), AI + Explanation. Participants were more willing to download the AI Explanation version than the others and were more willing to use it than the Base version for trying and avoiding conception, suggesting that the presence of explanation influenced their perceptions. Thought the AI predictions were more accurate than the Base predictions. Qualitative responses suggested that Ps perceived the Base descriptions as ""warmer"" and more ""personal,"" while the AI descriptions were perceived as more ""clinical.""",Mayara Costa Figueiredo and Elizabeth Ankrah and Jacquelyn E Powell and Daniel A Epstein and Yunan Chen,10.1145/3631414,NA,4,Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,"Artificial intelligence perceptions,Consumer health technologies,Fertility self-tracking",1,NA,NA,NA,Association for Computing Machinery,7,https://doi-org.stanford.idm.oclc.org/10.1145/3631414,2024,"New York, NY, USA",NA
ID-1847,inproceedings,"From ""AI"" to Probabilistic Automation: How Does Anthropomorphization of Technical Systems Descriptions Influence Trust?","In this paper we investigate how people’s level of trust (as reported through self-assessment) in so-called “AI” (artificial intelligence) is influenced by anthropomorphizing language in system descriptions. Building on prior work, we define four categories of anthropomorphization (1. Properties of a cognizer, 2. Agency, 3. Biological metaphors, and 4. Properties of a communicator). We use a survey-based approach (n=954) to investigate whether participants are likely to trust one of two (fictitious) “AI” systems by randomly assigning people to see either an anthropomorphized or a de-anthropomorphized description of the systems. We find that participants are no more likely to trust anthropomorphized over de-anthropmorphized product descriptions overall. The type of product or system in combination with different anthropomorphic categories appears to exert greater influence on trust than anthropomorphizing language alone, and age is the only demographic factor that significantly correlates with people’s preference for anthropomorphized or de-anthropomorphized descriptions. When elaborating on their choices, participants highlight factors such as lesser of two evils, lower or higher stakes contexts, and human favoritism as driving motivations when choosing between product A and B, irrespective of whether they saw an anthropomorphized or a de-anthropomorphized description of the product. Our results suggest that “anthropomorphism” in “AI” descriptions is an aggregate concept that may influence different groups differently, and provide nuance to the discussion of whether anthropomorphization leads to higher trust and over-reliance by the general public in systems sold as “AI”.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3630106.3659040,1,1,1,NA,NA,NA,Language/metaphors help build mental models of a system before interacting with it. These are especially critical for the average user who lacks the technical familiarity necessary to make their own accurate mental models. Sets expectations for using the system,Nanna Inie and Stefania Druga and Peter Zukerman and Emily M Bender,10.1145/3630106.3659040,NA,NA,NA,"AI,anthropomorphism,probabilistic automation,semantics,trust",NA,NA,2322-2347,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3630106.3659040,2024,"New York, NY, USA",NA
ID-1872,inproceedings,"Ignore, Trust, or Negotiate: Understanding Clinician Acceptance of AI-Based Treatment Recommendations in Health Care","Artificial intelligence (AI) in healthcare has the potential to improve patient outcomes, but clinician acceptance remains a critical barrier. We developed a novel decision support interface that provides interpretable treatment recommendations for sepsis, a life-threatening condition in which decisional uncertainty is common, treatment practices vary widely, and poor outcomes can occur even with optimal decisions. This system formed the basis of a mixed-methods study in which 24 intensive care clinicians made AI-assisted decisions on real patient cases. We found that explanations generally increased confidence in the AI, but concordance with specific recommendations varied beyond the binary acceptance or rejection described in prior work. Although clinicians sometimes ignored or trusted the AI, they also often prioritized aspects of the recommendations to follow, reject, or delay in a process we term “negotiation.” These results reveal novel barriers to adoption of treatment-focused AI tools and suggest ways to better support differing clinician perspectives.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3544548.3581075,1,"1, 2",1,medical,NA,NA,under what conditions do clinicians accept medical recommendations from AI? Manipulate decisional uncertainty,Venkatesh Sivaraman and Leigh A Bukowski and Joel Levin and Jeremy M Kahn and Adam Perer,10.1145/3544548.3581075,NA,NA,NA,"healthcare,human-AI interaction,interpretability,visualization",NA,NA,NA,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3544548.3581075,2023,"New York, NY, USA",NA
ID-1879,article,"I Know This Looks Bad, But I Can Explain: Understanding When AI Should Explain Actions In Human-AI Teams","Explanation of artificial intelligence (AI) decision-making has become an important research area in human–computer interaction (HCI) and computer-supported teamwork research. While plenty of research has investigated AI explanations with an intent to improve AI transparency and human trust in AI, how AI explanations function in teaming environments remains unclear. Given that a major benefit of AI giving explanations is to increase human trust understanding how AI explanations impact human trust is crucial to effective human-AI teamwork. An online experiment was conducted with 156 participants to explore this question by examining how a teammate’s explanations impact the perceived trust of the teammate and the effectiveness of the team and how these impacts vary based on whether the teammate is a human or an AI. This study shows that explanations facilitate trust in AI teammates when explaining why AI disobeyed humans’ orders but hindered trust when explaining why an AI lied to humans. In addition, participants’ personal characteristics (e.g., their gender and the individual’s ethical framework) impacted their perceptions of AI teammates both directly and indirectly in different scenarios. Our study contributes to interactive intelligent systems and HCI by shedding light on how an AI teammate’s actions and corresponding explanations are perceived by humans while identifying factors that impact trust and perceived effectiveness. This work provides an initial understanding of AI explanations in human-AI teams, which can be used for future research to build upon in exploring AI explanation implementation in collaborative environments.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3635474,1,1,1,NA,NA,NA,"""Imagine you are playing a multiplayer online game. You are teaming up with a [human teammate James/AI teammate Aeon] in a capture-the-flag scenario game. Please watch the video below and answer the following questions."" IVs: agent, scenario (lying, disobeying, etc) DVs: self-reported trust, effectiveness, ethical framework",Rui Zhang and Christopher Flathmann and Geoff Musick and Beau Schelble and Nathan J McNeese and Bart Knijnenburg and Wen Duan,10.1145/3635474,2160-6455,1,ACM Trans. Interact. Intell. Syst.,"AI teammates,Explanation,human-AI teaming,trust",2,NA,NA,NA,Association for Computing Machinery,14,https://doi-org.stanford.idm.oclc.org/10.1145/3635474,2024,"New York, NY, USA",NA
ID-1908,inproceedings,The Effects of Warmth and Competence Perceptions on Users' Choice of an AI System,"People increasingly rely on Artificial Intelligence (AI) based systems to aid decision-making in various domains and often face a choice between alternative systems. We explored the effects of users' perception of AI systems' warmth (perceived intent) and competence (perceived ability) on their choices. In a series of studies, we manipulated AI systems' warmth and competence levels. We show that, similar to the judgments of other people, there is often primacy for warmth over competence. Specifically, when faced with a choice between a high-competence system and a high-warmth system, more participants preferred the high-warmth system. Moreover, the precedence of warmth persisted even when the high-warmth system was overtly deficient in its competence compared to an alternative high competence-low warmth system. The current research proposes that it may be vital for AI systems designers to consider and communicate the system's warmth characteristics to its potential users.",https://doi-org.stanford.idm.oclc.org/10.1145/3411764.3446863,1,1,1,NA,NA,NA,people prioritize warmth over competence when choosing preferred AI system?,Zohar Gilad and Ofra Amir and Liat Levontin,10.1145/3411764.3446863,NA,NA,NA,"Artificial intelligence,Competence,Warmth",NA,NA,NA,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3411764.3446863,2021,"New York, NY, USA",NA
ID-1925,inproceedings,Online Dating Meets Artificial Intelligence: How the Perception of Algorithmically Generated Profile Text Impacts Attractiveness and Trust,"Online dating systems are widely used to meet romantic partners, yet people often struggle to write attractive profiles on these applications. Artificial intelligence (AI) has the potential to help online daters by automatically generating profile content, but little research has explored how the use of AI in online dating could affect users’ perceptions of one another. The present study investigated how the perceived involvement of AI influences ratings of attractiveness and trust in online dating. In a between-subjects experiment, participants (N = 48) were presented with the text of 10 dating profiles and were told that the profiles had been written by humans or with the help of AI. We found that the perceived involvement of AI did not have a significant impact on attractiveness, but that it did lead to a significant reduction in trustworthiness of the profile author. We interpret our findings through the lens of social information processing theory, discussing the tradeoffs associated with designing to reveal or hide the use of AI in online dating.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3441000.3441074,1,1,1,dating,NA,NA,AI-MC. Potential dates with bios labeled as AI-generated are perceived as less trustworthy,Yihan Wu and Ryan M Kelly,10.1145/3441000.3441074,NA,NA,NA,"AI,Artificial Intelligence,Dating,Online Dating,Profiles,Trust",NA,NA,444-453,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3441000.3441074,2021,"New York, NY, USA",NA
ID-1982,article,How Time Pressure in Different Phases of Decision-Making Influences Human-AI Collaboration,"Human cognitive and decision-making abilities depreciate under pressure, motivating the emergence of artificial intelligence (AI) systems as decision support tools to assist people in performing tasks under stress. In this work, we study human decision-making behavior and task performance under time pressure—induced from limitedinitial observation time (time to perform the task before providing an initial response without AI input) andfinal decision time (time to weigh an AI's suggestion before reaching a collective human-AI team answer)—for spatial reasoning and count estimation tasks. Our results show that, while the impact of initial observation time on AI-assisted decision-making was dependent on task nature, participants were more likely to follow AI suggestions when they were provided with longer final decision time; moreover, although participants generally tended to adhere to their initial responses, they had more agency when they were more logically engaged in a task. Our results offer a nuanced understanding of human-AI collaboration under time pressure in different phases of the decision-making process.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3610068,1,1,1,NA,NA,NA,NA,Shiye Cao and Catalina Gomez and Chien-Ming Huang,10.1145/3610068,NA,CSCW2,Proc. ACM Hum.-Comput. Interact.,"decision support tools,decision-making,human-AI interaction,time pressure",10,NA,NA,NA,Association for Computing Machinery,7,https://doi-org.stanford.idm.oclc.org/10.1145/3610068,2023,"New York, NY, USA",NA
ID-2008,inproceedings,Who Is Included in Human Perceptions of AI?: Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust,"Emerging research suggests that people trust algorithmic decisions less than human decisions. However, different populations, particularly in marginalized communities, may have different levels of trust in human decision-makers. Do people who mistrust human decision-makers perceive human decisions to be more trustworthy and fairer than algorithmic decisions? Or do they trust algorithmic decisions as much as or more than human decisions? We examine the role of mistrust in human systems in people’s perceptions of algorithmic decisions. We focus on healthcare Artificial Intelligence (AI), group-based medical mistrust, and Black people in the United States. We conducted a between-subjects online experiment to examine people’s perceptions of skin cancer screening decisions made by an AI versus a human physician depending on their medical mistrust, and we conducted interviews to understand how to cultivate trust in healthcare AI. Our findings highlight that research around human experiences of AI should consider critical differences in social groups.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3411764.3445570,1,"1, 2",2,healthcare,NA,NA,"Ps were randomly assigned to read about a medical scenario where either an AI system or a human physician performed cancer screening. Ps answered questions about the scenario and completed a scale about medical mistrust. results suggest that participants who mistrust human medical providers such as doctors and
nurses perceive healthcare AI as equally untrustworthy and as
unfair as human medical providers. Black people (marginalized), on average, show less algorithm aversion than White people. They perceive AI and human doctors as equally unfair",Min Kyung Lee and Katherine Rich,10.1145/3411764.3445570,NA,NA,NA,"Black Perspectives,Fairness,Group-Based Medical Mistrust Scale (GBMMS),Healthcare AI,Perceptions of Algorithmic Decisions,Trust",NA,NA,NA,NA,Association for Computing Machinery,NA,https://doi-org.stanford.idm.oclc.org/10.1145/3411764.3445570,2021,"New York, NY, USA",NA
ID-2038,article,Data Subjects' Conceptualizations of and Attitudes Toward Automatic Emotion Recognition-Enabled Wellbeing Interventions on Social Media,"Automatic emotion recognition (ER)-enabled wellbeing interventions use ER algorithms to infer the emotions of a data subject (i.e., a person about whom data is collected or processed to enable ER) based on data generated from their online interactions, such as social media activity, and intervene accordingly. The potential commercial applications of this technology are widely acknowledged, particularly in the context of social media. Yet, little is known about data subjects' conceptualizations of and attitudes toward automatic ER-enabled wellbeing interventions. To address this gap, we interviewed 13 US adult social media data subjects regarding social media-based automatic ER-enabled wellbeing interventions. We found that participants' attitudes toward automatic ER-enabled wellbeing interventions were predominantly negative. Negative attitudes were largely shaped by how participants compared their conceptualizations of Artificial Intelligence (AI) to the humans that traditionally deliver wellbeing support. Comparisons between AI and human wellbeing interventions were based upon human attributes participants doubted AI could hold: 1) helpfulness and authentic care; 2) personal and professional expertise; 3) morality; and 4) benevolence through shared humanity. In some cases, participants' attitudes toward automatic ER-enabled wellbeing interventions shifted when participants conceptualized automatic ER-enabled wellbeing interventions' impact on others, rather than themselves. Though with reluctance, a minority of participants held more positive attitudes toward their conceptualizations of automatic ER-enabled wellbeing interventions, citing their potential to benefit others: 1) by supporting academic research; 2) by increasing access to wellbeing support; and 3) through egregious harm prevention. However, most participants anticipated harms associated with their conceptualizations of automatic ER-enabled wellbeing interventions for others, such as re-traumatization, the spread of inaccurate health information, inappropriate surveillance, and interventions informed by inaccurate predictions. Lastly, while participants had qualms about automatic ER-enabled wellbeing interventions, we identified three development and delivery qualities of automatic ER-enabled wellbeing interventions upon which their attitudes toward them depended: 1) accuracy; 2) contextual sensitivity; and 3) positive outcome. Our study is not motivated to make normative statements about whether or how automatic ER-enabled wellbeing interventions should exist, but to center voices of the data subjects affected by this technology. We argue for the inclusion of data subjects in the development of requirements for ethical and trustworthy ER applications. To that end, we discuss ethical, social, and policy implications of our findings, suggesting that automatic ER-enabled wellbeing interventions imagined by participants are incompatible with aims to promote trustworthy, socially aware, and responsible AI technologies in the current practical and regulatory landscape in the US.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3476049,1,2,2,emotion recognition,NA,NA,"In interviews, Ps talked about four attributes that would be difficult for AI emotion recognition to emulate: 1) helpfulness and authentic care; 2) personal and professional expertise; 3) morality; and 4) benevolence through shared humanity",Kat Roemmich and Nazanin Andalibi,10.1145/3476049,NA,CSCW2,Proc. ACM Hum.-Comput. Interact.,"AI ethics,affect recognition,affective computing,algorithmic accountability,artificial emotional intelligence,emotion ai,emotion recognition,ethics,fairness,social media,wellbeing interventions",10,NA,NA,NA,Association for Computing Machinery,5,https://doi-org.stanford.idm.oclc.org/10.1145/3476049,2021,"New York, NY, USA",NA
ID-2062,article,Working With AI to Persuade: Examining a Large Language Model's Ability to Generate Pro-Vaccination Messages,"Artificial Intelligence (AI) is a transformative force in communication and messaging strategy, with potential to disrupt traditional approaches. Large language models (LLMs), a form of AI, are capable of generating high-quality, humanlike text. We investigate the persuasive quality of AI-generated messages to understand how AI could impact public health messaging. Specifically, through a series of studies designed to characterize and evaluate generative AI in developing public health messages, we analyze COVID-19 pro-vaccination messages generated by GPT-3, a state-of-the-art instantiation of a large language model. Study 1 is a systematic evaluation of GPT-3's ability to generate pro-vaccination messages. Study 2 then observed peoples' perceptions of curated GPT-3-generated messages compared to human-authored messages released by the CDC (Centers for Disease Control and Prevention), finding that GPT-3 messages were perceived as more effective, stronger arguments, and evoked more positive attitudes than CDC messages. Finally, Study 3 assessed the role of source labels on perceived quality, finding that while participants preferred AI-generated messages, they expressed dispreference for messages that were labeled as AI-generated. The results suggest that, with human supervision, AI can be used to create effective public health messages, but that individuals prefer their public health messages to come from human institutions rather than AI sources. We propose best practices for assessing generative outputs of large language models in future social science research and ways health professionals can use AI systems to augment public health messaging.",https://dl-acm-org.stanford.idm.oclc.org/doi/pdf/10.1145/3579592,1,1,1,public health,NA,NA,"Study 2 compares people's perceptions of public health messages generated by the CDC vs. ChatGPT. Used LIWC to analyze linguistic differences in the outputs.  Unlabeled GPT-3 messages relative to CDC messages were once again perceived as having higher argument strength, as more effective, and as evoking more positive attitudes. But when they were labeled as AI-generated, AI-labeled messages were perceived less positively compared to CDC-labeled messages",Elise Karinshak and Sunny Xun Liu and Joon Sung Park and Jeffrey T Hancock,10.1145/3579592,NA,CSCW1,Proc. ACM Hum.-Comput. Interact.,"AI-mediated communication,large language models,message factors,natural language processing,persuasion,public health messaging",4,NA,NA,NA,Association for Computing Machinery,7,https://doi-org.stanford.idm.oclc.org/10.1145/3579592,2023,"New York, NY, USA",NA
