---
title: "meta-analysis"
output: html_document
date: "2024-08-13"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import libraries

```{r}
library(tidyverse)
library(dplyr)
```

## Import csv of coded search results

```{r}
coded_articles <- read_csv("Inclusion_exclusion coding - mendeley_export.csv")
```

```{r}
# Identify remaining duplicates. These were all coded as irrelevant.

titles_abstracts <- coded_articles %>% 
  select(title, abstract)  
  
duplicates <- titles_abstracts %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Limit to unique entries
# distinct <- coded_articles %>% 
#   distinct(title) %>% 
#   filter()

# Count number of included articles
coded_articles %>% 
  count(`Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons`)

# Filter for relevant articles (1st round of coding). Manually go through and remove any remaining duplicates. (Initially not identified because of slight punctuation and case differences.) Use str_to_title() to convert all titles to title case
relevant_articles <- coded_articles %>% 
  filter(`Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons` == 1) %>% 
  filter(!(ID %in% c("ID-1355", "ID-291", "ID-264", "ID-109", "ID-137", "ID-1295", "ID-1342", "ID-144", "ID-1134", "ID-127", "ID-459", "ID-025", "ID-023", "ID-1272", "ID-1322", "ID-1444"))) %>% 
  mutate(title = str_to_title(title))


write_csv(relevant_articles, "2024_complete_list.csv")

```

```{r}
# Count proportion of each type of method for the relevant articles
relevant_articles %>% 
  count(`Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`) 

# Look at represented domains. It might be more informative to group into fewer buckets. For example, 'healthcare' and 'medical' could be combined into one category. 'Art' and 'visual art' could be combined. 
relevant_articles %>% 
  count(domain) %>% 
  print(n = 90)

# See the range of publication years
relevant_articles %>% 
  summarize(oldest_paper = min(year),
            newest_paper = max(year))

write_csv(relevant_articles, "1st_round_coding.csv")
```

```{r}
# Import spreadsheet of articles from ICA 2023 meta-analysis
old_articles <- read_csv("2nd_round_coding_2023_ICA.csv") %>% 
  rename("title" = "Article name",
         "domain" = "Task Domain",
         "first_author_last_name" = "First Author Last Name",
         "notes" = "Notes")

# See how many unique entries I added. 
new_articles <- anti_join(relevant_articles, old_articles, by = "title")

# Recode methods and domains in new list
# Last line uses string manipulation to only include first author's last name
new_articles_recoded <- new_articles %>% 
  mutate(Method = case_when(`Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "1" ~ "Quantitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "2" ~ "Qualitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "3" ~ "Review",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` %in% c("1, 2", NA) ~ "Other")) %>% 
  relocate(Method, .after = `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`) %>% 
  mutate(first_author_last_name = if_else(str_detect(author, "and") == TRUE, str_extract(author, "\\b(\\w+)\\b(?=\\s+and\\b)"), str_extract(author, "\\b([\\w-]+)\\b(?=[^\\w-]*$)")))

# Check domains from old list
old_articles %>% 
  count(Field)

old_articles %>% 
  filter(Field == "business") %>% 
  print(n = 34)

# Check domains from new list
new_articles_recoded %>% 
  count(domain) %>% 
  print(n = 82)

# Check distribution of publication years
ggplot(new_articles_recoded, aes(x=year)) +
  geom_histogram(bins = 7, color = "white")
  

```

```{r}
# Join old and new list to create 'Update on 2023 ICA 2nd round coding'
merged_list <- bind_rows(old_articles, new_articles_recoded)

# Delete unnecessary columns
merged_list_cleaned <- merged_list %>% 
  select(-c(author, `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`, `Article ID_Old`, `Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons`))


# Export for use in Google Sheets
write_csv(merged_list_cleaned, "update_on_2023_ICA_2nd_round_coding.csv")

```

```{r}
# Create a merged list called '2024 + 2023 first round of coding' with duplicates removed. This will probably be a smaller file

# Import csv
ICA_1st_round_coding <- read_csv("2023_1st_round_coding.csv") %>% 
  rename("title" = "Article Title",
         "first_author_last_name" = "First Author Last Name",
         "Statistics" = "Does this paper have statistical methods",
         "Method" = "Method (1) experiment, (2) survey, (3) review, (4) other") %>% 
  mutate(title = str_to_title(title),
         Year = as.double(Year))

# Multiple rows could refer to the same paper because each study is given its own row. Check to see how many *papers* there are
ICA_1st_round_coding %>% 
  distinct(title)

# There are 224 papers.
```


```{r}
# See how many unique entries I added. Rename columns so they correctly bind with old list.
unique_new_based_on_ICA_1st_round <- anti_join(relevant_articles, ICA_1st_round_coding, by = "title") 

unique_new_recoded <- unique_new_based_on_ICA_1st_round %>% 
  mutate(Method = case_when(`Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "1" ~ "quantitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "2" ~ "qualitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "3" ~ "review",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` %in% c("1, 2", NA) ~ "Other")) %>% 
  rename("Statistics" = `Statistics: (1) yes, (2) no`,
         "Journal" = "journal",
         "Year" = "year") %>% 
  mutate(Statistics = case_when(Statistics == "1" ~ "Yes",
                                Statistics == "2" ~ "No")) %>% 
  relocate(Method, .after = `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`) %>% 
  mutate(first_author_last_name = if_else(str_detect(author, "and") == TRUE, str_extract(author, "\\b(\\w+)\\b(?=\\s+and\\b)"), str_extract(author, "\\b([\\w-]+)\\b(?=[^\\w-]*$)"))) %>% 
  mutate(title = str_to_title(title))

# I added 185 unique articles. Export and get pdfs
write_csv(unique_new_recoded, "algorithm_aversion_papers_8.20.24.csv")
```


```{r}
# Join ICA 1st round coding + unique entries to create '2024 + 2023 first round of coding'
# Delete unnecessary columns

merged_based_on_ICA_1st_round <- bind_rows(ICA_1st_round_coding, unique_new_recoded) %>% 
    select(-c(author, `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`, `Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons`))


# Distribution of publication years
merged_based_on_ICA_1st_round %>% 
  filter(Year > 2014) %>% 
  ggplot(aes(x = Year)) +
  geom_histogram(bins = 7)

# Export
write_csv(merged_based_on_ICA_1st_round, "2024_+_2023_1st_round_coding.csv")

# 185 new + 224 old = 409
```

```{r}
# Import merged document with Sunny's second round of coding

merged_2nd_round_coding <- read_csv("2024_+_2023_2nd_round_coding_fixed_authors.csv")  %>% 
    rename("Sunny_note" = "...16",
         "title" = "title...2",
         "Sunny_coding" = "2024 systematic review inclusion and exclusion") 

```

```{r}
# Remove rows that are coded as irrelevant AND are not useful for the introduction of the paper. 
excluded_after_2nd_round <- merged_2nd_round_coding %>% 
  filter((Sunny_coding == "Exclude" & is.na(Sunny_note))) 

# Also remove duplicates
included_after_2nd_round <- anti_join(merged_2nd_round_coding, excluded_after_2nd_round) %>% 
  filter(Sunny_coding %in% c("Include", "Exclude", NA)) 

removed_duplicates_after_2nd_round <- included_after_2nd_round %>% 
  filter(!(new_list_ID %in% c("ID-293", "ID-1274"))) %>% 
  filter(!(first_author_last_name == "Bogert E, Lauharatanahirun N, Schecter A." & `Field/Discipline` == "Creative")) %>% 
  filter(!(first_author_last_name %in% c("Commerford, B.P.", "Commerford, Benjamin P.; Dennis, Sean A.; Joe, Jennifer R.; Ulla, Jenny W."))) 

# Check how many papers there are for meta-analysis
removed_duplicates_after_2nd_round %>% 
  filter(Sunny_coding %in% c("Include", NA)) %>% 
  distinct(title) 


```

```{r}
# For papers with more than one study, merge rows so that each row represents one paper. Sum sample sizes, combine all domains into one cell, etc. 

sample_sizes <- removed_duplicates_after_2nd_round %>% 
  filter(Sunny_coding %in% c("Include", NA)) %>% 
  mutate(`Sample Size` = if_else(str_detect(`Sample Size`, "[:alpha:]"), NA, `Sample Size`)) %>% 
  mutate(`Sample Size` = as.numeric(`Sample Size`)) %>% 
  group_by(first_author_last_name, title) %>% 
  summarize(sample_size = sum(`Sample Size`))

```


