---
title: "meta-analysis"
output: html_document
date: "2024-08-13"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import libraries

```{r}
library(tidyverse)
library(dplyr)
library(kableExtra)
library(colorspace)
```

## Import csv of coded search results

```{r}
coded_articles <- read_csv("Inclusion_exclusion coding - mendeley_export.csv")
```

```{r}
# Identify remaining duplicates. These were all coded as irrelevant.

titles_abstracts <- coded_articles %>% 
  select(title, abstract)  
  
duplicates <- titles_abstracts %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Limit to unique entries
# distinct <- coded_articles %>% 
#   distinct(title) %>% 
#   filter()

# Count number of included articles
coded_articles %>% 
  count(`Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons`)

# Filter for relevant articles (1st round of coding). Manually go through and remove any remaining duplicates. (Initially not identified because of slight punctuation and case differences.) Use str_to_title() to convert all titles to title case
relevant_articles <- coded_articles %>% 
  filter(`Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons` == 1) %>% 
  filter(!(ID %in% c("ID-1355", "ID-291", "ID-264", "ID-109", "ID-137", "ID-1295", "ID-1342", "ID-144", "ID-1134", "ID-127", "ID-459", "ID-025", "ID-023", "ID-1272", "ID-1322", "ID-1444"))) %>% 
  mutate(title = str_to_title(title))


write_csv(relevant_articles, "2024_complete_list.csv")

```

```{r}
# Count proportion of each type of method for the relevant articles
relevant_articles %>% 
  count(`Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`) 

# Look at represented domains. It might be more informative to group into fewer buckets. For example, 'healthcare' and 'medical' could be combined into one category. 'Art' and 'visual art' could be combined. 
relevant_articles %>% 
  count(domain) %>% 
  print(n = 90)

# See the range of publication years
relevant_articles %>% 
  summarize(oldest_paper = min(year),
            newest_paper = max(year))

write_csv(relevant_articles, "1st_round_coding.csv")
```

```{r}
# Import spreadsheet of articles from ICA 2023 meta-analysis
old_articles <- read_csv("2nd_round_coding_2023_ICA.csv") %>% 
  rename("title" = "Article name",
         "domain" = "Task Domain",
         "first_author_last_name" = "First Author Last Name",
         "notes" = "Notes")

# See how many unique entries I added. 
new_articles <- anti_join(relevant_articles, old_articles, by = "title")

# Recode methods and domains in new list
# Last line uses string manipulation to only include first author's last name
new_articles_recoded <- new_articles %>% 
  mutate(Method = case_when(`Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "1" ~ "Quantitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "2" ~ "Qualitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "3" ~ "Review",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` %in% c("1, 2", NA) ~ "Other")) %>% 
  relocate(Method, .after = `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`) %>% 
  mutate(first_author_last_name = if_else(str_detect(author, "and") == TRUE, str_extract(author, "\\b(\\w+)\\b(?=\\s+and\\b)"), str_extract(author, "\\b([\\w-]+)\\b(?=[^\\w-]*$)"))) %>% 
  mutate(year = as.factor(year))

# Check domains from old list
old_articles %>% 
  count(Field)

old_articles %>% 
  filter(Field == "business") %>% 
  print(n = 34)

# Check domains from new list
new_articles_recoded %>% 
  count(domain) %>% 
  print(n = 82)

# Check distribution of publication years
ggplot(new_articles_recoded, aes(x=year)) +
  geom_histogram(fill = "#f9da78", color = "white", stat = "count") +
  theme(axis.title = element_text(size = 14)) +
  labs(x="Year",
       y="Count")
  

```

```{r}
# Join old and new list to create 'Update on 2023 ICA 2nd round coding'
merged_list <- bind_rows(old_articles, new_articles_recoded)

# Delete unnecessary columns
merged_list_cleaned <- merged_list %>% 
  select(-c(author, `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`, `Article ID_Old`, `Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons`))


# Export for use in Google Sheets
write_csv(merged_list_cleaned, "update_on_2023_ICA_2nd_round_coding.csv")

```

```{r}
# Create a merged list called '2024 + 2023 first round of coding' with duplicates removed. This will probably be a smaller file

# Import csv
ICA_1st_round_coding <- read_csv("2023_1st_round_coding.csv") %>% 
  rename("title" = "Article Title",
         "first_author_last_name" = "First Author Last Name",
         "Statistics" = "Does this paper have statistical methods",
         "Method" = "Method (1) experiment, (2) survey, (3) review, (4) other") %>% 
  mutate(title = str_to_title(title),
         Year = as.factor(Year))

# Multiple rows could refer to the same paper because each study is given its own row. Check to see how many *papers* there are
ICA_1st_round_coding %>% 
  distinct(title)

# There are 224 papers.
```


```{r}
# See how many unique entries I added. Rename columns so they correctly bind with old list.
unique_new_based_on_ICA_1st_round <- anti_join(relevant_articles, ICA_1st_round_coding, by = "title") 

unique_new_recoded <- unique_new_based_on_ICA_1st_round %>% 
  mutate(Method = case_when(`Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "1" ~ "quantitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "2" ~ "qualitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "3" ~ "review",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` %in% c("1, 2", NA) ~ "Other")) %>% 
  rename("Statistics" = `Statistics: (1) yes, (2) no`,
         "Journal" = "journal",
         "Year" = "year") %>% 
  mutate(Statistics = case_when(Statistics == "1" ~ "Yes",
                                Statistics == "2" ~ "No")) %>% 
  relocate(Method, .after = `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`) %>% 
  mutate(first_author_last_name = if_else(str_detect(author, "and") == TRUE, str_extract(author, "\\b(\\w+)\\b(?=\\s+and\\b)"), str_extract(author, "\\b([\\w-]+)\\b(?=[^\\w-]*$)"))) %>% 
  mutate(title = str_to_title(title)) %>% 
  mutate(Year = as.factor(Year))

# I added 185 unique articles. Export and get pdfs
write_csv(unique_new_recoded, "algorithm_aversion_papers_8.20.24.csv")

# Check distribution of publication years
ggplot(unique_new_recoded, aes(x=Year)) +
  geom_histogram(fill = "#f9da78", color = "white", stat = "count") +
  theme(axis.title = element_text(size = 14)) +
  labs(x="Year",
       y="Count")
```


```{r}
# Join ICA 1st round coding + unique entries to create '2024 + 2023 first round of coding'
# Delete unnecessary columns

merged_based_on_ICA_1st_round <- bind_rows(ICA_1st_round_coding, unique_new_recoded) %>% 
    select(-c(author, `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`, `Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons`))


# # Distribution of publication years
# merged_based_on_ICA_1st_round %>% 
#   filter(Year > 2014) %>% 
#   ggplot(aes(x = Year)) +
#   geom_histogram(bins = 7)

# Export
write_csv(merged_based_on_ICA_1st_round, "2024_+_2023_1st_round_coding.csv")

# 185 new + 224 old = 409
```

```{r}
# Import merged document with Sunny's second round of coding

merged_2nd_round_coding <- read_csv("2024_+_2023_2nd_round_coding_fixed_authors.csv")  %>% 
    rename("Sunny_note" = "...16",
         "title" = "title...2",
         "Sunny_coding" = "2024 systematic review inclusion and exclusion") 

```

```{r}
# Remove rows that are coded as irrelevant AND are not useful for the introduction of the paper. 
excluded_after_2nd_round <- merged_2nd_round_coding %>% 
  filter((Sunny_coding == "Exclude" & is.na(Sunny_note))) 

# Also remove duplicates
included_after_2nd_round <- anti_join(merged_2nd_round_coding, excluded_after_2nd_round) %>% 
  filter(Sunny_coding %in% c("Include", "Exclude", NA)) 

removed_duplicates_after_2nd_round <- included_after_2nd_round %>% 
  filter(!(new_list_ID %in% c("ID-293", "ID-1274"))) %>% 
  filter(!(first_author_last_name == "Bogert E, Lauharatanahirun N, Schecter A." & `Field/Discipline` == "Creative")) %>% 
  filter(!(first_author_last_name %in% c("Commerford, B.P.", "Commerford, Benjamin P.; Dennis, Sean A.; Joe, Jennifer R.; Ulla, Jenny W."))) 

# Check how many papers there are for meta-analysis
removed_duplicates_after_2nd_round %>% 
  filter(Sunny_coding %in% c("Include", NA)) %>% 
  distinct(title) 


```

```{r}
# For papers with more than one study, merge rows so that each row represents one paper. Sum sample sizes, combine all domains into one cell, etc. 

# Fill empty cells with publication year and journal using the helpful function fill()

merged_sample_sizes_tasks <- removed_duplicates_after_2nd_round %>% 
  mutate(Journal = str_to_title(Journal)) %>% 
  group_by(first_author_last_name, title) %>% 
  fill(Year, Journal, Sunny_coding) %>% 
  filter(Sunny_coding == "Include") %>% 
  mutate(`Sample Size` = if_else(str_detect(`Sample Size`, "[:alpha:]"), NA, `Sample Size`)) %>% 
  mutate(`Sample Size` = as.numeric(`Sample Size`)) %>%
  unite(col = "Field_Domain", c(`Field/Discipline`, domain), sep = ", ", na.rm = TRUE) %>% 
  unite(col = "country", c(Region, city_country), sep = ";", na.rm = TRUE) %>% 
  group_by(first_author_last_name, title) %>% 
  mutate(all_DVs = paste0(`Outcome variables`, collapse = "; "),
         all_tasks = paste0(`Task type`, collapse = "; ")) %>%
  mutate(all_DVs = str_remove(all_DVs, "NA; "),
         all_tasks = str_remove(all_tasks, "NA; ")) %>% 
  group_by(first_author_last_name, title) %>% 
  mutate(total_sample_size = sum(`Sample Size`)) %>% 
  group_by(first_author_last_name, title) %>% 
  select(-(title...14)) %>% 
  mutate(first_author_last_name = str_extract(first_author_last_name, "[^,]+")) %>% 
  arrange(first_author_last_name) 

one_paper_per_row <- merged_sample_sizes_tasks %>% 
  filter(is.na(study))
# Export to fix manually in Google Sheets

write_csv(one_paper_per_row, "merged_sample_sizes_tasks.csv")


```

```{r}
# Analyze distribution of publication years from working table
working_table <- read_csv("working_table_Sep11.csv") %>% 
  mutate(Year = as.factor(Year)) %>% 
  filter(!is.na(Year)) %>% 
  filter(`Sunny notes` == "Include")

ggplot(working_table, aes(x=Year)) +
  geom_histogram(fill = "#f9da78", color = "white", stat = "count") +
  theme(axis.title = element_text(size = 14)) +
  labs(x="Year",
       y="Count")

```

## Make a nice table that summarizes all the included papers

```{r}
sample_table <- working_table %>% 
  filter(`Sunny notes` == "Include") %>% 
  select(first_author_last_name, title, Year, Journal, Method, total_sample_size, Field_Domain, Algorithm_orientations, task_objectivity) %>% 
  rename("First Author" = first_author_last_name,
         "Title" = title,
         "N" = total_sample_size,
         "Field" = Field_Domain,
         "Algorithm Orientations" = Algorithm_orientations,
         "Task Objectivity" = task_objectivity) %>% 
  mutate(Method = str_to_title(Method)) %>% 
  mutate(Method = if_else(`First Author` == "Aljuneidi", "Interviews", Method)) %>% 
  mutate(N = as.numeric(N))

sample_table %>% 
  kbl() %>% 
  kable_paper("striped")
```

```{r}
ggplot(sample_table, aes(x = Field)) +
  geom_bar() +
  coord_flip()

# Visualize year, author, sample size, and method
ggplot(sample_table, aes(x=`First Author`, y=Year, size=N, color=Method)) +
  geom_point() +
  coord_flip()

# Need to reduce number of fields to make the patterns more clear
ggplot(sample_table, aes(x = Method, fill = Field)) +
  geom_bar()

sample_table %>% 
  mutate(Field = if_else(str_detect(Field, "Medical"), "Medical", Field),
         Field = if_else(str_detect(Field, "Tech"), "Technology", Field),
         Field = if_else(str_detect(Field, "Finance"), "Business", Field)) %>% 
  ggplot(aes(x=Field, fill=Method)) +
  geom_bar() +
  coord_flip()
```

## I finished coding 175 articles. Now, double-checking work here!

```{r}
# Import coded 175 articles. 

coded_table <- read_csv("working_table_Oct4.csv")

# I see some gaps in agent task and some of the algorithm types near the bottom need to be recoded to use the exact wording in the paper.
```


```{r}
# Get frequencies of aversion, appreciation

coded_table %>% 
  count(Algorithm_orientations) %>% 
  mutate(prop = n/sum(n))

# Change the classes of objects
coded_table_classes <- coded_table %>% 
  drop_na(task_objectivity, Type_of_human_manipulated, N) %>% 
  mutate(N = as.numeric(N)) %>% 
  mutate(Algorithm_orientations = as.factor(Algorithm_orientations)) %>% 
  mutate(Algorithm_orientations = fct_relevel(Algorithm_orientations, c("Algorithm aversion", "Algorithm appreciation", "Both", "Neutral")))

# Combinations of task objectivity and human comparison
ggplot(coded_table_classes, aes(x=task_objectivity, y=Type_of_human_manipulated, color=Algorithm_orientations)) +
  geom_point(alpha=0.5) +
  geom_jitter() +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

```{r}
# Remake table
coded_table %>% 
  select(first_author_last_name, title, Year, Journal, journal_field, Method, N, task_objectivity, Type_of_human_manipulated, Algorithm_orientations) %>% 
  rename("First Author" = first_author_last_name,
         "Title" = title,
         "Journal Field" = journal_field,
         "Algorithm Orientations" = Algorithm_orientations,
         "Task Objectivity" = task_objectivity,
         "Human Comparison" = Type_of_human_manipulated) %>% 
  mutate(Method = str_to_title(Method)) %>% 
  kbl() %>% 
  kable_paper("striped")
```

```{r}
# Fields 
coded_table %>% 
  count(journal_field) %>% 
  arrange(desc(n)) %>% 
  print(n=34)
```

