---
title: "meta-analysis"
output: html_document
date: "2024-08-13"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import libraries

```{r}
library(tidyverse)
library(dplyr)
library(kableExtra)
library(colorspace)
library(EnvStats)
library(ggrepel)
library(treemapify)
library(wesanderson)
library(ggsci)
library(patchwork)
```

## Import csv of coded search results

```{r}
coded_articles <- read_csv("Inclusion_exclusion coding - mendeley_export.csv")
```

```{r}
# Identify remaining duplicates. These were all coded as irrelevant.

titles_abstracts <- coded_articles %>% 
  select(title, abstract)  
  
duplicates <- titles_abstracts %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Limit to unique entries
# distinct <- coded_articles %>% 
#   distinct(title) %>% 
#   filter()

# Count number of included articles
coded_articles %>% 
  count(`Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons`)

# Filter for relevant articles (1st round of coding). Manually go through and remove any remaining duplicates. (Initially not identified because of slight punctuation and case differences.) Use str_to_title() to convert all titles to title case
relevant_articles <- coded_articles %>% 
  filter(`Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons` == 1) %>% 
  filter(!(ID %in% c("ID-1355", "ID-291", "ID-264", "ID-109", "ID-137", "ID-1295", "ID-1342", "ID-144", "ID-1134", "ID-127", "ID-459", "ID-025", "ID-023", "ID-1272", "ID-1322", "ID-1444"))) %>% 
  mutate(title = str_to_title(title))


write_csv(relevant_articles, "2024_complete_list.csv")

```

```{r}
# Count proportion of each type of method for the relevant articles
relevant_articles %>% 
  count(`Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`) 

# Look at represented domains. It might be more informative to group into fewer buckets. For example, 'healthcare' and 'medical' could be combined into one category. 'Art' and 'visual art' could be combined. 
relevant_articles %>% 
  count(domain) %>% 
  print(n = 90)

# See the range of publication years
relevant_articles %>% 
  summarize(oldest_paper = min(year),
            newest_paper = max(year))

write_csv(relevant_articles, "1st_round_coding.csv")
```

```{r}
# Import spreadsheet of articles from ICA 2023 meta-analysis
old_articles <- read_csv("2nd_round_coding_2023_ICA.csv") %>% 
  rename("title" = "Article name",
         "domain" = "Task Domain",
         "first_author_last_name" = "First Author Last Name",
         "notes" = "Notes")

# See how many unique entries I added. 
new_articles <- anti_join(relevant_articles, old_articles, by = "title")

# Recode methods and domains in new list
# Last line uses string manipulation to only include first author's last name
new_articles_recoded <- new_articles %>% 
  mutate(Method = case_when(`Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "1" ~ "Quantitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "2" ~ "Qualitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "3" ~ "Review",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` %in% c("1, 2", NA) ~ "Other")) %>% 
  relocate(Method, .after = `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`) %>% 
  mutate(first_author_last_name = if_else(str_detect(author, "and") == TRUE, str_extract(author, "\\b(\\w+)\\b(?=\\s+and\\b)"), str_extract(author, "\\b([\\w-]+)\\b(?=[^\\w-]*$)"))) %>% 
  mutate(year = as.factor(year))

# Check domains from old list
old_articles %>% 
  count(Field)

old_articles %>% 
  filter(Field == "business") %>% 
  print(n = 34)

# Check domains from new list
new_articles_recoded %>% 
  count(domain) %>% 
  print(n = 82)

# Check distribution of publication years
ggplot(new_articles_recoded, aes(x=year)) +
  geom_histogram(fill = "#f9da78", color = "white", stat = "count") +
  theme(axis.title = element_text(size = 14)) +
  labs(x="Year",
       y="Count")
  

```

```{r}
# Join old and new list to create 'Update on 2023 ICA 2nd round coding'
merged_list <- bind_rows(old_articles, new_articles_recoded)

# Delete unnecessary columns
merged_list_cleaned <- merged_list %>% 
  select(-c(author, `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`, `Article ID_Old`, `Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons`))


# Export for use in Google Sheets
write_csv(merged_list_cleaned, "update_on_2023_ICA_2nd_round_coding.csv")

```

```{r}
# Create a merged list called '2024 + 2023 first round of coding' with duplicates removed. This will probably be a smaller file

# Import csv
ICA_1st_round_coding <- read_csv("2023_1st_round_coding.csv") %>% 
  rename("title" = "Article Title",
         "first_author_last_name" = "First Author Last Name",
         "Statistics" = "Does this paper have statistical methods",
         "Method" = "Method (1) experiment, (2) survey, (3) review, (4) other") %>% 
  mutate(title = str_to_title(title),
         Year = as.factor(Year))

# Multiple rows could refer to the same paper because each study is given its own row. Check to see how many *papers* there are
ICA_1st_round_coding %>% 
  distinct(title)

# There are 224 papers.
```


```{r}
# See how many unique entries I added. Rename columns so they correctly bind with old list.
unique_new_based_on_ICA_1st_round <- anti_join(relevant_articles, ICA_1st_round_coding, by = "title") 

unique_new_recoded <- unique_new_based_on_ICA_1st_round %>% 
  mutate(Method = case_when(`Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "1" ~ "quantitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "2" ~ "qualitative",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` == "3" ~ "review",
                                    `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews` %in% c("1, 2", NA) ~ "Other")) %>% 
  rename("Statistics" = `Statistics: (1) yes, (2) no`,
         "Journal" = "journal",
         "Year" = "year") %>% 
  mutate(Statistics = case_when(Statistics == "1" ~ "Yes",
                                Statistics == "2" ~ "No")) %>% 
  relocate(Method, .after = `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`) %>% 
  mutate(first_author_last_name = if_else(str_detect(author, "and") == TRUE, str_extract(author, "\\b(\\w+)\\b(?=\\s+and\\b)"), str_extract(author, "\\b([\\w-]+)\\b(?=[^\\w-]*$)"))) %>% 
  mutate(title = str_to_title(title)) %>% 
  mutate(Year = as.factor(Year))

# I added 185 unique articles. Export and get pdfs
write_csv(unique_new_recoded, "algorithm_aversion_papers_8.20.24.csv")

# Check distribution of publication years
ggplot(unique_new_recoded, aes(x=Year)) +
  geom_histogram(fill = "#f9da78", color = "white", stat = "count") +
  theme(axis.title = element_text(size = 14)) +
  labs(x="Year",
       y="Count")
```


```{r}
# Join ICA 1st round coding + unique entries to create '2024 + 2023 first round of coding'
# Delete unnecessary columns

merged_based_on_ICA_1st_round <- bind_rows(ICA_1st_round_coding, unique_new_recoded) %>% 
    select(-c(author, `Method: (1) quantitative, (2) qualitative, (3) meta analysis, reviews`, `Inclusion: (1) Yes (2) No, not English, (3) No, not focused on AI vs. human, (4) No, other reasons`))


# # Distribution of publication years
# merged_based_on_ICA_1st_round %>% 
#   filter(Year > 2014) %>% 
#   ggplot(aes(x = Year)) +
#   geom_histogram(bins = 7)

# Export
write_csv(merged_based_on_ICA_1st_round, "2024_+_2023_1st_round_coding.csv")

# 185 new + 224 old = 409
```

```{r}
# Import merged document with Sunny's second round of coding

merged_2nd_round_coding <- read_csv("2024_+_2023_2nd_round_coding_fixed_authors.csv")  %>% 
    rename("Sunny_note" = "...16",
         "title" = "title...2",
         "Sunny_coding" = "2024 systematic review inclusion and exclusion") 

```

```{r}
# Remove rows that are coded as irrelevant AND are not useful for the introduction of the paper. 
excluded_after_2nd_round <- merged_2nd_round_coding %>% 
  filter((Sunny_coding == "Exclude" & is.na(Sunny_note))) 

# Also remove duplicates
included_after_2nd_round <- anti_join(merged_2nd_round_coding, excluded_after_2nd_round) %>% 
  filter(Sunny_coding %in% c("Include", "Exclude", NA)) 

removed_duplicates_after_2nd_round <- included_after_2nd_round %>% 
  filter(!(new_list_ID %in% c("ID-293", "ID-1274"))) %>% 
  filter(!(first_author_last_name == "Bogert E, Lauharatanahirun N, Schecter A." & `Field/Discipline` == "Creative")) %>% 
  filter(!(first_author_last_name %in% c("Commerford, B.P.", "Commerford, Benjamin P.; Dennis, Sean A.; Joe, Jennifer R.; Ulla, Jenny W."))) 

# Check how many papers there are for meta-analysis
removed_duplicates_after_2nd_round %>% 
  filter(Sunny_coding %in% c("Include", NA)) %>% 
  distinct(title) 


```

```{r}
# For papers with more than one study, merge rows so that each row represents one paper. Sum sample sizes, combine all domains into one cell, etc. 

# Fill empty cells with publication year and journal using the helpful function fill()

merged_sample_sizes_tasks <- removed_duplicates_after_2nd_round %>% 
  mutate(Journal = str_to_title(Journal)) %>% 
  group_by(first_author_last_name, title) %>% 
  fill(Year, Journal, Sunny_coding) %>% 
  filter(Sunny_coding == "Include") %>% 
  mutate(`Sample Size` = if_else(str_detect(`Sample Size`, "[:alpha:]"), NA, `Sample Size`)) %>% 
  mutate(`Sample Size` = as.numeric(`Sample Size`)) %>%
  unite(col = "Field_Domain", c(`Field/Discipline`, domain), sep = ", ", na.rm = TRUE) %>% 
  unite(col = "country", c(Region, city_country), sep = ";", na.rm = TRUE) %>% 
  group_by(first_author_last_name, title) %>% 
  mutate(all_DVs = paste0(`Outcome variables`, collapse = "; "),
         all_tasks = paste0(`Task type`, collapse = "; ")) %>%
  mutate(all_DVs = str_remove(all_DVs, "NA; "),
         all_tasks = str_remove(all_tasks, "NA; ")) %>% 
  group_by(first_author_last_name, title) %>% 
  mutate(total_sample_size = sum(`Sample Size`)) %>% 
  group_by(first_author_last_name, title) %>% 
  select(-(title...14)) %>% 
  mutate(first_author_last_name = str_extract(first_author_last_name, "[^,]+")) %>% 
  arrange(first_author_last_name) 

one_paper_per_row <- merged_sample_sizes_tasks %>% 
  filter(is.na(study))
# Export to fix manually in Google Sheets

write_csv(one_paper_per_row, "merged_sample_sizes_tasks.csv")


```

```{r}
# Analyze distribution of publication years from working table
working_table <- read_csv("working_table_Sep11.csv") %>% 
  mutate(Year = as.factor(Year)) %>% 
  filter(!is.na(Year)) %>% 
  filter(`Sunny notes` == "Include")

ggplot(working_table, aes(x=Year)) +
  geom_histogram(fill = "#f9da78", color = "white", stat = "count") +
  theme(axis.title = element_text(size = 14)) +
  labs(x="Year",
       y="Count")

```

## Make a nice table that summarizes all the included papers

```{r}
sample_table <- working_table %>% 
  filter(`Sunny notes` == "Include") %>% 
  select(first_author_last_name, title, Year, Journal, Method, total_sample_size, Field_Domain, Algorithm_orientations, task_objectivity) %>% 
  rename("First Author" = first_author_last_name,
         "Title" = title,
         "N" = total_sample_size,
         "Field" = Field_Domain,
         "Algorithm Orientations" = Algorithm_orientations,
         "Task Objectivity" = task_objectivity) %>% 
  mutate(Method = str_to_title(Method)) %>% 
  mutate(Method = if_else(`First Author` == "Aljuneidi", "Interviews", Method)) %>% 
  mutate(N = as.numeric(N))

sample_table %>% 
  kbl() %>% 
  kable_paper("striped")
```

```{r}
ggplot(sample_table, aes(x = Field)) +
  geom_bar() +
  coord_flip()

# Visualize year, author, sample size, and method
ggplot(sample_table, aes(x=`First Author`, y=Year, size=N, color=Method)) +
  geom_point() +
  coord_flip()

# Need to reduce number of fields to make the patterns more clear
ggplot(sample_table, aes(x = Method, fill = Field)) +
  geom_bar()

sample_table %>% 
  mutate(Field = if_else(str_detect(Field, "Medical"), "Medical", Field),
         Field = if_else(str_detect(Field, "Tech"), "Technology", Field),
         Field = if_else(str_detect(Field, "Finance"), "Business", Field)) %>% 
  ggplot(aes(x=Field, fill=Method)) +
  geom_bar() +
  coord_flip()
```

## I finished coding 175 articles. Now, double-checking work here!

```{r}
# Import coded 175 articles. 

coded_table <- read_csv("working_table_Oct4.csv")

# I see some gaps in agent task and some of the algorithm types near the bottom need to be recoded to use the exact wording in the paper.
```


```{r}
# Get frequencies of aversion, appreciation

coded_table %>% 
  count(Algorithm_orientations) %>% 
  mutate(prop = n/sum(n))

# Change the classes of objects
coded_table_classes <- coded_table %>% 
  drop_na(task_objectivity, Type_of_human_manipulated, N) %>% 
  mutate(Type_of_human_manipulated = if_else(Type_of_human_manipulated == "Expert", "Experts", Type_of_human_manipulated)) %>% 
  mutate(N = as.numeric(N)) %>% 
  mutate(Algorithm_orientations = as.factor(Algorithm_orientations)) %>% 
  mutate(Algorithm_orientations = fct_relevel(Algorithm_orientations, c("Algorithm aversion", "Algorithm appreciation", "Both", "Neutral"))) %>% 
  mutate(Type_of_human_manipulated = fct_relevel(Type_of_human_manipulated, c("Experts", "Laypeople", "Both", "None"))) %>% 
  mutate(task_objectivity = as.factor(task_objectivity),
         task_objectivity = fct_relevel(task_objectivity, c("Objective", "Combined", "Subjective")))

# Combinations of task objectivity and human comparison
ggplot(coded_table_classes, aes(x=task_objectivity, y=Type_of_human_manipulated, color=Algorithm_orientations)) +
  geom_point(alpha=0.5) +
  geom_jitter() +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

```{r}
# Remake table
coded_table %>% 
  select(first_author_last_name, Year, Method, theory_framework, task_objectivity, Type_of_human_manipulated, Algorithm_orientations, Summary_of_findings) %>% 
  rename("First Author" = first_author_last_name,
         "Algorithm Orientations" = Algorithm_orientations,
         "Task Objectivity" = task_objectivity,
         "Human Comparison" = Type_of_human_manipulated,
         "Theoretical Framework" = theory_framework,
         "Summary of Findings" = Summary_of_findings) %>% 
  mutate(Method = str_to_title(Method)) %>% 
  kbl() %>% 
  kable_paper("striped")
```

```{r}
# Fields 
coded_table %>% 
  count(journal_field) %>% 
  arrange(desc(n)) %>% 
  print(n=34)

# Methods
coded_table %>% 
  count(Method) %>% 
  arrange(desc(n))

# Graph of methods. Good colors could be red for aversion, yellow for appreciation, orange for both, silver for neutral
coded_table_classes %>% 
  filter(Method %in% c("experiment", "survey", "mixed-methods", "interviews")) %>% 
  ggplot(aes(x=Method, fill=Algorithm_orientations)) +
  geom_bar(position = "fill") + 
  labs(y="Proportion") +
  scale_fill_manual(name="Algorithm Orientation",
                    values = c("#9D0208", "#FFBA08", "#E85D04", "#CED4DA")) +
  theme_light() 

# Table of types of algorithms

```

```{r}
# Import the edited table that addresses the identified gaps



# Table of types of algorithms and their frequencies
coded_table %>% 
  count(type_of_algorithm) %>% 
  arrange(desc(n)) %>% 
  kbl() %>% 
  kable_paper("striped")

# Graph of journal fields
coded_table_classes %>% 
  count(journal_field) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x=fct_reorder(journal_field, n), y=n)) +
  geom_col(fill="midnightblue") +
  coord_flip() +
  labs(x="Journal Field") +
  theme_light()

# Need to clean and combine categories. For example, Advertising should be under Communication 

coded_table %>% 
  mutate(AI_or_not = if_else(str_detect(type_of_algorithm, "AI"), "Yes", "No")) %>% 
  ggplot(aes(x=AI_or_not)) +
  geom_bar()
```

```{r}
# Rates of algorithm aversion over time
coded_table_classes %>% 
  drop_na(Algorithm_orientations) %>% 
  group_by(Year) %>% 
  count(Algorithm_orientations) %>%
  mutate(prop=n/sum(n)) %>% 
  ggplot(aes(x=Year, y=prop, color=Algorithm_orientations)) +
  geom_line(aes(group=Algorithm_orientations)) +
  geom_point() +
  scale_color_manual(name="Algorithm Orientation",
                    values = c("#9D0208", "#FFBA08", "#E85D04", "#CED4DA"))

```

```{r}
# Subjective vs. objective tasks, self vs. other, algorithm orientations

coded_table_classes %>% 
  count(agent_tasks) %>% 
  arrange(desc(n)) %>% 
  print(n=150)


# All features combined in one graph, too busy
# coded_table_classes %>% 
#   filter(task_objectivity %in% c("Subjective", "Combined", "Objective"),
#          `subject of the decision` %in% c("self", "others")) %>% 
#   ggplot(aes(x=task_objectivity, y=`subject of the decision`, color=Type_of_human_manipulated)) +
#   geom_jitter() +
#   geom_text(data=subset(coded_table_classes, str_detect(agent_tasks, "forecast the price of a stock|recommend a product|generate dating profile|HR|classify|provide a diagnosis|prioritize healthcare resources") & nchar(agent_tasks) < 32), aes(label = agent_tasks), size=2, color="black") +
#   facet_wrap(~Algorithm_orientations) +
#   labs(x="Task objectivity",
#        y="Subject of decision",
#        color="Relative to whom?") +
#   scale_color_manual(values = c("#99D98C", "#EDAE49", "#003D5B", "gray")) +
#   theme_light()
```

```{r}
# Make a simpler graph of frequencies of outcomes for different human comparisons

coded_table_classes %>% 
  mutate(Algorithm_orientations = case_when(Algorithm_orientations == "Both" ~ "Mixed",
                                            Algorithm_orientations == "Algorithm aversion" ~ "Aversion",
                                            Algorithm_orientations == "Algorithm appreciation" ~ "Appreciation",
                                            Algorithm_orientations == "Neutral" ~ "Neutral")) %>%
  mutate(Algorithm_orientations = fct_relevel(Algorithm_orientations, c("Aversion", "Appreciation", "Mixed", "Neutral"))) %>% 
  group_by(Algorithm_orientations) %>% 
  count(Type_of_human_manipulated) %>% 
  mutate(prop =n/sum(n)) %>% 
  filter(Type_of_human_manipulated %in% c("Experts", "Laypeople")) %>%
  ggplot(aes(x=Type_of_human_manipulated, y=prop, fill=Algorithm_orientations)) +
  geom_col() +
  facet_wrap(~Algorithm_orientations) +
  theme_light() +
  labs(x="Human reference point",
       y="Proportion of articles") +
  scale_y_continuous(breaks = c(0, .1, .2, .3, .4, .5, .6)) +
  scale_fill_manual(name = "Algorithm orientations",
    values = c("#A11E22", "#E8A631","#E8C898", "lightblue")) +
  theme(axis.title = element_text(size = 14),
        legend.position = "none")
```

```{r}
# Sample sizes across algorithm orientations

ggplot(coded_table_classes, aes(x=N, color=Algorithm_orientations)) +
  geom_density(alpha=.25) +
  xlim(c(0, 7500))
```

```{r}
# Year and method
coded_table_classes %>% 
  filter(Method %in% c("experiment", "field experiment", "interviews", "mixed-methods", "survey", "review")) %>% 
  ggplot(aes(x=Year, fill=Method)) +
  geom_bar()
```

```{r}
# Treemap for each algorithm orientation? Nah, too confusing

# coded_table_classes %>% 
#   filter(task_objectivity %in% c("Objective", "Combined", "Subjective")) %>% 
#   filter(Type_of_human_manipulated %in% c("Experts", "Laypeople", "Both")) %>%
#   group_by(Algorithm_orientations) %>% 
#   count(task_objectivity, Type_of_human_manipulated) %>% 
#   mutate(prop = n/sum(n)) %>% 
#   ggplot(aes(area=prop, fill=Type_of_human_manipulated, label=task_objectivity, subgroup=Type_of_human_manipulated)) +
#   geom_treemap() +
#   geom_treemap_text(color="white",place="center", size=12) +
#   facet_wrap(~Algorithm_orientations,
#              labeller = labeller(Algorithm_orientations = algorithm_orientation.labs)) +
#   labs(fill="Human reference point") 
```

```{r}
# Import updated table
task_domains <- read_csv("working_table_Oct10.csv")
```


```{r}
# Graph of task domain frequencies
relabeled_domains <- task_domains %>% 
  drop_na(task_domain) %>% 
  mutate(task_domain = if_else(first_author_last_name == "Hohensinn", "Legal/Ethical decisions", task_domain)) %>% 
  mutate(task_domain = case_when(task_domain == "Forecasting and Predictive Analysis" ~ "Finance and Predictive Analysis",
                                 task_domain == "HR/Education decisions" ~ "Career and Education",
                                 task_domain == "Creative tasks" ~ "Arts and Language",
                                 task_domain == "Healthcare/Medical decisions" ~ "Healthcare and Medical",
                                 task_domain == "Legal/Ethical decisions" ~ "Legal and Ethical",
                                 task_domain == "Customer and community services" ~ "Customer and Public Service",
                                 task_domain == "Classification/Memory tasks" ~ "Classification and Memory",
                                 task_domain == "Multiple" ~ "Multiple")) 

relabeled_domains %>% 
  count(task_domain) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x=fct_reorder(task_domain, n), y=n)) +
  geom_col(fill="midnightblue") +
  coord_flip() +
  labs(x="Task Domain") +
  theme_light() +
  labs(x="Number of Articles")

# Would use the following if you wanted to arrange in descending order by frequency of domain
# ggplot(aes(x = forcats::fct_rev(fct_infreq(task_domain)))) 

relabeled_domains %>% 
  drop_na(Algorithm_orientations) %>% 
  count(Algorithm_orientations) %>% 
  mutate(prop = n/sum(n))
```


```{r}
# Orientations by Task Domain
results_by_task_domain <- relabeled_domains %>% 
  drop_na(Algorithm_orientations) %>% 
  filter(task_domain != "Multiple") %>% 
  mutate(Algorithm_orientations = fct_relevel(Algorithm_orientations, c("Neutral", "Both", "Algorithm appreciation", "Algorithm aversion"))) %>% 
  ggplot(aes(x = fct_rev(fct_infreq(task_domain)))) +
  geom_bar(aes(fill = Algorithm_orientations), position="stack") +
  coord_flip() +
  labs(x="Task domain",
       y="Number of articles",
       fill="Result",
       tag = "a") +
  theme_light() +
  scale_fill_manual(
                    values = c("#A11E22", "#E8A631","#E8C898", "lightblue"),
                    breaks=c('Algorithm aversion', 'Algorithm appreciation', 'Both', "Neutral"),
                    labels=c("Aversion", "Appreciation", "Mixed", "Neutral")) +
  theme(legend.position = "top",
        legend.key.spacing = unit(1, "mm"),
        legend.title = element_text(size = 10),
        legend.text = element_text(size=8),
        axis.title = element_text(size=14))

results_by_task_domain
```


```{r}
# Just algorithm aversion
algorithm_aversion_by_task_domain <- relabeled_domains %>% 
  drop_na(Algorithm_orientations) %>% 
  group_by(task_domain) %>% 
  count(Algorithm_orientations) %>% 
  mutate(prop = n/sum(n)) %>% 
  filter(Algorithm_orientations == "Algorithm aversion") %>% 
  filter(task_domain != "Multiple") %>% 
  mutate(Algorithm_orientations = fct_relevel(Algorithm_orientations, c("Neutral", "Both", "Algorithm appreciation", "Algorithm aversion"))) %>% 
  ggplot(aes(x=reorder(task_domain, prop), y=prop)) +
  geom_col(fill="#A11E22") +
  geom_hline(yintercept=.5, linetype=2, color="darkgray") +
  coord_flip() +
  theme_light() +
  labs(x="",
       y="Proportion of articles finding algorithm aversion",
       caption = "Dashed line at y = 0.5 indicates the mean rate of algorithm aversion\nacross all articles in the sample",
       tag = "b") +
  theme(axis.title = element_text(size=14))

algorithm_aversion_by_task_domain
  
```

```{r}
# Combine the plots
results_by_task_domain + algorithm_aversion_by_task_domain
```


```{r}
task_domains %>% 
  count(type_of_algorithm) %>% 
  arrange(desc(n))

broad_tech_categories <- task_domains %>% 
  mutate(broad_tech_category = case_when(str_detect(type_of_algorithm, "AI|ChatGPT") ~ "AI",
                                         str_detect(type_of_algorithm, "robot") ~ "robot",
                                         str_detect(type_of_algorithm, "chatbot") ~ "chatbot",
                                         str_detect(type_of_algorithm, "machine learning|machine-learning") ~ "machine learning algorithm",
                                         str_detect(type_of_algorithm, "vehicle|car") ~ "self-driving vehicle")) %>%
  mutate(broad_tech_category = replace_na(broad_tech_category, replace="rule-based algorithm")) %>% 
  mutate(broadest_tech_category = case_when(broad_tech_category == "AI" ~ "AI",
                                            broad_tech_category == "rule-based algorithm" ~ "rule-based algorithm",
                                            broad_tech_category %in% c("chatbot", "robot", "machine learning algorithm", "self-driving vehicle") ~ "other"))

broad_tech_categories %>% 
  count(broadest_tech_category) %>% 
  arrange(desc(n))

# Prevalence of types of algorithms over time
broad_tech_categories %>% 
  mutate(Year = if_else(first_author_last_name == "Chiou", 2021, Year)) %>% 
  group_by(Year) %>% 
  count(broadest_tech_category) %>% 
  ggplot(aes(x=Year, y=n, color=broadest_tech_category)) +
  geom_line(aes(group=broadest_tech_category), size = 1) +
  geom_point(size=3) +
  labs(x="Year",
       y="Number of articles",
       color="Technology") +
  scale_x_continuous(breaks = seq(2015, 2024, by=1)) +
  scale_y_continuous(limits = c(0,30), breaks = seq(0, 30, by=5)) +
  scale_color_npg(breaks=c("AI", "rule-based algorithm", "other"),
                  labels=c("'AI'", "rule-based 'algorithm'", "other")) +
  theme_light() +
  theme(axis.title = element_text(size = 16),
        axis.text = element_text(size=14),
        legend.title = element_blank(),
        legend.text = element_text(size=14))
```

