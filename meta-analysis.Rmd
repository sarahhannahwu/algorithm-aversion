---
title: "meta-analysis"
output: html_document
date: "2024-11-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(dplyr)
library(metafor)
library(clubSandwich)

```

## Import Data


```{r, eval=FALSE}
# import data file from ICA 2024
# Jinxu tab in 1029-MATA-DATA sheet
ICA_2024 <- read_csv("ICA_2024.csv") %>% 
  select(number, `First Author Last Name`, `Article name`, Substudy, Study, predictor, `Outcome variable 1
(Weight on advice - algo appreciate)`)
# import ICA 2025 systematic review table for other variables
systematic_review <- read_csv("systematic_review_table.csv")
#name data file and read in .csv 
dat1 <- read.csv("meta-analysis-data.csv") 
#fix cells of data file
dat1[226, 4] = 0
dat1[218, 4] = 0
dat1[206, 10] = -738
dat1[206, 11] = 4112
dat1[206, 13] = -658
dat1[206, 14] = 3397
```

```{r, eval=FALSE}
# typos
typos <- c("advior", "psychologicst")
corrections <- c("advisor", "psychologist")

# join Jinxu's simplified meta-analysis data with the more comprehensive data file
ICA_2024_full <- full_join(ICA_2024, SMD.Data, by="number") %>% 
  select(-c(X, X.1)) %>% 
  rename("first_author_last_name" = "First Author Last Name",
         "title" = "Article name",
         "DV" = "Outcome variable 1
(Weight on advice - algo appreciate)",
         "human_mean" = "humanmean",
         "human_sd" = "humansd",
         "human_n" = "humanN",
         "algorithm_mean" = "AImean",
         "algorithm_sd" = "AISD",
         "algorithm_n" = "AIN") %>% 
  fill(first_author_last_name,
       title,
       Substudy,
       Study) %>% 
# correct typos
  mutate(predictor = str_replace_all(predictor, "advior", "advisor")) %>% 
  mutate(predictor = str_replace_all(predictor, "psychologicst", "psychologist")) %>% 
# correc author names
  mutate(first_author_last_name = if_else(title == "Who is the Expert? Reconciling Algorithm Aversion and Algorithm Appreciation in AI-Supported Decision Making", "Hou", first_author_last_name)) %>% 
  mutate(first_author_last_name = if_else(title == "Algorithm appreciation or aversion? Comparing in-service and pre-service teachers’ acceptance of computerized expert models", "Kaufmann", first_author_last_name))
```

```{r, eval=FALSE}
# try escalc for ICA 2024 data
ICA_2024_effect_size <- escalc(measure="SMD", m1i=human_mean, sd1i=human_sd, n1i=human_n,
               m2i=algorithm_mean, sd2i=algorithm_sd, n2i=algorithm_n, data=ICA_2024_full) 
```


```{r, eval=FALSE}
included_in_meta_analysis <- dat1 %>% 
  filter(Involved==1) %>% 
  filter(first_author_last_name != "Freisinger") %>% 
  filter(first_author_last_name != "Millet") %>% 
  mutate(title = str_to_title(title))


# combine with data from systematic review
meta_analysis_with_moderators <- left_join(included_in_meta_analysis, systematic_review, by = c("first_author_last_name", "title")) %>% 
  rename("outcome_variable" = "Outcome.variable.1..Weight.on.advice...algo.appreciate.") %>% 
  rename("Method" = "Method.y") %>% 
  select(Article.ID_New, first_author_last_name, title, Substudy, Sample.size, predictor, outcome_variable, Operationalization, human_mean, human_sd, human_n, algorithm_mean, algorithm_sd, algorithm_n, Female, Age_Mean, year, Method) %>% 
  fill(year) %>% 
  fill(Method) 
```


```{r, eval=FALSE}
cols.num <- c("human_mean","human_sd", "algorithm_mean", "algorithm_sd")
meta_analysis_with_moderators[cols.num] <- sapply(meta_analysis_with_moderators[cols.num],as.numeric)
sapply(meta_analysis_with_moderators, class)

#calculate overall effect size, in this case standardized mean difference (Hedges g), and variance. 
overall_effect_size <- escalc(measure="SMD", m1i=human_mean, sd1i=human_sd, n1i=human_n,
               m2i=algorithm_mean, sd2i=algorithm_sd, n2i=algorithm_n, data=meta_analysis_with_moderators) 
#display dataset with effect size and variance
overall_effect_size

#reverse score studies by Bonezzi, Longoni 1a-6c 
ICA_2025 <- overall_effect_size %>% 
  mutate(yi = if_else(Article.ID_New == 153, yi*(-1), yi)) %>% 
  mutate(yi = if_else(Article.ID_New == 54, yi*(-1), yi)) %>% 
  mutate(number = 63:124) %>% 
  rename("female" = "Female",
         "age" = "Age_Mean",
         "DV" = "outcome_variable") %>% 
  mutate(female = as.numeric(female),
         age = as.numeric(age))
```

```{r, eval=FALSE}
# Bind rows to join ICA 2024 data with ICA 2025 data
all_effects <- bind_rows(ICA_2024_effect_size, ICA_2025, .id="number") %>% 
  filter(first_author_last_name != "Allen, Ryan T") %>%  # exclude article with irrelevant DV
  mutate(number = 1:123) %>%
  rename("effect_id" = "number") %>% 
  relocate(Operationalization, .after = "DV") %>% 
  select(-Article.ID_New) %>% 
  mutate(study_id = as.integer(factor(title))) %>%
  mutate(study_id = match(title, unique(title))) %>% 
  relocate(study_id, .after = effect_id)

write_csv(all_effects, "all_effects.csv")

# check and fix the numbers descriptives for Logg. something looks wrong...

all_effects[4, 10] = 154
all_effects[4, 11] = 0.35
all_effects[4, 12] = 0.36
all_effects[4, 13] = 51
all_effects[4, 14] = 0.50
all_effects[4, 15] = 0.37
all_effects[4, 16] = 51

# fix SDs for Sharan study. Right now it has SEs

all_effects <- all_effects %>% 
  mutate(human_sd = if_else(effect_id == 27, .91, human_sd)) %>% 
  mutate(algorithm_sd = ifelse(effect_id == 27, .97, algorithm_sd))

write_csv(all_effects, "all_effects.csv")
```


```{r}
# fix SDs for Commerford. Right now it has SEs. Also, for now use overall Proposed DV
all_effects_12.16.24 <- read_csv("all_effects_12.16.24.csv")



```


```{r, eval=FALSE}
##########analyses
# Uncomment to mutate df 
# unite(col = "Study", c("first_author_last_name", "year", "Substudy"), sep = "_")

#run overall random effects meta-analysis
# overallresult <- rma(yi, vi, data=all_effects)
# #display overall result
# overallresult

#forest plot
# forest.rma(overallresult, slab = all_effects$Substudy, header="Study", order = "obs")

# for papers where DV was a sliding scale from human to AI, maybe use midpoint and find z-score to standardize
```

```{r, eval=FALSE}
# Use robust variance estimation
m_multi <- rma.mv(yi = yi, V = vi, random = list(~1 | effect_id, ~1 | study_id), data = all_effects)
m_multi

# One effect per substudy
one_effect_per_substudy <- all_effects_12.16.24 %>% 
  group_by(title, Substudy) %>% 
  slice_head(n=1) %>% 
  mutate(Substudy = str_to_upper(Substudy)) %>% 
  unite(col = "Study", c("first_author_last_name", "year", "Substudy"), sep = ", ") %>% 
  filter(effect_id != 61)

one_effect_per_substudy_result <- rma(yi, vi, data=one_effect_per_substudy)

forest.rma(one_effect_per_substudy_result, slab = one_effect_per_substudy$Study, header = "Study", order="obs")

rve <- rma.mv(yi = yi, V = vi, random = list(~1 | effect_id, ~1 | study_id), data = one_effect_per_substudy)
rve

forest.rma(rve, slab = one_effect_per_substudy$Study, header = "Study", order = "obs")

```

```{r, eval=FALSE}
# titles must be exactly the same in the two data files, otherwise the join won't work. So make sure the case and punctuation are exactly the same for all titles
systematic_review <- read_csv("systematic_review_table.csv") %>% 
  mutate(title = str_to_title(title),
         title = str_remove(title, "\\."),
         title = str_replace(title, " – ", ": ")) 
all_effects_moderators <- read_csv("all_effects_moderators_1.8.25.csv") %>% 
  mutate(title = str_to_title(title), 
        title = str_remove(title, "\\."),
        title = str_replace(title, "—", ": "))

# Check the way the algorithm was framed to participants
all_effects_vars <- left_join(all_effects_moderators, systematic_review, by = "title") %>% 
  mutate(within_between_design = if_else(Sample.size == human_n & Sample.size == algorithm_n, "within", "between")) %>% # code experimental design
  relocate(human_alternative, .after = predictor) %>% 
  select(-c(Year, journal_field, `If other, specify method`, Method.y, N, first_author_last_name.y)) %>% 
  mutate(Substudy = str_to_lower(Substudy)) %>% 
  rename(first_author_last_name = first_author_last_name.x,
         method = Method.x,
         algorithm_category = `Type of algorithm categories`) %>% 
  mutate(first_author_last_name = if_else(title == "Online Dating Meets Artificial Intelligence: How The Perception Of Algorithmically Generated Profile Text Impacts Attractiveness And Trust", "Wu, Y", first_author_last_name)) %>% 
  mutate(first_author_last_name = if_else(title == "Acceptance Of Medical Treatment Regimens Provided By Ai Vs Human", "Wu, J", first_author_last_name)) %>% 
  mutate(title = str_replace_all(title, "Ai", "AI")) %>% 
  relocate(task_objectivity, .after = objective) %>% 
  relocate(algorithm_category, .after = tech_function) %>% 
  relocate(task_domain, .after = domain) %>% 
  mutate(objective = if_else(task_objectivity == "Objective", 1, objective),
         objective = if_else(task_objectivity == "Subjective", 2, objective)) %>% 
  mutate(effect_id = 1:n())
```


```{r, eval=FALSE}
# Since each row in the meta-analysis represents a single effect size, go back to the columns from the systematic review that are coded at the paper level and replace those values with the values from the substudy

# Also fill in empty Operationalization cells
write_csv(all_effects_vars, "all_effects_vars.csv")
# SW working in Google Sheet to fill these in
```


```{r}
# import filled in data sheet
all_effects_moderators <- read_csv("all_effects_moderators_1.13.25.csv")

# edited outcomes categories column to replace "Multiple" with the specific outcome category for that effect size
```

```{r}
all_effects_moderators_subset <- all_effects_moderators %>% 
  select(effect_id, study_id, first_author_last_name, title, Substudy, human_mean, algorithm_mean, human_sd, algorithm_sd, human_n, algorithm_n)

es_from_mean_sd <- metafor::escalc(measure = "SMD", 
                          vtype="LS", 
                          m1i = human_mean,           
                          m2i = algorithm_mean, 
                          sd1i = human_sd,
                          sd2i = algorithm_sd, 
                          n1i = human_n, 
                          n2i = algorithm_n,
                          data = all_effects_moderators_subset,
                          slab = effect_id)  

es_correct_directions <- es_from_mean_sd %>% 
  mutate(yi = if_else(effect_id %in% c(29, 65:71, 89:96), yi*(-1), yi))

main_model <- rma.mv(yi = yi, 
                     V = vi, 
                     data = es_correct_directions, 
                     tdist=TRUE, 
                     method = "REML",         
                     level = 95, 
                     digits = 7, 
                     slab = effect_id, 
                     random = ~ 1 | study_id/Substudy/effect_id)   

main_model_robust <- robust(main_model, cluster = study_id, clubSandwich=TRUE)

print(summary(main_model_robust))
```
```{r}
# Check for outliers

#adapting CI calculation and plotting from Vaccaro et al. (2024) https://osf.io/rgqsc?view_only=b9e1e86079c048b4bfb03bee6966e560
 # Standardized residuals of correlations (each effect size)
  resid_es <- rstandard(main_model)
  outliers_resid_es <- resid_es %>%
    subset(resid_es$z > 3 | resid_es$z < -3) # determine outliers
  print(outliers_resid_es)
  
  # Cooks distance (each effect size)
  cooks_es <- cooks.distance(main_model_robust, progbar=TRUE) # takes a long time to run
  threshold <- 4 / (length(es_correct_directions$effect_id))
  outliers_cooks_es <- cooks_es %>%
    subset(cooks_es > threshold) # determine influence
  print(outliers_cooks_es)
  
  df_sens <- cbind(es_correct_directions, resid = resid_es$z, cooks = cooks_es)

  # Perform re-analysis on dataset excluding outliers and influential points
  df_sens <- filter(df_sens, (abs(df_sens$resid) < 3) & (df_sens$cooks < threshold)) 
  main_model_sens <- rma.mv(yi = yi, 
                  V = vi,
                  data = df_sens,
                  method = "REML",         # restricted maximum likelihood
                  level = 95,              # CI
                  digits = 7,              # decimal points
                  slab = effect_id,          # paper labels
                  random = ~ 1 | study_id/Substudy/effect_id # multilevel model to handle sample dependency
                  )  
  print(summary(main_model_sens))
  
  main_model_sens_robust <- robust(main_model_sens, cluster = df_sens$effect_id)
  print(summary(main_model_sens_robust))
  
   # Outliers
  df_outliers <- df_sens %>% 
    filter(abs(resid) > 3 | cooks > threshold)
```

```{r}
# Forest plot of all effect sizes
forest(main_model_robust,
       annotate=FALSE,
       slab = NA,
       order = "obs",
       col = "salmon",
       xlab = "Effect sizes (Hedge’s g) and 95% confidence intervals",
       cex.axis = .75,
       efac = 2)
```



```{r}

# Dummy-code the objectivity moderator column, create three columns for objective or not, subjective or not, combined or not. Each would be entered as a moderator
dummy_coded <- all_effects_moderators %>% 
  mutate(objective_yn = if_else(objective == 1, 1, 0)) %>% 
  mutate(subjective_yn = if_else(objective == 2, 1, 0)) %>% 
  mutate(combined_yn = if_else(objective == 3, 1, 0))

# Separate data by DV, then do a multiple test correction. 
# divide alpha by number of separations
# Bonferroni correction

dummy_coded %>% 
  count(DV) %>% 
  arrange(desc(n)) %>% 
  print(n=59)
# 
# clean_DVs <- dummy_coded %>% 
#   mutate(DV_concept = if_else(DV == regex("^algorithm appreciate$"), "weight on advice", DV)) %>% 
#   mutate(DV_concept = if_else(str_detect(DV, regex("weight on advice", ignore_case = TRUE)), "weight on advice", DV)) %>% 
#   relocate(DV_concept, .after = DV)


# report variance of differences grouped by substudies

```
```{r}
# moderator analyses

# Nested effect sizes
nested_mod <- rma.mv(yi = yi,
                     V = vi,
                     data = dummy_coded,
                     random = ~1|study_id/Substudy/effect_id,
                     method = "REML",
                     test = "t",
                     dfs = "contain",
                     mods = ~factor(expert, 
                                    objective_yn, 
                                    subjective_yn, 
                                    combined_yn, 
                                    paradigm, 
                                    within_between_design))
summary(nested_mod)
```


```

```{r, eval=FALSE}
##check for influence
#influence analysis
influence(nested_mod)
#Logg et al. 2019 Study 2, Kaufmann et al. 2021 Study 2, and Commerford et al. 2021 Study 1
```


```{r, eval=FALSE}
##categorical moderator

#moderator test to calculate qbetween value for categorical moderator
mod.gradeq <- rma(yi, vi, mods = ~ factor(graderange), data=dat1)
mod.gradeq

# Save QM Test and write it into a text file
Qgrade_collapsed_string <- paste(mod.gradeq[["QMdf"]])
Qgrade_type1 <- data.frame(CollapsedQMdf = Qgrade_collapsed_string)
Qgrade_type2 <- round(mod.gradeq$QM,2)
Qgrade_type3 <- round(mod.gradeq$QMp,3)

QgradeQ <- paste(
  "Qb(",Qgrade_collapsed_string,") =", 
  Qgrade_type2,
  ", p =", 
  Qgrade_type3,
  collapse = " "
)

cat(QgradeQ, "\n")
gradeQtest <- data.frame(Text = QgradeQ)
write.table(gradeQtest, file = "QgradeQ.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)

#moderator test to get mean effect size for each group categorical moderator (removes intercept)
mod.grade <- rma(yi, vi, mods = ~ factor(graderange)-1, data=dat1)
#Display moderator result
mod.grade

##save moderator results to table
#Only save table results to new data item
mod.grade_table <-coef(summary(mod.grade))
#calculate participants in each group and add it to the table, then save the table.
gradeSumParticipants <- dat1 %>%
  group_by(graderange) %>%
  summarise(nexp = sum(intn, na.rm = TRUE),
            nctrl = sum(cn, na.rm = TRUE))
gradeNumComp <- dat1 %>%
  count(graderange)
gradeNumComp <- rename(gradeNumComp, kcomparisons = n)
grade_type_table.final<- cbind(gradeSumParticipants,gradeNumComp[c(2)], mod.grade_table) 
write.csv(grade_type_table.final, "Mod.gradeResult.csv")


##continuous moderator
#continuous moderator test
mod.cont <- rma(yi, vi, mods = ~ cont, data=dat1)
mod.cont




#########publication bias analyses

#standard funnel plot
funnel(overallresult)
# carry out trim-and-fill analysis
trimandfill <- trimfill(overallresult)
trimandfill
funnel(trimandfill)
#Eggers regression
regtest(overallresult)
#Rosenthal, Orwin, & Rosenberg Fail Safe N test 
fsn(yi, vi, data=dat1)
fsn(yi, vi, data=dat1, type="Orwin")
fsn(yi, vi, data=dat1, type="Rosenberg")
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
